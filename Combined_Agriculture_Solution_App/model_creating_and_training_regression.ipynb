{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "34a018b1-7bee-4630-b1dd-8ef20e7176a4",
      "metadata": {
        "id": "34a018b1-7bee-4630-b1dd-8ef20e7176a4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JUFWhaP04UNf"
      },
      "id": "JUFWhaP04UNf",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "5221dd1c-060a-4740-b92f-261ccae29db5",
      "metadata": {
        "id": "5221dd1c-060a-4740-b92f-261ccae29db5"
      },
      "outputs": [],
      "source": [
        "dataset = np.loadtxt(\"dataset.csv\", delimiter=',', dtype = np.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "a0c8181f-b714-467c-afed-56bc1dc5707f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0c8181f-b714-467c-afed-56bc1dc5707f",
        "outputId": "b4c9eeff-39ea-440c-f233-2bdd7bbad17e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "999913"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "len(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "b8de6c79-9cce-48d4-839f-212fc585bfa2",
      "metadata": {
        "id": "b8de6c79-9cce-48d4-839f-212fc585bfa2"
      },
      "outputs": [],
      "source": [
        "training_dataset = dataset[0:900000]\n",
        "testing_dataset = dataset[900001:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "bac21617-ad8d-4d0e-a727-debb4b200bf5",
      "metadata": {
        "id": "bac21617-ad8d-4d0e-a727-debb4b200bf5"
      },
      "outputs": [],
      "source": [
        "class dataset (Dataset):\n",
        "    def __init__(self,data,label):\n",
        "        self.data = data\n",
        "        self.label = label\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    def __getitem__(self,idx):\n",
        "        return self.data[idx],self.label[idx]\n",
        "data_set = dataset(training_dataset[:,:-1],training_dataset[:,-1])\n",
        "dataloader = DataLoader(data_set,batch_size=10000,shuffle = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "d368e442-1c61-4343-bd29-f3b6c3bb91c9",
      "metadata": {
        "id": "d368e442-1c61-4343-bd29-f3b6c3bb91c9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "8093d4f7-4520-4e65-984f-57ff8f25bffc",
      "metadata": {
        "id": "8093d4f7-4520-4e65-984f-57ff8f25bffc"
      },
      "outputs": [],
      "source": [
        "#creating the model for prediction part\n",
        "class Prediction_Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Prediction_Model,self).__init__()\n",
        "        self.l1 = nn.Linear(9,20)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.l2 = nn.Linear(20,10)\n",
        "        self.l3 = nn.Linear(10,1)\n",
        "    def forward(self,x):\n",
        "        out = self.l1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.l2(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.l3(out)\n",
        "        return out\n",
        "\n",
        "model = Prediction_Model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "1b43d5bf-f746-4c85-bb09-45a499eced5a",
      "metadata": {
        "id": "1b43d5bf-f746-4c85-bb09-45a499eced5a"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\")\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "ced246bf-325d-4fe3-9f2d-6a418bfde98a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ced246bf-325d-4fe3-9f2d-6a418bfde98a",
        "outputId": "90b30ec7-480c-48f9-d272-8324b347d0c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch : 1 training loss : 6.599350929260254 testing loss : 0.9998264186214687\n",
            "epoch : 2 training loss : 1.4451292753219604 testing loss : 0.9519779302409261\n",
            "epoch : 3 training loss : 1.2998615503311157 testing loss : 0.9012227431199548\n",
            "epoch : 4 training loss : 1.1487154960632324 testing loss : 0.8462470549177983\n",
            "epoch : 5 training loss : 1.0229151248931885 testing loss : 0.8064680815674654\n",
            "epoch : 6 training loss : 0.9231826066970825 testing loss : 0.7595710313137853\n",
            "epoch : 7 training loss : 0.7955646514892578 testing loss : 0.6942424985717498\n",
            "epoch : 8 training loss : 0.6391938924789429 testing loss : 0.6061949507771511\n",
            "epoch : 9 training loss : 0.48274630308151245 testing loss : 0.5190107022276095\n",
            "epoch : 10 training loss : 0.36697226762771606 testing loss : 0.4575035668735715\n",
            "epoch : 11 training loss : 0.30692434310913086 testing loss : 0.42851929506935665\n",
            "epoch : 12 training loss : 0.2812122702598572 testing loss : 0.4204715909614935\n",
            "overtrained\n"
          ]
        }
      ],
      "source": [
        "#training area and getting validation curve\n",
        "loss = nn.MSELoss()\n",
        "optim = torch.optim.Adam(model.parameters(),lr = 0.001)\n",
        "training_loss = []\n",
        "testing_loss = []\n",
        "epoch = []\n",
        "for e in range(20):\n",
        "    net_training_loss = 0.0\n",
        "    net_iteration = 0\n",
        "    for i,(data,label) in enumerate(dataloader):\n",
        "        label = label.view(10000,1)\n",
        "        optim.zero_grad()\n",
        "        data = data.to(device)\n",
        "        label = label.to(device)\n",
        "        prediction = model.forward(data)\n",
        "        l = loss(prediction,label)\n",
        "        l.backward()\n",
        "        optim.step()\n",
        "        net_training_loss += l\n",
        "        net_iteration += 1\n",
        "    trainingloss = net_training_loss/net_iteration\n",
        "    training_loss.append(trainingloss)\n",
        "    net_testing_loss = 0.0\n",
        "    net_iteration = 0\n",
        "    for data_for_testing in testing_dataset:\n",
        "        dt,lt = data_for_testing[:-1], data_for_testing[-1]\n",
        "        prediction = model.forward(torch.from_numpy(dt).view(1,9).to(device))\n",
        "        l = abs(lt.item() - prediction.item())\n",
        "        net_testing_loss += l\n",
        "        net_iteration += 1\n",
        "    testingloss = net_testing_loss/net_iteration\n",
        "    testing_loss.append(testingloss)\n",
        "    epoch.append(e+1)\n",
        "    if((testingloss-trainingloss)/trainingloss > 0.5):\n",
        "      print('overtrained')\n",
        "      break\n",
        "    print(f\"epoch : {e+1} training loss : {trainingloss} testing loss : {testingloss}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f6d5b06-7d39-43e1-9181-5b8c7533951f",
      "metadata": {
        "id": "4f6d5b06-7d39-43e1-9181-5b8c7533951f"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d45e478-fa1a-40ff-91cd-5107d499cdc0",
      "metadata": {
        "id": "4d45e478-fa1a-40ff-91cd-5107d499cdc0",
        "outputId": "2f31a153-63a9-4b0d-dc48-4a66843c1624"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([4.8073], grad_fn=<ViewBackward0>)\n",
            "4.584716\n"
          ]
        }
      ],
      "source": [
        "#testing dataset\n",
        "import random\n",
        "dt,lt = testing_dataset[:,:-1],testing_dataset[:,-1]\n",
        "# print(dt.shape,lt.shape)\n",
        "i = random.randint(0,99900)\n",
        "print(model.forward_training(torch.from_numpy(dt[i])))\n",
        "print(lt[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1aa00d8-aecd-42c8-8fd1-a0377909b8ba",
      "metadata": {
        "id": "d1aa00d8-aecd-42c8-8fd1-a0377909b8ba"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5fc7ed0-234a-42fa-a676-99c9035463b7",
      "metadata": {
        "id": "e5fc7ed0-234a-42fa-a676-99c9035463b7"
      },
      "outputs": [],
      "source": [
        "#saving the model\n",
        "# torch.save(model,\"user_interface/model/agriculture_yield_prediction\")\n",
        "model_scripted = torch.jit.script(model) # Export to TorchScript\n",
        "model_scripted.save('user_interface/model/agriculture_yield_prediction.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37892e86-9f3c-4869-9264-3c799b63b6f3",
      "metadata": {
        "id": "37892e86-9f3c-4869-9264-3c799b63b6f3",
        "outputId": "07684821-024f-4e34-9049-fe6466bbcfdb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "RecursiveScriptModule(\n",
              "  original_name=Prediction_Model\n",
              "  (l1): RecursiveScriptModule(original_name=Linear)\n",
              "  (relu): RecursiveScriptModule(original_name=ReLU)\n",
              "  (l2): RecursiveScriptModule(original_name=Linear)\n",
              "  (l3): RecursiveScriptModule(original_name=Linear)\n",
              ")"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# #loading it\n",
        "# loadmodel = torch.load(\"user_interface/model/agriculture_yield_prediction\")\n",
        "# loadmodel.eval()\n",
        "loadmodel = torch.jit.load('user_interface/model/agriculture_yield_prediction.pt')\n",
        "loadmodel.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6be15ab6-f5fb-4647-a992-c4a5a1a6a30d",
      "metadata": {
        "id": "6be15ab6-f5fb-4647-a992-c4a5a1a6a30d",
        "outputId": "ff5461a6-8584-43b2-f630-bde0289e1a54"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[  1.         0.         1.       745.2252    21.727076   1.\n",
            "   1.         0.       102.      ]\n",
            "tensor([6.9891], grad_fn=<ViewBackward0>)\n",
            "6.2779455\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "dt,lt = testing_dataset[:,:-1],testing_dataset[:,-1]\n",
        "# print(dt.shape,lt.shape)\n",
        "i = random.randint(0,99900)\n",
        "print(dt[i])\n",
        "print(loadmodel.forward(torch.from_numpy(dt[i])))\n",
        "print(lt[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ac19dbb-6418-4840-bcfc-afc173d86e1b",
      "metadata": {
        "id": "6ac19dbb-6418-4840-bcfc-afc173d86e1b"
      },
      "outputs": [],
      "source": [
        "#model testing\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c49814ba-c264-4d2c-a928-131ccd987f05",
      "metadata": {
        "id": "c49814ba-c264-4d2c-a928-131ccd987f05"
      },
      "outputs": [],
      "source": [
        "total_loss = 0.0\n",
        "total_iteration = 0.0\n",
        "loss = []\n",
        "iteration = []\n",
        "for data_for_testing in testing_dataset:\n",
        "    dt,lt = data_for_testing[:-1], data_for_testing[-1]\n",
        "    prediction = loadmodel.forward(torch.from_numpy(dt).view(1,9))\n",
        "    l = abs(lt.item() - prediction.item())\n",
        "    loss.append(l)\n",
        "    total_loss += l\n",
        "    total_iteration += 1\n",
        "    iteration.append(total_iteration)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8a0caea-f70d-477e-b6fa-9c705877f7a8",
      "metadata": {
        "id": "b8a0caea-f70d-477e-b6fa-9c705877f7a8",
        "outputId": "08ebb5e8-b07f-41b7-b550-94c671e75d05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "40697.914004597726\n",
            "99912.0\n",
            "0.40733759713145296\n"
          ]
        }
      ],
      "source": [
        "#average loss\n",
        "print(total_loss)\n",
        "print(total_iteration)\n",
        "print(total_loss/total_iteration)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f686d3c-81d6-4ca0-b94e-7a986b2ebaa6",
      "metadata": {
        "id": "7f686d3c-81d6-4ca0-b94e-7a986b2ebaa6",
        "outputId": "63245942-3b67-4cee-8d4f-c24fe1096cce"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#generation graph\n",
        "plt.plot(iteration,loss)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef7a9606-ec1d-4842-ba17-4b8a9d1b46e6",
      "metadata": {
        "id": "ef7a9606-ec1d-4842-ba17-4b8a9d1b46e6"
      },
      "outputs": [],
      "source": [
        "#graph by averaging 1000 value\n",
        "avg_loss = []\n",
        "itera = []\n",
        "for i in range(100):\n",
        "    avg = 0.0\n",
        "    for j in range(1000):\n",
        "        avg += loss[i+j]\n",
        "    avg_loss.append(avg/1000)\n",
        "    itera.append(i)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4615282-0fbe-4753-a655-3ad6d4f119e7",
      "metadata": {
        "id": "b4615282-0fbe-4753-a655-3ad6d4f119e7",
        "outputId": "693031b3-d6f7-4cec-bd30-c4f787630633"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x1d61d5287a0>]"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#plottiong it\n",
        "plt.plot(itera,avg_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0259708e-f852-44fa-a98b-b4646e68f3c5",
      "metadata": {
        "id": "0259708e-f852-44fa-a98b-b4646e68f3c5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "4780b2b5-bfa8-4a6b-9962-17b2bf3c4083",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "4780b2b5-bfa8-4a6b-9962-17b2bf3c4083",
        "outputId": "15872290-3c6c-49e8-efde-a654d14aef4c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "for i in range(epoch[-1]):\n",
        "    training_loss[i] = training_loss[i].to(torch.device(\"cpu\")).detach().numpy()\n",
        "plt.plot(epoch,training_loss)\n",
        "plt.plot(epoch,testing_loss)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47403529-1a81-4f89-82a1-9a6709e930cf",
      "metadata": {
        "id": "47403529-1a81-4f89-82a1-9a6709e930cf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "script = torch.jit.script(model)\n",
        "script.save(\"agriculture_yield.pt\")"
      ],
      "metadata": {
        "id": "PM1fbFeN1v4O"
      },
      "id": "PM1fbFeN1v4O",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5jRUmeu-2LOs"
      },
      "id": "5jRUmeu-2LOs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier"
      ],
      "metadata": {
        "id": "hcJZ58GE2UDD"
      },
      "id": "hcJZ58GE2UDD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = DecisionTreeRegressor(max_depth=10)"
      ],
      "metadata": {
        "id": "M9gprBRF2d0L"
      },
      "id": "M9gprBRF2d0L",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = np.loadtxt(\"dataset.csv\",dtype=np.float32,delimiter=',')"
      ],
      "metadata": {
        "id": "71holVHY2igi"
      },
      "id": "71holVHY2igi",
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = dataset[:900000, :-1]\n",
        "y_train = dataset[:900000, -1]"
      ],
      "metadata": {
        "id": "RKlVq2542wOj"
      },
      "id": "RKlVq2542wOj",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Z8mYtQff-tJo"
      },
      "id": "Z8mYtQff-tJo"
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(x_train,y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "bFr6taaS28LD",
        "outputId": "b3654dff-493c-4c3c-e504-66b81ec77007"
      },
      "id": "bFr6taaS28LD",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestRegressor(max_depth=9, n_estimators=10)"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {\n",
              "  /* Definition of color scheme common for light and dark mode */\n",
              "  --sklearn-color-text: #000;\n",
              "  --sklearn-color-text-muted: #666;\n",
              "  --sklearn-color-line: gray;\n",
              "  /* Definition of color scheme for unfitted estimators */\n",
              "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
              "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
              "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
              "  --sklearn-color-unfitted-level-3: chocolate;\n",
              "  /* Definition of color scheme for fitted estimators */\n",
              "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
              "  --sklearn-color-fitted-level-1: #d4ebff;\n",
              "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
              "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
              "\n",
              "  /* Specific color for light theme */\n",
              "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
              "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-icon: #696969;\n",
              "\n",
              "  @media (prefers-color-scheme: dark) {\n",
              "    /* Redefinition of color scheme for dark theme */\n",
              "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
              "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-icon: #878787;\n",
              "  }\n",
              "}\n",
              "\n",
              "#sk-container-id-1 {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 pre {\n",
              "  padding: 0;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-hidden--visually {\n",
              "  border: 0;\n",
              "  clip: rect(1px 1px 1px 1px);\n",
              "  clip: rect(1px, 1px, 1px, 1px);\n",
              "  height: 1px;\n",
              "  margin: -1px;\n",
              "  overflow: hidden;\n",
              "  padding: 0;\n",
              "  position: absolute;\n",
              "  width: 1px;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-dashed-wrapped {\n",
              "  border: 1px dashed var(--sklearn-color-line);\n",
              "  margin: 0 0.4em 0.5em 0.4em;\n",
              "  box-sizing: border-box;\n",
              "  padding-bottom: 0.4em;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-container {\n",
              "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
              "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
              "     so we also need the `!important` here to be able to override the\n",
              "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
              "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
              "  display: inline-block !important;\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-text-repr-fallback {\n",
              "  display: none;\n",
              "}\n",
              "\n",
              "div.sk-parallel-item,\n",
              "div.sk-serial,\n",
              "div.sk-item {\n",
              "  /* draw centered vertical line to link estimators */\n",
              "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
              "  background-size: 2px 100%;\n",
              "  background-repeat: no-repeat;\n",
              "  background-position: center center;\n",
              "}\n",
              "\n",
              "/* Parallel-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item::after {\n",
              "  content: \"\";\n",
              "  width: 100%;\n",
              "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
              "  flex-grow: 1;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel {\n",
              "  display: flex;\n",
              "  align-items: stretch;\n",
              "  justify-content: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
              "  align-self: flex-end;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
              "  align-self: flex-start;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
              "  width: 0;\n",
              "}\n",
              "\n",
              "/* Serial-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-serial {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "  align-items: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  padding-right: 1em;\n",
              "  padding-left: 1em;\n",
              "}\n",
              "\n",
              "\n",
              "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
              "clickable and can be expanded/collapsed.\n",
              "- Pipeline and ColumnTransformer use this feature and define the default style\n",
              "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
              "*/\n",
              "\n",
              "/* Pipeline and ColumnTransformer style (default) */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable {\n",
              "  /* Default theme specific background. It is overwritten whether we have a\n",
              "  specific estimator or a Pipeline/ColumnTransformer */\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "/* Toggleable label */\n",
              "#sk-container-id-1 label.sk-toggleable__label {\n",
              "  cursor: pointer;\n",
              "  display: flex;\n",
              "  width: 100%;\n",
              "  margin-bottom: 0;\n",
              "  padding: 0.5em;\n",
              "  box-sizing: border-box;\n",
              "  text-align: center;\n",
              "  align-items: start;\n",
              "  justify-content: space-between;\n",
              "  gap: 0.5em;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
              "  font-size: 0.6rem;\n",
              "  font-weight: lighter;\n",
              "  color: var(--sklearn-color-text-muted);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
              "  /* Arrow on the left of the label */\n",
              "  content: \"▸\";\n",
              "  float: left;\n",
              "  margin-right: 0.25em;\n",
              "  color: var(--sklearn-color-icon);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "/* Toggleable content - dropdown */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content {\n",
              "  max-height: 0;\n",
              "  max-width: 0;\n",
              "  overflow: hidden;\n",
              "  text-align: left;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content pre {\n",
              "  margin: 0.2em;\n",
              "  border-radius: 0.25em;\n",
              "  color: var(--sklearn-color-text);\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
              "  /* Expand drop-down */\n",
              "  max-height: 200px;\n",
              "  max-width: 100%;\n",
              "  overflow: auto;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
              "  content: \"▾\";\n",
              "}\n",
              "\n",
              "/* Pipeline/ColumnTransformer-specific style */\n",
              "\n",
              "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator-specific style */\n",
              "\n",
              "/* Colorize estimator box */\n",
              "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  /* The background is the default theme color */\n",
              "  color: var(--sklearn-color-text-on-default-background);\n",
              "}\n",
              "\n",
              "/* On hover, darken the color of the background */\n",
              "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "/* Label box, darken color on hover, fitted */\n",
              "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator label */\n",
              "\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  font-family: monospace;\n",
              "  font-weight: bold;\n",
              "  display: inline-block;\n",
              "  line-height: 1.2em;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label-container {\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "/* Estimator-specific */\n",
              "#sk-container-id-1 div.sk-estimator {\n",
              "  font-family: monospace;\n",
              "  border: 1px dotted var(--sklearn-color-border-box);\n",
              "  border-radius: 0.25em;\n",
              "  box-sizing: border-box;\n",
              "  margin-bottom: 0.5em;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "/* on hover */\n",
              "#sk-container-id-1 div.sk-estimator:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
              "\n",
              "/* Common style for \"i\" and \"?\" */\n",
              "\n",
              ".sk-estimator-doc-link,\n",
              "a:link.sk-estimator-doc-link,\n",
              "a:visited.sk-estimator-doc-link {\n",
              "  float: right;\n",
              "  font-size: smaller;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1em;\n",
              "  height: 1em;\n",
              "  width: 1em;\n",
              "  text-decoration: none !important;\n",
              "  margin-left: 0.5em;\n",
              "  text-align: center;\n",
              "  /* unfitted */\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted,\n",
              "a:link.sk-estimator-doc-link.fitted,\n",
              "a:visited.sk-estimator-doc-link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "/* Span, style for the box shown on hovering the info icon */\n",
              ".sk-estimator-doc-link span {\n",
              "  display: none;\n",
              "  z-index: 9999;\n",
              "  position: relative;\n",
              "  font-weight: normal;\n",
              "  right: .2ex;\n",
              "  padding: .5ex;\n",
              "  margin: .5ex;\n",
              "  width: min-content;\n",
              "  min-width: 20ex;\n",
              "  max-width: 50ex;\n",
              "  color: var(--sklearn-color-text);\n",
              "  box-shadow: 2pt 2pt 4pt #999;\n",
              "  /* unfitted */\n",
              "  background: var(--sklearn-color-unfitted-level-0);\n",
              "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted span {\n",
              "  /* fitted */\n",
              "  background: var(--sklearn-color-fitted-level-0);\n",
              "  border: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link:hover span {\n",
              "  display: block;\n",
              "}\n",
              "\n",
              "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link {\n",
              "  float: right;\n",
              "  font-size: 1rem;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1rem;\n",
              "  height: 1rem;\n",
              "  width: 1rem;\n",
              "  text-decoration: none;\n",
              "  /* unfitted */\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "#sk-container-id-1 a.estimator_doc_link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestRegressor(max_depth=9, n_estimators=10)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>RandomForestRegressor</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.ensemble.RandomForestRegressor.html\">?<span>Documentation for RandomForestRegressor</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>RandomForestRegressor(max_depth=9, n_estimators=10)</pre></div> </div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_test = dataset[900000:, :-1]\n",
        "y_test = dataset[900000:, -1]"
      ],
      "metadata": {
        "id": "hEZ1Ofh83BCD"
      },
      "id": "hEZ1Ofh83BCD",
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.score(x_test,y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eqY65uIWlLDd",
        "outputId": "a3046383-76fc-4a34-e0f6-0220c55cf40c"
      },
      "id": "eqY65uIWlLDd",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9126390635478924"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ITSIYOlRlK_-"
      },
      "id": "ITSIYOlRlK_-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = loadmodel.predict(x_test)\n",
        "loss = np.mean((y_pred-y_test)**2)\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y6UcvQmq3SxS",
        "outputId": "e7116a1a-90fc-4658-99a9-a7ac16e2f6bd"
      },
      "id": "Y6UcvQmq3SxS",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.2507545960646856\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "model = RandomForestRegressor(n_estimators=10, max_depth=9)"
      ],
      "metadata": {
        "id": "pN5-JRLP4OW7"
      },
      "id": "pN5-JRLP4OW7",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "joblib.dump(model,\"agricultural_yield_prediction.rn\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZP2deoh7d7L",
        "outputId": "bc088253-ecc3-47b7-9eeb-ae55cfb0d676"
      },
      "id": "YZP2deoh7d7L",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['agricultural_yield_prediction.rn']"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loadmodel = joblib.load(\"agricultural_yield_prediction.rn\")"
      ],
      "metadata": {
        "id": "qjT6AiYNAdK2"
      },
      "id": "qjT6AiYNAdK2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IGotBGkQAtoy"
      },
      "id": "IGotBGkQAtoy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for crop price prediction"
      ],
      "metadata": {
        "id": "_svE3XWArAl_"
      },
      "id": "_svE3XWArAl_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZqbhgNM2rKMU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset"
      ],
      "id": "ZqbhgNM2rKMU"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-nVW8bVerKMV"
      },
      "outputs": [],
      "source": [
        "dataset = np.loadtxt(\"dataset.csv\", delimiter=',', dtype = np.float32)"
      ],
      "id": "-nVW8bVerKMV"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "010a456d-11d3-49c5-c341-86a95b5dd4d9",
        "id": "tcatgRvprKMV"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4452"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "len(dataset)"
      ],
      "id": "tcatgRvprKMV"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6pTtB_l5rKMW"
      },
      "outputs": [],
      "source": [
        "training_dataset = dataset[0:4000]\n",
        "testing_dataset = dataset[4000:]"
      ],
      "id": "6pTtB_l5rKMW"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q9qhwjP8rKMW"
      },
      "outputs": [],
      "source": [
        "class dataset (Dataset):\n",
        "    def __init__(self,data,label):\n",
        "        self.data = data\n",
        "        self.label = label\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    def __getitem__(self,idx):\n",
        "        return self.data[idx],self.label[idx]\n",
        "data_set = dataset(training_dataset[:,:-1],training_dataset[:,-1])\n",
        "dataloader = DataLoader(data_set,batch_size=1000,shuffle = True)"
      ],
      "id": "Q9qhwjP8rKMW"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2rBZlhpsrKMW"
      },
      "outputs": [],
      "source": [
        "#creating the model for prediction part\n",
        "class Prediction_Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Prediction_Model,self).__init__()\n",
        "        self.l1 = nn.Linear(2,100)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.l2 = nn.Linear(100,60)\n",
        "        self.l3 = nn.Linear(60,60)\n",
        "        self.l4 = nn.Linear(60,60)\n",
        "        self.l5 = nn.Linear(60,1)\n",
        "    def forward(self,x):\n",
        "        out = self.l1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.l2(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.l3(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.l4(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.l5(out)\n",
        "        return out\n",
        "\n",
        "model = Prediction_Model()"
      ],
      "id": "2rBZlhpsrKMW"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DJGJKqZarKMW"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\")\n",
        "model = model.to(device)"
      ],
      "id": "DJGJKqZarKMW"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "cc42660c-aef7-434e-86ac-94f5eced53e9",
        "id": "HzLqzBXBrKMX"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch : 2 training loss : 2531617.5 testing loss : 1100.5271874294674\n",
            "epoch : 3 training loss : 2527783.75 testing loss : 1098.227044017425\n",
            "epoch : 4 training loss : 2522592.5 testing loss : 1094.3267154289426\n",
            "epoch : 5 training loss : 2513596.75 testing loss : 1087.438235303865\n",
            "epoch : 6 training loss : 2497389.5 testing loss : 1075.3031495367795\n",
            "epoch : 7 training loss : 2469774.75 testing loss : 1054.7885275872145\n",
            "epoch : 8 training loss : 2425842.5 testing loss : 1022.9537800218152\n",
            "epoch : 9 training loss : 2354202.5 testing loss : 979.3769785076116\n",
            "epoch : 10 training loss : 2253210.0 testing loss : 934.4177642754748\n",
            "epoch : 11 training loss : 2117510.5 testing loss : 892.9474172549965\n",
            "epoch : 12 training loss : 1954779.5 testing loss : 870.4443577622945\n",
            "epoch : 13 training loss : 1805589.0 testing loss : 906.2708033418234\n",
            "epoch : 14 training loss : 1737887.125 testing loss : 988.854312595013\n",
            "epoch : 15 training loss : 1766066.25 testing loss : 1023.1825112937826\n",
            "epoch : 16 training loss : 1755199.0 testing loss : 980.7280804182576\n",
            "epoch : 17 training loss : 1714063.5 testing loss : 927.911219970315\n",
            "epoch : 18 training loss : 1697983.875 testing loss : 898.5957244413089\n",
            "epoch : 19 training loss : 1695751.5 testing loss : 890.9834879795008\n",
            "epoch : 20 training loss : 1688274.5 testing loss : 897.9029908412326\n",
            "epoch : 21 training loss : 1676970.375 testing loss : 915.6907011998438\n",
            "epoch : 22 training loss : 1666179.5 testing loss : 929.4300046958755\n",
            "epoch : 23 training loss : 1661757.0 testing loss : 933.6122577865567\n",
            "epoch : 24 training loss : 1656917.5 testing loss : 928.4169250019884\n",
            "epoch : 25 training loss : 1651900.375 testing loss : 917.7717940511957\n",
            "epoch : 26 training loss : 1647564.375 testing loss : 911.1960892782802\n",
            "epoch : 27 training loss : 1644755.25 testing loss : 908.495017653018\n",
            "epoch : 28 training loss : 1642215.25 testing loss : 910.1159529960261\n",
            "epoch : 29 training loss : 1640491.375 testing loss : 915.427164721278\n",
            "epoch : 30 training loss : 1638680.75 testing loss : 915.4958492274833\n",
            "epoch : 31 training loss : 1637785.25 testing loss : 912.3654652844488\n",
            "epoch : 32 training loss : 1636658.25 testing loss : 912.8941806924026\n",
            "epoch : 33 training loss : 1635925.25 testing loss : 911.0842105624951\n",
            "epoch : 34 training loss : 1635312.0 testing loss : 910.4091121855035\n",
            "epoch : 35 training loss : 1634802.375 testing loss : 909.0561777870212\n",
            "epoch : 36 training loss : 1634261.0 testing loss : 908.571210048895\n",
            "epoch : 37 training loss : 1633846.5 testing loss : 912.0774921294862\n",
            "epoch : 38 training loss : 1633094.5 testing loss : 909.7074727737798\n",
            "epoch : 39 training loss : 1632315.875 testing loss : 908.6326179820879\n",
            "epoch : 40 training loss : 1631564.75 testing loss : 907.1728109912535\n",
            "epoch : 41 training loss : 1630626.75 testing loss : 909.0908351471994\n",
            "epoch : 42 training loss : 1630123.5 testing loss : 911.5744289107027\n",
            "epoch : 43 training loss : 1629775.25 testing loss : 911.9764443097916\n",
            "epoch : 44 training loss : 1629217.5 testing loss : 911.0910643666191\n",
            "epoch : 45 training loss : 1629183.5 testing loss : 903.0590751677488\n",
            "epoch : 46 training loss : 1628196.5 testing loss : 904.95359303255\n",
            "epoch : 47 training loss : 1627714.75 testing loss : 908.2937145718431\n",
            "epoch : 48 training loss : 1627073.75 testing loss : 908.5164724176964\n",
            "epoch : 49 training loss : 1626662.5 testing loss : 908.5646579434386\n",
            "epoch : 50 training loss : 1626397.5 testing loss : 904.5326768212614\n",
            "epoch : 51 training loss : 1625855.5 testing loss : 909.1033347454746\n",
            "epoch : 52 training loss : 1625154.75 testing loss : 909.1820136614606\n",
            "epoch : 53 training loss : 1624620.5 testing loss : 905.5238626340849\n",
            "epoch : 54 training loss : 1624121.125 testing loss : 904.4456351436345\n",
            "epoch : 55 training loss : 1623636.25 testing loss : 907.3073104398441\n",
            "epoch : 56 training loss : 1623108.5 testing loss : 905.7215184806722\n",
            "epoch : 57 training loss : 1622480.25 testing loss : 904.7047510716767\n",
            "epoch : 58 training loss : 1622008.25 testing loss : 907.0380595025763\n",
            "epoch : 59 training loss : 1621566.75 testing loss : 903.279370483044\n",
            "epoch : 60 training loss : 1620990.5 testing loss : 907.3777941286037\n",
            "epoch : 61 training loss : 1620011.875 testing loss : 904.4816694998108\n",
            "epoch : 62 training loss : 1619064.375 testing loss : 904.2423657590309\n",
            "epoch : 63 training loss : 1617812.0 testing loss : 903.916237179157\n",
            "epoch : 64 training loss : 1615844.75 testing loss : 905.2803475181613\n",
            "epoch : 65 training loss : 1614412.0 testing loss : 907.2367067737916\n",
            "epoch : 66 training loss : 1613371.25 testing loss : 905.0088645441342\n",
            "epoch : 67 training loss : 1612299.25 testing loss : 897.5012123289362\n",
            "epoch : 68 training loss : 1611413.375 testing loss : 898.5849183660692\n",
            "epoch : 69 training loss : 1610368.0 testing loss : 903.5648561811025\n",
            "epoch : 70 training loss : 1608651.0 testing loss : 902.3960414122691\n",
            "epoch : 71 training loss : 1606717.25 testing loss : 902.1785666014241\n",
            "epoch : 72 training loss : 1604138.5 testing loss : 897.449660813914\n",
            "epoch : 73 training loss : 1600987.0 testing loss : 899.9670434441186\n",
            "epoch : 74 training loss : 1598297.75 testing loss : 900.9250581095704\n",
            "epoch : 75 training loss : 1596799.875 testing loss : 902.7425309219192\n",
            "epoch : 76 training loss : 1594539.75 testing loss : 898.5440234610464\n",
            "epoch : 77 training loss : 1590910.125 testing loss : 886.1920630509875\n",
            "epoch : 78 training loss : 1587553.0 testing loss : 895.2323503726352\n",
            "epoch : 79 training loss : 1583648.625 testing loss : 899.0744990099848\n",
            "epoch : 80 training loss : 1580773.0 testing loss : 896.4786941034604\n",
            "epoch : 81 training loss : 1578461.125 testing loss : 888.8277327592394\n",
            "epoch : 82 training loss : 1575352.75 testing loss : 890.730018546096\n",
            "epoch : 83 training loss : 1572374.375 testing loss : 889.3920973022427\n",
            "epoch : 84 training loss : 1569128.75 testing loss : 892.1237761552355\n",
            "epoch : 85 training loss : 1564768.0 testing loss : 882.9359942727384\n",
            "epoch : 86 training loss : 1559926.75 testing loss : 889.800191826525\n",
            "epoch : 87 training loss : 1553547.0 testing loss : 886.1543583806637\n",
            "epoch : 88 training loss : 1547970.75 testing loss : 882.5663652989716\n",
            "epoch : 89 training loss : 1541494.0 testing loss : 878.1084458321596\n",
            "epoch : 90 training loss : 1535617.5 testing loss : 883.433179870116\n",
            "epoch : 91 training loss : 1528882.875 testing loss : 869.3183421561148\n",
            "epoch : 92 training loss : 1522929.75 testing loss : 875.2550468086141\n",
            "epoch : 93 training loss : 1515175.125 testing loss : 867.778893055114\n",
            "epoch : 94 training loss : 1507230.5 testing loss : 873.0060030485677\n",
            "epoch : 95 training loss : 1497485.75 testing loss : 861.3081825032698\n",
            "epoch : 96 training loss : 1486031.375 testing loss : 862.26480500677\n",
            "epoch : 97 training loss : 1475988.625 testing loss : 849.8663463740222\n",
            "epoch : 98 training loss : 1462404.75 testing loss : 863.5530611540364\n",
            "epoch : 99 training loss : 1446498.5 testing loss : 838.0566363018171\n",
            "epoch : 100 training loss : 1428576.25 testing loss : 856.2561399725686\n",
            "epoch : 101 training loss : 1413450.25 testing loss : 828.9119293731926\n",
            "epoch : 102 training loss : 1398991.75 testing loss : 840.8272073100098\n",
            "epoch : 103 training loss : 1382395.25 testing loss : 809.1954478014887\n",
            "epoch : 104 training loss : 1367047.25 testing loss : 848.0005401320162\n",
            "epoch : 105 training loss : 1355270.0 testing loss : 805.5928233977968\n",
            "epoch : 106 training loss : 1336512.625 testing loss : 824.901540163344\n",
            "epoch : 107 training loss : 1324125.0 testing loss : 804.4572232655719\n",
            "epoch : 108 training loss : 1313045.25 testing loss : 827.6703772059584\n",
            "epoch : 109 training loss : 1306578.0 testing loss : 783.6607412131486\n",
            "epoch : 110 training loss : 1298819.75 testing loss : 841.0939641610711\n",
            "epoch : 111 training loss : 1292051.75 testing loss : 779.8080852791271\n",
            "epoch : 112 training loss : 1289648.75 testing loss : 834.6174309633475\n",
            "epoch : 113 training loss : 1285877.0 testing loss : 785.7444668639023\n",
            "epoch : 114 training loss : 1280442.125 testing loss : 820.2133778238718\n",
            "epoch : 115 training loss : 1276306.25 testing loss : 786.2918146162962\n",
            "epoch : 116 training loss : 1277058.25 testing loss : 820.4783045654804\n",
            "epoch : 117 training loss : 1277071.75 testing loss : 815.5759477172278\n",
            "epoch : 118 training loss : 1273246.0 testing loss : 794.3749917608446\n",
            "epoch : 119 training loss : 1273750.5 testing loss : 833.8654504940573\n",
            "epoch : 120 training loss : 1276109.625 testing loss : 780.6171700785645\n",
            "epoch : 121 training loss : 1271735.125 testing loss : 826.8455915176763\n",
            "epoch : 122 training loss : 1270575.0 testing loss : 797.2203402202741\n",
            "epoch : 123 training loss : 1267236.875 testing loss : 825.7030657856865\n",
            "epoch : 124 training loss : 1266856.75 testing loss : 786.5143052476698\n",
            "epoch : 125 training loss : 1265934.375 testing loss : 843.9882800325884\n",
            "epoch : 126 training loss : 1266109.0 testing loss : 773.0221278751845\n",
            "epoch : 127 training loss : 1270000.5 testing loss : 852.6804074291634\n",
            "epoch : 128 training loss : 1272104.75 testing loss : 778.0449264640301\n",
            "epoch : 129 training loss : 1267353.5 testing loss : 819.3405548344671\n",
            "epoch : 130 training loss : 1264243.5 testing loss : 793.1285925649963\n",
            "epoch : 131 training loss : 1264887.75 testing loss : 801.9851038413765\n",
            "epoch : 132 training loss : 1262703.125 testing loss : 793.0713555644044\n",
            "epoch : 133 training loss : 1266509.125 testing loss : 829.616329697381\n",
            "epoch : 134 training loss : 1262158.25 testing loss : 776.8990818779025\n",
            "epoch : 135 training loss : 1260329.125 testing loss : 827.7559638888436\n",
            "epoch : 136 training loss : 1260922.25 testing loss : 788.4679160307994\n",
            "epoch : 137 training loss : 1260946.75 testing loss : 811.5517653722678\n",
            "epoch : 138 training loss : 1258110.5 testing loss : 790.4253361098534\n",
            "epoch : 139 training loss : 1260309.75 testing loss : 793.1673851329668\n",
            "epoch : 140 training loss : 1260395.5 testing loss : 808.4003066995502\n",
            "epoch : 141 training loss : 1257386.375 testing loss : 805.3258942439493\n",
            "epoch : 142 training loss : 1257036.5 testing loss : 795.7822954274911\n",
            "epoch : 143 training loss : 1256296.875 testing loss : 810.3154915307475\n",
            "epoch : 144 training loss : 1258224.75 testing loss : 789.3321608125636\n",
            "epoch : 145 training loss : 1261931.25 testing loss : 800.1603941769727\n",
            "epoch : 146 training loss : 1263628.625 testing loss : 806.338453577683\n",
            "epoch : 147 training loss : 1256449.0 testing loss : 783.1425515212844\n",
            "epoch : 148 training loss : 1259849.25 testing loss : 817.8516811923644\n",
            "epoch : 149 training loss : 1254749.875 testing loss : 777.2305459870702\n",
            "epoch : 150 training loss : 1253479.125 testing loss : 822.2670996315711\n",
            "epoch : 151 training loss : 1250840.875 testing loss : 783.0249685080705\n",
            "epoch : 152 training loss : 1251850.625 testing loss : 806.2198281056058\n",
            "epoch : 153 training loss : 1249391.125 testing loss : 792.094129374597\n",
            "epoch : 154 training loss : 1252038.25 testing loss : 808.8204482065893\n",
            "epoch : 155 training loss : 1247315.625 testing loss : 783.1812330241752\n",
            "epoch : 156 training loss : 1247195.25 testing loss : 811.4729117988485\n",
            "epoch : 157 training loss : 1246740.125 testing loss : 796.2174213354566\n",
            "epoch : 158 training loss : 1248919.625 testing loss : 801.5573485361791\n",
            "epoch : 159 training loss : 1252731.0 testing loss : 801.1663480754447\n",
            "epoch : 160 training loss : 1248447.625 testing loss : 785.1510244027703\n",
            "epoch : 161 training loss : 1245485.75 testing loss : 822.9878646019285\n",
            "epoch : 162 training loss : 1242925.375 testing loss : 774.9797268061511\n",
            "epoch : 163 training loss : 1243474.25 testing loss : 809.5237876398373\n",
            "epoch : 164 training loss : 1241414.5 testing loss : 796.8604537807735\n",
            "epoch : 165 training loss : 1240752.375 testing loss : 801.7110578802834\n",
            "epoch : 166 training loss : 1239274.5 testing loss : 793.3170410405218\n",
            "epoch : 167 training loss : 1238465.0 testing loss : 797.1007304212688\n",
            "epoch : 168 training loss : 1238662.875 testing loss : 802.2920054861929\n",
            "epoch : 169 training loss : 1235992.0 testing loss : 785.2212176006452\n",
            "epoch : 170 training loss : 1237143.375 testing loss : 806.9584202027954\n",
            "epoch : 171 training loss : 1234218.375 testing loss : 784.2660895807553\n",
            "epoch : 172 training loss : 1234244.125 testing loss : 807.9053027376665\n",
            "epoch : 173 training loss : 1232855.125 testing loss : 789.785759577709\n",
            "epoch : 174 training loss : 1232217.25 testing loss : 794.4470129751526\n",
            "epoch : 175 training loss : 1230303.75 testing loss : 797.5490011683607\n",
            "epoch : 176 training loss : 1229421.625 testing loss : 796.3527696301452\n",
            "epoch : 177 training loss : 1228023.75 testing loss : 792.3388140496954\n",
            "epoch : 178 training loss : 1228495.375 testing loss : 784.2101888044746\n",
            "epoch : 179 training loss : 1227670.0 testing loss : 803.8486440181732\n",
            "epoch : 180 training loss : 1225804.375 testing loss : 794.4369712740975\n",
            "epoch : 181 training loss : 1223333.75 testing loss : 787.5968578621349\n",
            "epoch : 182 training loss : 1223416.625 testing loss : 788.6595922128289\n",
            "epoch : 183 training loss : 1221789.5 testing loss : 806.713803247013\n",
            "epoch : 184 training loss : 1223673.125 testing loss : 782.5632804917024\n",
            "epoch : 185 training loss : 1220499.125 testing loss : 798.2547761339002\n",
            "epoch : 186 training loss : 1216751.0 testing loss : 791.3980962681559\n",
            "epoch : 187 training loss : 1212194.25 testing loss : 781.3239072930496\n",
            "epoch : 188 training loss : 1211551.5 testing loss : 785.2855309659401\n",
            "epoch : 189 training loss : 1209614.75 testing loss : 791.325092752423\n",
            "epoch : 190 training loss : 1208567.75 testing loss : 787.9915068613744\n",
            "epoch : 191 training loss : 1208059.5 testing loss : 790.8883486026157\n",
            "epoch : 192 training loss : 1206780.5 testing loss : 791.3963034279578\n",
            "epoch : 193 training loss : 1207344.25 testing loss : 778.097704387344\n",
            "epoch : 194 training loss : 1203719.5 testing loss : 790.3591877063818\n",
            "epoch : 195 training loss : 1203021.75 testing loss : 785.0207762570508\n",
            "epoch : 196 training loss : 1200146.625 testing loss : 782.0454400640673\n",
            "epoch : 197 training loss : 1199937.75 testing loss : 777.5115669702006\n",
            "epoch : 198 training loss : 1199430.875 testing loss : 800.0174946721676\n",
            "epoch : 199 training loss : 1199761.25 testing loss : 790.1787593449111\n",
            "epoch : 200 training loss : 1195579.75 testing loss : 791.9211808580213\n",
            "epoch : 201 training loss : 1199408.625 testing loss : 796.0346837655633\n",
            "epoch : 202 training loss : 1198019.5 testing loss : 779.3538345045748\n",
            "epoch : 203 training loss : 1193125.125 testing loss : 783.2310672840185\n",
            "epoch : 204 training loss : 1192260.5 testing loss : 776.9203553980431\n",
            "epoch : 205 training loss : 1192148.125 testing loss : 799.7754821081077\n",
            "epoch : 206 training loss : 1188743.0 testing loss : 772.2694500252209\n",
            "epoch : 207 training loss : 1187705.75 testing loss : 794.2453353320603\n",
            "epoch : 208 training loss : 1184461.875 testing loss : 774.5367602436943\n",
            "epoch : 209 training loss : 1184700.0 testing loss : 793.7631058798427\n",
            "epoch : 210 training loss : 1185378.0 testing loss : 770.2919782094195\n",
            "epoch : 211 training loss : 1185609.375 testing loss : 796.6844221110892\n",
            "epoch : 212 training loss : 1182819.375 testing loss : 772.4232497742746\n",
            "epoch : 213 training loss : 1184817.5 testing loss : 788.3694418151822\n",
            "epoch : 214 training loss : 1180224.5 testing loss : 783.039546188\n",
            "epoch : 215 training loss : 1177461.25 testing loss : 782.5703523327819\n",
            "epoch : 216 training loss : 1174040.125 testing loss : 795.6859310432873\n",
            "epoch : 217 training loss : 1172508.625 testing loss : 759.9947820321648\n",
            "epoch : 218 training loss : 1175448.75 testing loss : 774.2422199059376\n",
            "epoch : 219 training loss : 1169554.0 testing loss : 780.5957764460977\n",
            "epoch : 220 training loss : 1169635.375 testing loss : 772.4613499578121\n",
            "epoch : 221 training loss : 1170701.375 testing loss : 784.969340904624\n",
            "epoch : 222 training loss : 1167715.625 testing loss : 770.7202293577448\n",
            "epoch : 223 training loss : 1165263.625 testing loss : 779.9001022587835\n",
            "epoch : 224 training loss : 1167056.125 testing loss : 766.6578273920886\n",
            "epoch : 225 training loss : 1168503.625 testing loss : 791.4320728652245\n",
            "epoch : 226 training loss : 1168961.25 testing loss : 769.0887470730638\n",
            "epoch : 227 training loss : 1165608.125 testing loss : 773.2326626376769\n",
            "epoch : 228 training loss : 1158533.0 testing loss : 787.5384896856493\n",
            "epoch : 229 training loss : 1162224.5 testing loss : 768.7643323894096\n",
            "epoch : 230 training loss : 1154250.625 testing loss : 788.4150897654812\n",
            "epoch : 231 training loss : 1157861.375 testing loss : 768.6522210699267\n",
            "epoch : 232 training loss : 1153959.875 testing loss : 788.9833902658614\n",
            "epoch : 233 training loss : 1154785.625 testing loss : 768.3567187532915\n",
            "epoch : 234 training loss : 1150928.375 testing loss : 768.4821808380364\n",
            "epoch : 235 training loss : 1146611.0 testing loss : 768.0794484615326\n",
            "epoch : 236 training loss : 1144311.5 testing loss : 769.2904688046042\n",
            "epoch : 237 training loss : 1144517.125 testing loss : 775.0212176816653\n",
            "epoch : 238 training loss : 1141593.0 testing loss : 765.0827566480215\n",
            "epoch : 239 training loss : 1140739.0 testing loss : 781.631528126455\n",
            "epoch : 240 training loss : 1138976.25 testing loss : 761.6170591485184\n",
            "epoch : 241 training loss : 1137270.75 testing loss : 770.4213507977207\n",
            "epoch : 242 training loss : 1134736.875 testing loss : 767.6250588725098\n",
            "epoch : 243 training loss : 1133356.0 testing loss : 765.8137859897276\n",
            "epoch : 244 training loss : 1130761.125 testing loss : 770.5847049118144\n",
            "epoch : 245 training loss : 1132006.875 testing loss : 771.0079324076661\n",
            "epoch : 246 training loss : 1131577.75 testing loss : 750.4757405487837\n",
            "epoch : 247 training loss : 1129024.875 testing loss : 780.4918820520418\n",
            "epoch : 248 training loss : 1129269.125 testing loss : 748.2858070419953\n",
            "epoch : 249 training loss : 1124703.25 testing loss : 769.4355286433633\n",
            "epoch : 250 training loss : 1122353.0 testing loss : 750.4499984990179\n",
            "epoch : 251 training loss : 1119519.125 testing loss : 783.5260587688041\n",
            "epoch : 252 training loss : 1119673.0 testing loss : 747.1401537983818\n",
            "epoch : 253 training loss : 1115727.25 testing loss : 771.3364571744362\n",
            "epoch : 254 training loss : 1115625.5 testing loss : 748.5353311163134\n",
            "epoch : 255 training loss : 1114154.25 testing loss : 780.9995849048142\n",
            "epoch : 256 training loss : 1112947.875 testing loss : 736.1063077787383\n",
            "epoch : 257 training loss : 1114887.5 testing loss : 781.0406402971893\n",
            "epoch : 258 training loss : 1120475.875 testing loss : 740.5857155027643\n",
            "epoch : 259 training loss : 1114635.875 testing loss : 758.8141217506037\n",
            "epoch : 260 training loss : 1110295.0 testing loss : 749.9623546072867\n",
            "epoch : 261 training loss : 1111353.375 testing loss : 772.3073360455775\n",
            "epoch : 262 training loss : 1105832.875 testing loss : 749.1106347594641\n",
            "epoch : 263 training loss : 1102789.875 testing loss : 748.2710465266641\n",
            "epoch : 264 training loss : 1098567.375 testing loss : 761.0891176388327\n",
            "epoch : 265 training loss : 1096488.5 testing loss : 739.5152396121912\n",
            "epoch : 266 training loss : 1096526.25 testing loss : 747.3005666120918\n",
            "epoch : 267 training loss : 1094811.25 testing loss : 735.5446335763003\n",
            "epoch : 268 training loss : 1093392.5 testing loss : 749.0261805690495\n",
            "epoch : 269 training loss : 1092121.25 testing loss : 729.5200093678668\n",
            "epoch : 270 training loss : 1091534.625 testing loss : 752.9313024651688\n",
            "epoch : 271 training loss : 1087203.625 testing loss : 742.3373210514541\n",
            "epoch : 272 training loss : 1086017.375 testing loss : 748.0462697100851\n",
            "epoch : 273 training loss : 1085166.5 testing loss : 733.5667981151986\n",
            "epoch : 274 training loss : 1082610.875 testing loss : 757.0857122957179\n",
            "epoch : 275 training loss : 1081964.125 testing loss : 738.577089965871\n",
            "epoch : 276 training loss : 1082813.75 testing loss : 744.0394655054649\n",
            "epoch : 277 training loss : 1081589.625 testing loss : 750.6984131040826\n",
            "epoch : 278 training loss : 1079461.875 testing loss : 737.0026085756522\n",
            "epoch : 279 training loss : 1077398.0 testing loss : 733.2509362528809\n",
            "epoch : 280 training loss : 1073999.5 testing loss : 729.4296642214852\n",
            "epoch : 281 training loss : 1072337.25 testing loss : 755.4404946702772\n",
            "epoch : 282 training loss : 1073851.0 testing loss : 723.4485874998886\n",
            "epoch : 283 training loss : 1072130.875 testing loss : 751.694583333699\n",
            "epoch : 284 training loss : 1070151.0 testing loss : 731.6134058838397\n",
            "epoch : 285 training loss : 1069083.75 testing loss : 752.4767362354077\n",
            "epoch : 286 training loss : 1070139.25 testing loss : 733.868309440866\n",
            "epoch : 287 training loss : 1066088.625 testing loss : 753.7909700490732\n",
            "epoch : 288 training loss : 1064133.25 testing loss : 712.5674037405875\n",
            "epoch : 289 training loss : 1077358.375 testing loss : 783.3549297277906\n",
            "epoch : 290 training loss : 1082719.5 testing loss : 714.9358173112954\n",
            "epoch : 291 training loss : 1077666.875 testing loss : 751.3215128615894\n",
            "epoch : 292 training loss : 1067803.625 testing loss : 718.9312936879892\n",
            "epoch : 293 training loss : 1061149.625 testing loss : 754.5073202415905\n",
            "epoch : 294 training loss : 1058726.625 testing loss : 713.9719243408305\n",
            "epoch : 295 training loss : 1058152.125 testing loss : 753.1933345435995\n",
            "epoch : 296 training loss : 1059904.25 testing loss : 720.268019623461\n",
            "epoch : 297 training loss : 1054367.5 testing loss : 740.3931759078946\n",
            "epoch : 298 training loss : 1054022.625 testing loss : 708.3857774713398\n",
            "epoch : 299 training loss : 1056453.25 testing loss : 758.6870181813704\n",
            "epoch : 300 training loss : 1056029.5 testing loss : 703.8047027313604\n",
            "epoch : 301 training loss : 1058723.625 testing loss : 746.9226141798813\n",
            "epoch : 302 training loss : 1058404.25 testing loss : 708.1866991646522\n",
            "epoch : 303 training loss : 1048948.75 testing loss : 744.0583319853893\n",
            "epoch : 304 training loss : 1047804.6875 testing loss : 716.0992799923483\n",
            "epoch : 305 training loss : 1050696.75 testing loss : 734.1051564237713\n",
            "epoch : 306 training loss : 1050170.125 testing loss : 732.3677918320209\n",
            "epoch : 307 training loss : 1049086.375 testing loss : 714.3106442236267\n",
            "epoch : 308 training loss : 1047574.875 testing loss : 735.4887174990325\n",
            "epoch : 309 training loss : 1047246.9375 testing loss : 701.3893654135476\n",
            "epoch : 310 training loss : 1052863.0 testing loss : 747.7464082241058\n",
            "epoch : 311 training loss : 1049517.5 testing loss : 724.7006976351274\n",
            "epoch : 312 training loss : 1041904.375 testing loss : 713.6134221680396\n",
            "epoch : 313 training loss : 1040707.25 testing loss : 723.3305708391476\n",
            "epoch : 314 training loss : 1042127.375 testing loss : 703.5471669614842\n",
            "epoch : 315 training loss : 1041608.0 testing loss : 741.3347753149218\n",
            "epoch : 316 training loss : 1039414.875 testing loss : 707.2183793561649\n",
            "epoch : 317 training loss : 1040061.5 testing loss : 723.9942847205474\n",
            "epoch : 318 training loss : 1033483.125 testing loss : 699.1955347926216\n",
            "epoch : 319 training loss : 1038071.875 testing loss : 729.7657585502726\n",
            "epoch : 320 training loss : 1038580.8125 testing loss : 722.0323751268133\n",
            "epoch : 321 training loss : 1034793.6875 testing loss : 707.3947992388127\n",
            "epoch : 322 training loss : 1033864.875 testing loss : 741.5159994796314\n",
            "epoch : 323 training loss : 1034657.0625 testing loss : 679.6456691543613\n",
            "epoch : 324 training loss : 1046842.4375 testing loss : 755.9008071253785\n",
            "epoch : 325 training loss : 1044086.375 testing loss : 698.5739030816914\n",
            "epoch : 326 training loss : 1037558.625 testing loss : 725.8370989719324\n",
            "epoch : 327 training loss : 1035041.3125 testing loss : 715.3261580150739\n",
            "epoch : 328 training loss : 1031644.6875 testing loss : 702.4230787796257\n",
            "epoch : 329 training loss : 1027902.25 testing loss : 728.7288289428813\n",
            "epoch : 330 training loss : 1030404.875 testing loss : 688.1606481666058\n",
            "epoch : 331 training loss : 1029330.25 testing loss : 735.8612500481901\n",
            "epoch : 332 training loss : 1032705.25 testing loss : 687.9663073594592\n",
            "epoch : 333 training loss : 1030971.9375 testing loss : 764.9031682035564\n",
            "epoch : 334 training loss : 1038664.875 testing loss : 679.6328422221462\n",
            "epoch : 335 training loss : 1032350.25 testing loss : 767.5092609688244\n",
            "epoch : 336 training loss : 1031345.5 testing loss : 679.6127569464455\n",
            "epoch : 337 training loss : 1033612.9375 testing loss : 740.6766887069804\n",
            "epoch : 338 training loss : 1026456.6875 testing loss : 685.6110698797006\n",
            "epoch : 339 training loss : 1032558.375 testing loss : 741.5656512669757\n",
            "epoch : 340 training loss : 1028514.3125 testing loss : 677.1239676834208\n",
            "epoch : 341 training loss : 1018573.1875 testing loss : 747.7088943983601\n",
            "epoch : 342 training loss : 1025896.875 testing loss : 693.5003257747245\n",
            "epoch : 343 training loss : 1025938.25 testing loss : 716.2156457795506\n",
            "epoch : 344 training loss : 1020166.0 testing loss : 699.9957667641935\n",
            "epoch : 345 training loss : 1021211.875 testing loss : 711.6799499608774\n",
            "epoch : 346 training loss : 1017893.375 testing loss : 690.9874068804547\n",
            "epoch : 347 training loss : 1017097.1875 testing loss : 723.8914591084539\n",
            "epoch : 348 training loss : 1018425.25 testing loss : 702.7482419203868\n",
            "epoch : 349 training loss : 1015495.3125 testing loss : 709.9651315803021\n",
            "epoch : 350 training loss : 1017766.5625 testing loss : 697.0871294051145\n",
            "epoch : 351 training loss : 1015158.75 testing loss : 719.1709576138353\n",
            "epoch : 352 training loss : 1014873.625 testing loss : 686.5302394406986\n",
            "epoch : 353 training loss : 1018473.0 testing loss : 746.4219134149298\n",
            "epoch : 354 training loss : 1027565.5625 testing loss : 678.5123478100363\n",
            "epoch : 355 training loss : 1026584.625 testing loss : 728.7528039522931\n",
            "epoch : 356 training loss : 1023104.375 testing loss : 699.2356379411917\n",
            "epoch : 357 training loss : 1019266.25 testing loss : 696.9083008365294\n",
            "epoch : 358 training loss : 1012448.875 testing loss : 693.1888605619954\n",
            "epoch : 359 training loss : 1014908.75 testing loss : 702.8080578504411\n",
            "epoch : 360 training loss : 1015361.1875 testing loss : 713.1651837656983\n",
            "epoch : 361 training loss : 1014373.8125 testing loss : 683.0819497213954\n",
            "epoch : 362 training loss : 1012258.375 testing loss : 715.8841953256489\n",
            "epoch : 363 training loss : 1009329.6875 testing loss : 699.5857032822297\n",
            "epoch : 364 training loss : 1009231.0 testing loss : 694.7201128913238\n",
            "epoch : 365 training loss : 1009541.625 testing loss : 711.2431489197554\n",
            "epoch : 366 training loss : 1011661.1875 testing loss : 701.3255848146118\n",
            "epoch : 367 training loss : 1009590.75 testing loss : 680.5027294306628\n",
            "epoch : 368 training loss : 1010426.625 testing loss : 714.6893272210011\n",
            "epoch : 369 training loss : 1011320.875 testing loss : 691.9735263702089\n",
            "epoch : 370 training loss : 1012208.125 testing loss : 705.5185940139062\n",
            "epoch : 371 training loss : 1013090.875 testing loss : 713.9188671259753\n",
            "epoch : 372 training loss : 1006578.3125 testing loss : 682.7641853716522\n",
            "epoch : 373 training loss : 1007796.5 testing loss : 713.6590508127634\n",
            "epoch : 374 training loss : 1007492.9375 testing loss : 707.5928213216562\n",
            "epoch : 375 training loss : 1006495.25 testing loss : 679.715638673411\n",
            "epoch : 376 training loss : 1005912.9375 testing loss : 705.8159340187511\n",
            "epoch : 377 training loss : 1008314.1875 testing loss : 712.9329889327024\n",
            "epoch : 378 training loss : 1009141.4375 testing loss : 690.269821671258\n",
            "epoch : 379 training loss : 1003198.375 testing loss : 714.3585293335198\n",
            "epoch : 380 training loss : 1003620.625 testing loss : 680.7220502005214\n",
            "epoch : 381 training loss : 1002125.5 testing loss : 717.398948346619\n",
            "epoch : 382 training loss : 1003370.6875 testing loss : 676.4158227950071\n",
            "epoch : 383 training loss : 1002803.375 testing loss : 713.9163277022607\n",
            "epoch : 384 training loss : 1003820.5 testing loss : 674.5612735389608\n",
            "epoch : 385 training loss : 1009614.9375 testing loss : 703.6188029715445\n",
            "epoch : 386 training loss : 999300.0 testing loss : 687.0044768151984\n",
            "epoch : 387 training loss : 1001049.875 testing loss : 680.3478676428837\n",
            "epoch : 388 training loss : 1004079.25 testing loss : 693.2788515998199\n",
            "epoch : 389 training loss : 1000293.375 testing loss : 679.3353747794058\n",
            "epoch : 390 training loss : 1000965.375 testing loss : 699.3348270496435\n",
            "epoch : 391 training loss : 1003243.75 testing loss : 681.3023914045992\n",
            "epoch : 392 training loss : 999181.375 testing loss : 696.2312011022483\n",
            "epoch : 393 training loss : 999404.9375 testing loss : 694.796074316565\n",
            "epoch : 394 training loss : 998028.9375 testing loss : 687.3448395159393\n",
            "epoch : 395 training loss : 997526.875 testing loss : 672.7974738040857\n",
            "epoch : 396 training loss : 1011474.0 testing loss : 705.7905002336587\n",
            "epoch : 397 training loss : 1006970.9375 testing loss : 681.4174646002001\n",
            "epoch : 398 training loss : 1004363.6875 testing loss : 682.5422622186948\n",
            "epoch : 399 training loss : 1002063.25 testing loss : 692.48899999762\n",
            "epoch : 400 training loss : 996682.0625 testing loss : 688.2655815846097\n",
            "epoch : 401 training loss : 997814.5 testing loss : 699.0053478321142\n",
            "epoch : 402 training loss : 1000374.9375 testing loss : 663.9062346040675\n",
            "epoch : 403 training loss : 1020760.75 testing loss : 727.9451197037655\n",
            "epoch : 404 training loss : 1021265.0 testing loss : 681.658171727594\n",
            "epoch : 405 training loss : 1013177.75 testing loss : 724.5704469237708\n",
            "epoch : 406 training loss : 1009661.0625 testing loss : 672.7267676475828\n",
            "epoch : 407 training loss : 1006215.25 testing loss : 709.7780682703035\n",
            "epoch : 408 training loss : 1005649.9375 testing loss : 679.4249366131504\n",
            "epoch : 409 training loss : 999695.875 testing loss : 709.2253196323867\n",
            "epoch : 410 training loss : 1000315.5625 testing loss : 680.1133654075386\n",
            "epoch : 411 training loss : 997430.625 testing loss : 679.0102421562228\n",
            "epoch : 412 training loss : 995493.25 testing loss : 689.8843049222389\n",
            "epoch : 413 training loss : 993409.25 testing loss : 685.9457223921751\n",
            "epoch : 414 training loss : 992444.875 testing loss : 685.88977327389\n",
            "epoch : 415 training loss : 993155.0 testing loss : 673.0494476229744\n",
            "epoch : 416 training loss : 995482.8125 testing loss : 713.3641059314256\n",
            "epoch : 417 training loss : 996890.125 testing loss : 661.8867945017013\n",
            "epoch : 418 training loss : 990185.25 testing loss : 698.5586073862768\n",
            "epoch : 419 training loss : 988856.125 testing loss : 687.0228311298168\n",
            "epoch : 420 training loss : 988018.375 testing loss : 683.3286505146364\n",
            "epoch : 421 training loss : 987767.1875 testing loss : 663.380556593954\n",
            "epoch : 422 training loss : 991310.3125 testing loss : 687.3703618999076\n",
            "epoch : 423 training loss : 988039.875 testing loss : 685.1042901414686\n",
            "epoch : 424 training loss : 987828.125 testing loss : 674.2357175687773\n",
            "epoch : 425 training loss : 987497.625 testing loss : 696.476978029825\n",
            "epoch : 426 training loss : 985132.125 testing loss : 669.7572537628951\n",
            "epoch : 427 training loss : 986865.5 testing loss : 698.0931842749097\n",
            "epoch : 428 training loss : 987483.125 testing loss : 673.150969174056\n",
            "epoch : 429 training loss : 981027.8125 testing loss : 685.6910991141226\n",
            "epoch : 430 training loss : 982630.125 testing loss : 674.5684691889096\n",
            "epoch : 431 training loss : 984650.5 testing loss : 688.9887030251258\n",
            "epoch : 432 training loss : 983468.625 testing loss : 659.1216204124214\n",
            "epoch : 433 training loss : 987650.625 testing loss : 698.0023732122066\n",
            "epoch : 434 training loss : 986219.25 testing loss : 667.9065856237327\n",
            "epoch : 435 training loss : 980802.0 testing loss : 677.756115666533\n",
            "epoch : 436 training loss : 983289.8125 testing loss : 689.082543936451\n",
            "epoch : 437 training loss : 985473.875 testing loss : 665.0064035917806\n",
            "epoch : 438 training loss : 981012.375 testing loss : 702.3884382311222\n",
            "epoch : 439 training loss : 986954.5 testing loss : 656.054817805248\n",
            "epoch : 440 training loss : 986509.25 testing loss : 699.3319089750273\n",
            "epoch : 441 training loss : 989522.25 testing loss : 669.1646129958398\n",
            "epoch : 442 training loss : 980703.0625 testing loss : 693.102579435416\n",
            "epoch : 443 training loss : 982801.8125 testing loss : 652.0793557272548\n",
            "epoch : 444 training loss : 985036.5625 testing loss : 694.2336776847333\n",
            "epoch : 445 training loss : 978101.8125 testing loss : 663.2060331813002\n",
            "epoch : 446 training loss : 975174.125 testing loss : 695.6362040675847\n",
            "epoch : 447 training loss : 979989.0625 testing loss : 663.8137189455792\n",
            "epoch : 448 training loss : 977222.625 testing loss : 681.2144702451419\n",
            "epoch : 449 training loss : 976401.4375 testing loss : 676.5740830391909\n",
            "epoch : 450 training loss : 977398.5 testing loss : 674.6958241736994\n",
            "epoch : 451 training loss : 974964.6875 testing loss : 665.6897415207551\n",
            "epoch : 452 training loss : 975311.5 testing loss : 677.6105433080048\n",
            "epoch : 453 training loss : 978253.625 testing loss : 661.1285253182976\n",
            "epoch : 454 training loss : 977701.125 testing loss : 696.5302894031053\n",
            "epoch : 455 training loss : 984199.875 testing loss : 668.2719935168208\n",
            "epoch : 456 training loss : 977085.3125 testing loss : 679.7450800511689\n",
            "epoch : 457 training loss : 973979.75 testing loss : 677.2480908263046\n",
            "epoch : 458 training loss : 974080.1875 testing loss : 670.269951978616\n",
            "epoch : 459 training loss : 972386.125 testing loss : 667.3760327005808\n",
            "epoch : 460 training loss : 972508.5 testing loss : 675.4057296461764\n",
            "epoch : 461 training loss : 972752.25 testing loss : 654.5003463251401\n",
            "epoch : 462 training loss : 979975.5 testing loss : 694.2075414383306\n",
            "epoch : 463 training loss : 973621.5 testing loss : 652.5510106361018\n",
            "epoch : 464 training loss : 972300.4375 testing loss : 685.0605615577866\n",
            "epoch : 465 training loss : 973890.5 testing loss : 674.321449075125\n",
            "epoch : 466 training loss : 974055.875 testing loss : 655.0281835889394\n",
            "epoch : 467 training loss : 973083.6875 testing loss : 688.4659001489656\n",
            "epoch : 468 training loss : 970897.625 testing loss : 650.4440426425597\n",
            "epoch : 469 training loss : 973893.25 testing loss : 692.9773682429727\n",
            "epoch : 470 training loss : 979217.0625 testing loss : 667.0456077081967\n",
            "epoch : 471 training loss : 975593.1875 testing loss : 663.9386345158636\n",
            "epoch : 472 training loss : 988963.625 testing loss : 692.2885480538934\n",
            "epoch : 473 training loss : 990560.4375 testing loss : 671.2331098349748\n",
            "epoch : 474 training loss : 977976.25 testing loss : 700.0052814631335\n",
            "epoch : 475 training loss : 975007.875 testing loss : 631.9465826232877\n",
            "epoch : 476 training loss : 980885.25 testing loss : 723.6996113241246\n",
            "epoch : 477 training loss : 979766.5 testing loss : 648.6528103224999\n",
            "epoch : 478 training loss : 975292.375 testing loss : 694.8716918451596\n",
            "epoch : 479 training loss : 973968.0 testing loss : 662.6383110632938\n",
            "epoch : 480 training loss : 969321.375 testing loss : 683.3260211417105\n",
            "epoch : 481 training loss : 967543.75 testing loss : 669.5753723055916\n",
            "epoch : 482 training loss : 971046.5 testing loss : 648.3914397227026\n",
            "epoch : 483 training loss : 973844.8125 testing loss : 682.5626902263776\n",
            "epoch : 484 training loss : 976155.75 testing loss : 674.9495575807791\n",
            "epoch : 485 training loss : 975186.375 testing loss : 650.6595991670558\n",
            "epoch : 486 training loss : 970408.5625 testing loss : 669.3400129786635\n",
            "epoch : 487 training loss : 967082.375 testing loss : 668.4711404964987\n",
            "epoch : 488 training loss : 967740.75 testing loss : 684.3428876737578\n",
            "epoch : 489 training loss : 974254.5 testing loss : 658.8752729344156\n",
            "epoch : 490 training loss : 968778.25 testing loss : 661.6121075364341\n",
            "epoch : 491 training loss : 967341.0625 testing loss : 678.3520476733689\n",
            "epoch : 492 training loss : 969654.3125 testing loss : 642.0797405728197\n",
            "epoch : 493 training loss : 973372.5625 testing loss : 695.6990276079263\n",
            "epoch : 494 training loss : 965340.75 testing loss : 649.1059993959107\n",
            "epoch : 495 training loss : 962914.3125 testing loss : 683.6569943364742\n",
            "epoch : 496 training loss : 965241.0625 testing loss : 659.0567806526623\n",
            "epoch : 497 training loss : 967487.0625 testing loss : 678.5984442698217\n",
            "epoch : 498 training loss : 965935.6875 testing loss : 661.1852723159622\n",
            "epoch : 499 training loss : 965851.1875 testing loss : 659.0849371357301\n",
            "epoch : 500 training loss : 963324.5625 testing loss : 674.4333588908204\n",
            "epoch : 501 training loss : 964443.625 testing loss : 653.3996619481956\n",
            "epoch : 502 training loss : 966477.75 testing loss : 686.7835820607379\n",
            "epoch : 503 training loss : 968945.125 testing loss : 642.143644786514\n",
            "epoch : 504 training loss : 968480.5 testing loss : 709.4980969703303\n",
            "epoch : 505 training loss : 977147.875 testing loss : 645.8629404232565\n",
            "epoch : 506 training loss : 974249.375 testing loss : 661.9973399048358\n",
            "epoch : 507 training loss : 965862.9375 testing loss : 678.9233614132468\n",
            "epoch : 508 training loss : 968109.75 testing loss : 658.965089373884\n",
            "epoch : 509 training loss : 963628.9375 testing loss : 664.9934019557143\n",
            "epoch : 510 training loss : 964824.625 testing loss : 678.2773752444614\n",
            "epoch : 511 training loss : 969826.625 testing loss : 635.5672991571173\n",
            "epoch : 512 training loss : 977938.3125 testing loss : 696.2244968562\n",
            "epoch : 513 training loss : 969432.25 testing loss : 644.5933367277669\n",
            "epoch : 514 training loss : 966909.0 testing loss : 689.53712858141\n",
            "epoch : 515 training loss : 968174.75 testing loss : 661.9661760899872\n",
            "epoch : 516 training loss : 983062.625 testing loss : 668.3754515837779\n",
            "epoch : 517 training loss : 979209.625 testing loss : 684.7550849218284\n",
            "epoch : 518 training loss : 974481.0 testing loss : 659.2330990660507\n",
            "epoch : 519 training loss : 966154.8125 testing loss : 672.1812530428963\n",
            "epoch : 520 training loss : 965829.25 testing loss : 665.1343571126988\n",
            "epoch : 521 training loss : 962190.1875 testing loss : 659.6916250697279\n",
            "epoch : 522 training loss : 961464.75 testing loss : 691.3623144753211\n",
            "epoch : 523 training loss : 967980.6875 testing loss : 661.7827238551283\n",
            "epoch : 524 training loss : 959393.5 testing loss : 670.0005826000619\n",
            "epoch : 525 training loss : 960509.875 testing loss : 665.2525412951951\n",
            "epoch : 526 training loss : 960892.625 testing loss : 658.7264408664366\n",
            "epoch : 527 training loss : 957128.875 testing loss : 675.0820596049317\n",
            "epoch : 528 training loss : 957639.125 testing loss : 663.8895201197768\n",
            "epoch : 529 training loss : 959000.25 testing loss : 676.5818060794763\n",
            "epoch : 530 training loss : 962180.25 testing loss : 662.7506420148158\n",
            "epoch : 531 training loss : 962286.75 testing loss : 649.5697347906838\n",
            "epoch : 532 training loss : 966339.875 testing loss : 699.4372150771386\n",
            "epoch : 533 training loss : 968671.25 testing loss : 637.8247408887981\n",
            "epoch : 534 training loss : 962374.6875 testing loss : 670.3677255389965\n",
            "epoch : 535 training loss : 956663.9375 testing loss : 659.3920039265556\n",
            "epoch : 536 training loss : 957747.9375 testing loss : 649.3692917591702\n",
            "epoch : 537 training loss : 955265.8125 testing loss : 671.3197653082619\n",
            "epoch : 538 training loss : 954855.875 testing loss : 644.4019172107224\n",
            "epoch : 539 training loss : 955346.125 testing loss : 663.0212107611968\n",
            "epoch : 540 training loss : 952759.4375 testing loss : 645.4073757049257\n",
            "epoch : 541 training loss : 954235.5 testing loss : 651.1095391986645\n",
            "epoch : 542 training loss : 954426.5625 testing loss : 654.3684721056339\n",
            "epoch : 543 training loss : 953158.625 testing loss : 664.0150574181987\n",
            "epoch : 544 training loss : 955214.75 testing loss : 645.0270891421665\n",
            "epoch : 545 training loss : 954616.75 testing loss : 661.1929289535084\n",
            "epoch : 546 training loss : 951454.125 testing loss : 656.2096613297421\n",
            "epoch : 547 training loss : 952133.5625 testing loss : 665.0684414395189\n",
            "epoch : 548 training loss : 955540.5 testing loss : 643.6776934914884\n",
            "epoch : 549 training loss : 950677.9375 testing loss : 671.557682777928\n",
            "epoch : 550 training loss : 950721.0625 testing loss : 650.1127177867214\n",
            "epoch : 551 training loss : 950209.875 testing loss : 665.9347970992063\n",
            "epoch : 552 training loss : 951935.5625 testing loss : 636.656919629173\n",
            "epoch : 553 training loss : 954194.25 testing loss : 694.4369024069963\n",
            "epoch : 554 training loss : 953634.125 testing loss : 640.8691876820758\n",
            "epoch : 555 training loss : 955515.5625 testing loss : 667.5659760310587\n",
            "epoch : 556 training loss : 952100.5625 testing loss : 647.010316931041\n",
            "epoch : 557 training loss : 953604.125 testing loss : 651.3513837134943\n",
            "epoch : 558 training loss : 952517.875 testing loss : 671.7790427735422\n",
            "epoch : 559 training loss : 949216.125 testing loss : 638.8838153919287\n",
            "epoch : 560 training loss : 948895.4375 testing loss : 670.4242269106671\n",
            "epoch : 561 training loss : 948132.25 testing loss : 645.7718777930842\n",
            "epoch : 562 training loss : 947198.125 testing loss : 653.9238406810085\n",
            "epoch : 563 training loss : 945350.3125 testing loss : 651.1078327124097\n",
            "epoch : 564 training loss : 945588.5 testing loss : 661.1066733820248\n",
            "epoch : 565 training loss : 949316.6875 testing loss : 638.0338022666695\n",
            "epoch : 566 training loss : 949614.875 testing loss : 667.173467144502\n",
            "epoch : 567 training loss : 951755.0 testing loss : 652.9622386809999\n",
            "epoch : 568 training loss : 953388.25 testing loss : 651.9428359491635\n",
            "epoch : 569 training loss : 947651.75 testing loss : 647.3573848737025\n",
            "epoch : 570 training loss : 946761.0 testing loss : 675.4001406420649\n",
            "epoch : 571 training loss : 949443.625 testing loss : 627.0672947010107\n",
            "epoch : 572 training loss : 950521.375 testing loss : 693.6240641142415\n",
            "epoch : 573 training loss : 958140.125 testing loss : 621.871550903911\n",
            "epoch : 574 training loss : 967426.6875 testing loss : 689.4029028542274\n",
            "epoch : 575 training loss : 964011.5625 testing loss : 643.3617567530775\n",
            "epoch : 576 training loss : 961563.5 testing loss : 654.0245808268015\n",
            "epoch : 577 training loss : 952665.0 testing loss : 652.4172916560046\n",
            "epoch : 578 training loss : 957391.6875 testing loss : 630.636022818827\n",
            "epoch : 579 training loss : 958335.3125 testing loss : 697.0035825049982\n",
            "epoch : 580 training loss : 955531.6875 testing loss : 619.2844366204422\n",
            "epoch : 581 training loss : 968185.125 testing loss : 702.1567455709508\n",
            "epoch : 582 training loss : 950587.4375 testing loss : 628.9943282519822\n",
            "epoch : 583 training loss : 948451.0625 testing loss : 675.9972933857841\n",
            "epoch : 584 training loss : 941561.5 testing loss : 633.5051774535559\n",
            "epoch : 585 training loss : 940402.625 testing loss : 670.1783144431831\n",
            "epoch : 586 training loss : 943707.6875 testing loss : 634.1645081423026\n",
            "epoch : 587 training loss : 944730.125 testing loss : 666.1503568645072\n",
            "epoch : 588 training loss : 946616.625 testing loss : 649.5355959508271\n",
            "epoch : 589 training loss : 941563.4375 testing loss : 651.5801828413938\n",
            "epoch : 590 training loss : 937382.4375 testing loss : 641.2896778456933\n",
            "epoch : 591 training loss : 937442.25 testing loss : 652.3632729095696\n",
            "epoch : 592 training loss : 940211.3125 testing loss : 646.956156601948\n",
            "epoch : 593 training loss : 938023.125 testing loss : 654.4300818295605\n",
            "epoch : 594 training loss : 946894.125 testing loss : 648.8564597252196\n",
            "epoch : 595 training loss : 942912.375 testing loss : 649.2210341575926\n",
            "epoch : 596 training loss : 942042.5625 testing loss : 638.3032704222519\n",
            "epoch : 597 training loss : 944913.0 testing loss : 652.4813293503449\n",
            "epoch : 598 training loss : 944038.375 testing loss : 671.8735472396412\n",
            "epoch : 599 training loss : 955064.6875 testing loss : 644.7743067424909\n",
            "epoch : 600 training loss : 951802.375 testing loss : 679.7924213894701\n",
            "epoch : 601 training loss : 950518.625 testing loss : 634.2495033108028\n",
            "epoch : 602 training loss : 943840.6875 testing loss : 677.6235348503146\n",
            "epoch : 603 training loss : 946487.625 testing loss : 636.5627710101879\n",
            "epoch : 604 training loss : 938015.5 testing loss : 667.996808345339\n",
            "epoch : 605 training loss : 937634.75 testing loss : 645.9854351123877\n",
            "epoch : 606 training loss : 936164.5 testing loss : 651.073963838341\n",
            "epoch : 607 training loss : 938079.875 testing loss : 653.024321292354\n",
            "epoch : 608 training loss : 936544.1875 testing loss : 643.7011762614799\n",
            "epoch : 609 training loss : 936410.1875 testing loss : 652.6791353457797\n",
            "epoch : 610 training loss : 939313.9375 testing loss : 630.9197738086228\n",
            "epoch : 611 training loss : 939696.1875 testing loss : 660.7447042190923\n",
            "epoch : 612 training loss : 930819.75 testing loss : 626.8619196372749\n",
            "epoch : 613 training loss : 933990.4375 testing loss : 652.4696634997309\n",
            "epoch : 614 training loss : 933537.25 testing loss : 650.008389732479\n",
            "epoch : 615 training loss : 931565.25 testing loss : 635.4714919887813\n",
            "epoch : 616 training loss : 931909.0 testing loss : 656.3754513812277\n",
            "epoch : 617 training loss : 929927.375 testing loss : 623.6824444437449\n",
            "epoch : 618 training loss : 934061.8125 testing loss : 667.6839540173522\n",
            "epoch : 619 training loss : 934346.125 testing loss : 634.3827080832118\n",
            "epoch : 620 training loss : 934883.0625 testing loss : 656.680721010782\n",
            "epoch : 621 training loss : 930568.8125 testing loss : 646.2195182374094\n",
            "epoch : 622 training loss : 929086.75 testing loss : 630.2816408524471\n",
            "epoch : 623 training loss : 930070.25 testing loss : 641.5650348473439\n",
            "epoch : 624 training loss : 930018.25 testing loss : 663.3778941715713\n",
            "epoch : 625 training loss : 932543.0 testing loss : 626.769672830548\n",
            "epoch : 626 training loss : 926451.75 testing loss : 658.0120582559467\n",
            "epoch : 627 training loss : 932876.9375 testing loss : 642.8558094712486\n",
            "epoch : 628 training loss : 931447.125 testing loss : 642.9720703373968\n",
            "epoch : 629 training loss : 929520.875 testing loss : 648.0827245775578\n",
            "epoch : 630 training loss : 931614.0 testing loss : 642.925538592634\n",
            "epoch : 631 training loss : 927111.5 testing loss : 647.8235520535866\n",
            "epoch : 632 training loss : 929890.625 testing loss : 656.2491421636227\n",
            "epoch : 633 training loss : 930757.5625 testing loss : 617.1073927900433\n",
            "epoch : 634 training loss : 942749.1875 testing loss : 699.9045114411717\n",
            "epoch : 635 training loss : 939701.0 testing loss : 616.9579358712762\n",
            "epoch : 636 training loss : 934209.375 testing loss : 680.7610394954681\n",
            "epoch : 637 training loss : 931350.5 testing loss : 626.9433471861139\n",
            "epoch : 638 training loss : 927502.0 testing loss : 655.8578722097177\n",
            "epoch : 639 training loss : 925671.6875 testing loss : 630.4977976347493\n",
            "epoch : 640 training loss : 924957.0 testing loss : 648.4197428521857\n",
            "epoch : 641 training loss : 924606.1875 testing loss : 631.1418158586047\n",
            "epoch : 642 training loss : 924708.9375 testing loss : 671.6635863021412\n",
            "epoch : 643 training loss : 942082.5 testing loss : 610.9862270502917\n",
            "epoch : 644 training loss : 932153.3125 testing loss : 679.8053938920519\n",
            "epoch : 645 training loss : 938140.625 testing loss : 628.3304807591227\n",
            "epoch : 646 training loss : 935082.875 testing loss : 624.1423474518599\n",
            "epoch : 647 training loss : 929821.625 testing loss : 654.3593267909193\n",
            "epoch : 648 training loss : 927587.875 testing loss : 627.2711831637189\n",
            "epoch : 649 training loss : 930290.75 testing loss : 666.9245848170424\n",
            "epoch : 650 training loss : 928945.3125 testing loss : 621.9602292807756\n",
            "epoch : 651 training loss : 923627.375 testing loss : 668.4806223485322\n",
            "epoch : 652 training loss : 930741.125 testing loss : 619.8204417081006\n",
            "epoch : 653 training loss : 937322.625 testing loss : 661.8402994185423\n",
            "epoch : 654 training loss : 940254.75 testing loss : 649.7803314322919\n",
            "epoch : 655 training loss : 937169.375 testing loss : 623.2473683293941\n",
            "epoch : 656 training loss : 935641.4375 testing loss : 647.2630427347875\n",
            "epoch : 657 training loss : 931455.6875 testing loss : 647.7562869565677\n",
            "epoch : 658 training loss : 928476.375 testing loss : 626.0860642943762\n",
            "epoch : 659 training loss : 923938.3125 testing loss : 640.0542590702529\n",
            "epoch : 660 training loss : 925100.375 testing loss : 654.5374950286562\n",
            "epoch : 661 training loss : 925777.6875 testing loss : 618.405502401622\n",
            "epoch : 662 training loss : 931552.25 testing loss : 670.5898582639948\n",
            "epoch : 663 training loss : 928733.4375 testing loss : 611.4947046579513\n",
            "epoch : 664 training loss : 933063.125 testing loss : 678.7417578211928\n",
            "epoch : 665 training loss : 933659.625 testing loss : 607.7047995993521\n",
            "epoch : 666 training loss : 932043.125 testing loss : 681.3580961797089\n",
            "epoch : 667 training loss : 932826.8125 testing loss : 623.9056524745131\n",
            "epoch : 668 training loss : 922635.5625 testing loss : 646.8596120154963\n",
            "epoch : 669 training loss : 925314.4375 testing loss : 633.2893036508982\n",
            "epoch : 670 training loss : 924596.1875 testing loss : 629.8814758178407\n",
            "epoch : 671 training loss : 924635.625 testing loss : 643.3410281607535\n",
            "epoch : 672 training loss : 922204.9375 testing loss : 639.7919139840961\n",
            "epoch : 673 training loss : 920977.875 testing loss : 632.5121725955896\n",
            "epoch : 674 training loss : 920051.8125 testing loss : 639.1473743282588\n",
            "epoch : 675 training loss : 920230.0 testing loss : 643.8068229012785\n",
            "epoch : 676 training loss : 919050.6875 testing loss : 637.511423024456\n",
            "epoch : 677 training loss : 920797.4375 testing loss : 638.5879365511701\n",
            "epoch : 678 training loss : 924810.875 testing loss : 617.5067515394329\n",
            "epoch : 679 training loss : 919975.625 testing loss : 648.7998435244097\n",
            "epoch : 680 training loss : 920473.5625 testing loss : 632.3908345361726\n",
            "epoch : 681 training loss : 919115.375 testing loss : 650.8956985790118\n",
            "epoch : 682 training loss : 920214.5625 testing loss : 616.387501064655\n",
            "epoch : 683 training loss : 917507.5625 testing loss : 637.2291982511504\n",
            "epoch : 684 training loss : 918511.4375 testing loss : 633.0933528642739\n",
            "epoch : 685 training loss : 917775.875 testing loss : 641.0623889496896\n",
            "epoch : 686 training loss : 916819.5 testing loss : 640.9181826072456\n",
            "epoch : 687 training loss : 917215.875 testing loss : 628.3421787983548\n",
            "epoch : 688 training loss : 915521.8125 testing loss : 637.5950562785157\n",
            "epoch : 689 training loss : 917776.125 testing loss : 638.3946124874385\n",
            "epoch : 690 training loss : 919649.125 testing loss : 643.025268586336\n",
            "epoch : 691 training loss : 919909.5 testing loss : 648.7885702162718\n",
            "epoch : 692 training loss : 934458.8125 testing loss : 602.3636751575807\n",
            "epoch : 693 training loss : 937560.875 testing loss : 668.6332000441256\n",
            "epoch : 694 training loss : 920746.875 testing loss : 626.9354631521005\n",
            "epoch : 695 training loss : 929486.875 testing loss : 654.1716380309215\n",
            "epoch : 696 training loss : 919070.0625 testing loss : 616.4123406178128\n",
            "epoch : 697 training loss : 919642.0 testing loss : 646.6815087288882\n",
            "epoch : 698 training loss : 918590.875 testing loss : 626.7972144721883\n",
            "epoch : 699 training loss : 915512.0 testing loss : 637.2440430442844\n",
            "epoch : 700 training loss : 917931.125 testing loss : 622.7557804120325\n",
            "epoch : 701 training loss : 918980.4375 testing loss : 639.0150055484435\n",
            "epoch : 702 training loss : 914220.25 testing loss : 631.432491536689\n",
            "epoch : 703 training loss : 918161.4375 testing loss : 627.8441797927418\n",
            "epoch : 704 training loss : 922904.25 testing loss : 651.691803347748\n",
            "epoch : 705 training loss : 922135.5 testing loss : 619.3585180582198\n",
            "epoch : 706 training loss : 924968.875 testing loss : 670.5991611459614\n",
            "epoch : 707 training loss : 923772.1875 testing loss : 614.6323531813326\n",
            "epoch : 708 training loss : 917568.125 testing loss : 648.5111681993029\n",
            "epoch : 709 training loss : 914895.25 testing loss : 631.1584628430088\n",
            "epoch : 710 training loss : 914095.875 testing loss : 627.4803418838873\n",
            "epoch : 711 training loss : 917284.875 testing loss : 634.5765477049667\n",
            "epoch : 712 training loss : 915496.0 testing loss : 633.2345322950752\n",
            "epoch : 713 training loss : 915640.875 testing loss : 636.3268515810503\n",
            "epoch : 714 training loss : 914122.75 testing loss : 626.8674100669084\n",
            "epoch : 715 training loss : 914665.3125 testing loss : 630.3186944923569\n",
            "epoch : 716 training loss : 913643.25 testing loss : 636.0208278904973\n",
            "epoch : 717 training loss : 914496.125 testing loss : 609.2411227753732\n",
            "epoch : 718 training loss : 922384.25 testing loss : 659.8695886303893\n",
            "epoch : 719 training loss : 918160.375 testing loss : 628.2496128736344\n",
            "epoch : 720 training loss : 914360.9375 testing loss : 621.253693840145\n",
            "epoch : 721 training loss : 914314.125 testing loss : 637.4348552986584\n",
            "epoch : 722 training loss : 912623.8125 testing loss : 632.0489409940433\n",
            "epoch : 723 training loss : 913361.375 testing loss : 640.1838587005582\n",
            "epoch : 724 training loss : 911974.8125 testing loss : 627.7472921029656\n",
            "epoch : 725 training loss : 910237.75 testing loss : 642.602952026688\n",
            "epoch : 726 training loss : 913969.5625 testing loss : 628.1422561691926\n",
            "epoch : 727 training loss : 912118.4375 testing loss : 634.466922504712\n",
            "epoch : 728 training loss : 911435.9375 testing loss : 635.1819931785617\n",
            "epoch : 729 training loss : 913719.5 testing loss : 646.9653319616233\n",
            "epoch : 730 training loss : 915102.75 testing loss : 632.6197090634203\n",
            "epoch : 731 training loss : 913744.0 testing loss : 642.6951870517394\n",
            "epoch : 732 training loss : 914266.1875 testing loss : 627.319360967231\n",
            "epoch : 733 training loss : 915972.125 testing loss : 631.5389202400646\n",
            "epoch : 734 training loss : 909665.5625 testing loss : 640.4970326022765\n",
            "epoch : 735 training loss : 910961.5625 testing loss : 630.5766984361463\n",
            "epoch : 736 training loss : 910913.125 testing loss : 636.2543964871263\n",
            "epoch : 737 training loss : 913140.875 testing loss : 631.8648764158772\n",
            "epoch : 738 training loss : 909665.3125 testing loss : 637.4914946070814\n",
            "epoch : 739 training loss : 913582.8125 testing loss : 627.8016267603477\n",
            "epoch : 740 training loss : 909862.5 testing loss : 623.8479111743185\n",
            "epoch : 741 training loss : 910578.9375 testing loss : 637.7104368441928\n",
            "epoch : 742 training loss : 912812.125 testing loss : 635.6062620023711\n",
            "epoch : 743 training loss : 912094.875 testing loss : 625.9578634933033\n",
            "epoch : 744 training loss : 911928.125 testing loss : 652.0322366372674\n",
            "epoch : 745 training loss : 916637.0625 testing loss : 619.5562179848156\n",
            "epoch : 746 training loss : 910599.4375 testing loss : 657.3495841553781\n",
            "epoch : 747 training loss : 912301.875 testing loss : 620.1287639900646\n",
            "epoch : 748 training loss : 912904.0 testing loss : 626.4703419081933\n",
            "epoch : 749 training loss : 915580.75 testing loss : 666.41740886511\n",
            "epoch : 750 training loss : 915207.125 testing loss : 619.400236228926\n",
            "epoch : 751 training loss : 910067.25 testing loss : 645.133155736248\n",
            "epoch : 752 training loss : 911022.4375 testing loss : 623.2223215166447\n",
            "epoch : 753 training loss : 911797.0625 testing loss : 625.1834214113455\n",
            "epoch : 754 training loss : 913007.0 testing loss : 664.3340731405578\n",
            "epoch : 755 training loss : 915143.25 testing loss : 616.6458952891088\n",
            "epoch : 756 training loss : 910371.3125 testing loss : 641.1999737203648\n",
            "epoch : 757 training loss : 910560.0 testing loss : 618.7521055926264\n",
            "epoch : 758 training loss : 912668.9375 testing loss : 658.9244979736025\n",
            "epoch : 759 training loss : 917922.5625 testing loss : 633.5234391182925\n",
            "epoch : 760 training loss : 909515.375 testing loss : 627.445123450946\n",
            "epoch : 761 training loss : 913882.75 testing loss : 642.5861012598054\n",
            "epoch : 762 training loss : 911988.1875 testing loss : 640.5266077138681\n",
            "epoch : 763 training loss : 914720.8125 testing loss : 601.7297233965545\n",
            "epoch : 764 training loss : 915556.3125 testing loss : 661.1766974504015\n",
            "epoch : 765 training loss : 909830.5625 testing loss : 614.9790226906802\n",
            "epoch : 766 training loss : 914535.875 testing loss : 664.6125317780318\n",
            "epoch : 767 training loss : 914251.125 testing loss : 619.2522647127641\n",
            "epoch : 768 training loss : 908471.875 testing loss : 660.9627839632794\n",
            "epoch : 769 training loss : 910882.5 testing loss : 605.3012113465672\n",
            "epoch : 770 training loss : 917859.375 testing loss : 660.402284400653\n",
            "epoch : 771 training loss : 915403.1875 testing loss : 621.8216857045097\n",
            "epoch : 772 training loss : 915464.0 testing loss : 617.7193471836832\n",
            "epoch : 773 training loss : 911696.5625 testing loss : 649.6338181601161\n",
            "epoch : 774 training loss : 908174.1875 testing loss : 620.1737859080323\n",
            "epoch : 775 training loss : 906399.5 testing loss : 650.9005384846071\n",
            "epoch : 776 training loss : 908440.9375 testing loss : 627.6712246426439\n",
            "epoch : 777 training loss : 906452.375 testing loss : 642.0411740847394\n",
            "epoch : 778 training loss : 912039.0 testing loss : 620.549098527537\n",
            "epoch : 779 training loss : 909533.25 testing loss : 644.8572328934628\n",
            "epoch : 780 training loss : 912869.75 testing loss : 632.7024344364099\n",
            "epoch : 781 training loss : 905962.5 testing loss : 631.8522739220509\n",
            "epoch : 782 training loss : 907961.75 testing loss : 635.6514521046022\n",
            "epoch : 783 training loss : 908517.875 testing loss : 637.1968991756439\n",
            "epoch : 784 training loss : 905921.625 testing loss : 630.9962400229631\n",
            "epoch : 785 training loss : 907437.125 testing loss : 633.8041946255\n",
            "epoch : 786 training loss : 905595.0 testing loss : 639.0952412238163\n",
            "epoch : 787 training loss : 906891.8125 testing loss : 641.7784889166334\n",
            "epoch : 788 training loss : 911506.3125 testing loss : 650.9864042480435\n",
            "epoch : 789 training loss : 912616.125 testing loss : 607.616042075959\n",
            "epoch : 790 training loss : 921996.5625 testing loss : 689.3755362498022\n",
            "epoch : 791 training loss : 923935.6875 testing loss : 609.2077229402762\n",
            "epoch : 792 training loss : 922264.1875 testing loss : 670.3199694114448\n",
            "epoch : 793 training loss : 915533.125 testing loss : 608.5305515707066\n",
            "epoch : 794 training loss : 921180.4375 testing loss : 632.4894547230374\n",
            "epoch : 795 training loss : 912641.375 testing loss : 644.0872705826717\n",
            "epoch : 796 training loss : 914042.625 testing loss : 608.9934916348584\n",
            "epoch : 797 training loss : 919159.625 testing loss : 687.0693107516365\n",
            "epoch : 798 training loss : 939639.0 testing loss : 611.9582992127512\n",
            "epoch : 799 training loss : 931403.125 testing loss : 679.9004068269139\n",
            "epoch : 800 training loss : 916513.9375 testing loss : 620.3078275001154\n",
            "epoch : 801 training loss : 915019.0625 testing loss : 654.888165328355\n",
            "epoch : 802 training loss : 917768.3125 testing loss : 621.4302175720181\n",
            "epoch : 803 training loss : 913440.25 testing loss : 651.767413145673\n",
            "epoch : 804 training loss : 906870.1875 testing loss : 624.9128022636987\n",
            "epoch : 805 training loss : 911624.9375 testing loss : 637.1642230865175\n",
            "epoch : 806 training loss : 908879.125 testing loss : 621.7118998649901\n",
            "epoch : 807 training loss : 911323.6875 testing loss : 645.1085341103309\n",
            "epoch : 808 training loss : 906051.375 testing loss : 623.922820603953\n",
            "epoch : 809 training loss : 910382.6875 testing loss : 637.167182447636\n",
            "epoch : 810 training loss : 907899.5625 testing loss : 618.8894445347574\n",
            "epoch : 811 training loss : 906867.8125 testing loss : 649.1318827920255\n",
            "epoch : 812 training loss : 906544.25 testing loss : 633.0483328874133\n",
            "epoch : 813 training loss : 912528.1875 testing loss : 608.2696709400784\n",
            "epoch : 814 training loss : 916972.3125 testing loss : 669.2015281588631\n",
            "epoch : 815 training loss : 916874.8125 testing loss : 618.3606741892553\n",
            "epoch : 816 training loss : 912986.125 testing loss : 649.0924477682704\n",
            "epoch : 817 training loss : 908863.9375 testing loss : 616.2748383357462\n",
            "epoch : 818 training loss : 906568.375 testing loss : 629.3218465125666\n",
            "epoch : 819 training loss : 912316.4375 testing loss : 654.6630353315742\n",
            "epoch : 820 training loss : 911667.4375 testing loss : 624.5205564899782\n",
            "epoch : 821 training loss : 909439.875 testing loss : 635.2967335840242\n",
            "epoch : 822 training loss : 901866.375 testing loss : 625.6334645748138\n",
            "epoch : 823 training loss : 906383.125 testing loss : 630.1232909459983\n",
            "epoch : 824 training loss : 902665.5 testing loss : 636.7336383562173\n",
            "epoch : 825 training loss : 905325.375 testing loss : 612.5103007945339\n",
            "epoch : 826 training loss : 910786.0 testing loss : 648.5085847546569\n",
            "epoch : 827 training loss : 904279.0 testing loss : 626.5953102428301\n",
            "epoch : 828 training loss : 903474.875 testing loss : 639.8124031113313\n",
            "epoch : 829 training loss : 902286.25 testing loss : 622.9410164736014\n",
            "epoch : 830 training loss : 904766.4375 testing loss : 638.3471577378501\n",
            "epoch : 831 training loss : 903779.375 testing loss : 629.6808150953951\n",
            "epoch : 832 training loss : 903460.0 testing loss : 632.0053815229804\n",
            "epoch : 833 training loss : 904255.25 testing loss : 618.8363279473465\n",
            "epoch : 834 training loss : 906483.75 testing loss : 632.9743419052226\n",
            "epoch : 835 training loss : 908249.5 testing loss : 650.1281664666876\n",
            "epoch : 836 training loss : 908431.4375 testing loss : 603.8148886735461\n",
            "epoch : 837 training loss : 909667.5625 testing loss : 655.669549787994\n",
            "epoch : 838 training loss : 908437.5 testing loss : 618.6399759929792\n",
            "epoch : 839 training loss : 904304.125 testing loss : 653.4579424879192\n",
            "epoch : 840 training loss : 904444.8125 testing loss : 615.1425589481287\n",
            "epoch : 841 training loss : 906431.9375 testing loss : 655.0343141787876\n",
            "epoch : 842 training loss : 905024.0 testing loss : 613.2175569428807\n",
            "epoch : 843 training loss : 908176.125 testing loss : 644.7419603039733\n",
            "epoch : 844 training loss : 902483.25 testing loss : 620.2118496493956\n",
            "epoch : 845 training loss : 904746.3125 testing loss : 631.7511814737742\n",
            "epoch : 846 training loss : 905697.25 testing loss : 643.6402537569536\n",
            "epoch : 847 training loss : 903865.625 testing loss : 622.0349977522825\n",
            "epoch : 848 training loss : 902480.8125 testing loss : 620.4218893452028\n",
            "epoch : 849 training loss : 900724.375 testing loss : 638.1529772914616\n",
            "epoch : 850 training loss : 903143.5 testing loss : 627.0932396724161\n",
            "epoch : 851 training loss : 900540.5625 testing loss : 624.9609625466102\n",
            "epoch : 852 training loss : 903455.25 testing loss : 635.3169663332205\n",
            "epoch : 853 training loss : 902311.9375 testing loss : 601.3619547797515\n",
            "epoch : 854 training loss : 906178.0 testing loss : 662.4684468476119\n",
            "epoch : 855 training loss : 906248.875 testing loss : 633.3529438909176\n",
            "epoch : 856 training loss : 901661.375 testing loss : 637.3496917433444\n",
            "epoch : 857 training loss : 900728.1875 testing loss : 626.122164023661\n",
            "epoch : 858 training loss : 905620.5625 testing loss : 641.69220557888\n",
            "epoch : 859 training loss : 900844.5 testing loss : 611.4764445123419\n",
            "epoch : 860 training loss : 902813.25 testing loss : 642.1703541152245\n",
            "epoch : 861 training loss : 900413.375 testing loss : 627.3599322812747\n",
            "epoch : 862 training loss : 899855.5 testing loss : 633.6431593072098\n",
            "epoch : 863 training loss : 902300.8125 testing loss : 612.9618370912772\n",
            "epoch : 864 training loss : 903137.375 testing loss : 635.4019702789003\n",
            "epoch : 865 training loss : 899916.3125 testing loss : 623.77309221715\n",
            "epoch : 866 training loss : 902341.0 testing loss : 621.4852542687306\n",
            "epoch : 867 training loss : 905183.75 testing loss : 648.4690395772984\n",
            "epoch : 868 training loss : 912167.6875 testing loss : 627.0596564617832\n",
            "epoch : 869 training loss : 905497.6875 testing loss : 627.4088748134343\n",
            "epoch : 870 training loss : 904671.8125 testing loss : 644.5909694211673\n",
            "epoch : 871 training loss : 905345.0 testing loss : 635.4970062369794\n",
            "epoch : 872 training loss : 904370.4375 testing loss : 618.6957661767976\n",
            "epoch : 873 training loss : 902711.125 testing loss : 654.2714686752421\n",
            "epoch : 874 training loss : 898664.3125 testing loss : 616.8648415603469\n",
            "epoch : 875 training loss : 901022.8125 testing loss : 639.6918260671396\n",
            "epoch : 876 training loss : 898643.875 testing loss : 631.4993616943866\n",
            "epoch : 877 training loss : 906385.5 testing loss : 606.6249718771571\n",
            "epoch : 878 training loss : 909697.4375 testing loss : 657.0964088081258\n",
            "epoch : 879 training loss : 906609.625 testing loss : 613.3893080158571\n",
            "epoch : 880 training loss : 901858.5 testing loss : 639.313839361731\n",
            "epoch : 881 training loss : 904373.5625 testing loss : 638.0420046759917\n",
            "epoch : 882 training loss : 902470.375 testing loss : 614.2393299520543\n",
            "epoch : 883 training loss : 903135.0 testing loss : 642.7450452462762\n",
            "epoch : 884 training loss : 902141.9375 testing loss : 628.3570036402846\n",
            "epoch : 885 training loss : 899797.3125 testing loss : 629.6430031071721\n",
            "epoch : 886 training loss : 904112.875 testing loss : 636.2475261835925\n",
            "epoch : 887 training loss : 904778.6875 testing loss : 619.7825296431516\n",
            "epoch : 888 training loss : 899141.625 testing loss : 642.840722343563\n",
            "epoch : 889 training loss : 897871.875 testing loss : 614.5110974417323\n",
            "epoch : 890 training loss : 914605.5 testing loss : 663.991952168203\n",
            "epoch : 891 training loss : 913092.75 testing loss : 614.7703692659868\n",
            "epoch : 892 training loss : 910878.25 testing loss : 640.9903488475665\n",
            "epoch : 893 training loss : 904720.3125 testing loss : 628.9462218136914\n",
            "epoch : 894 training loss : 906380.625 testing loss : 611.1415575732173\n",
            "epoch : 895 training loss : 909444.8125 testing loss : 682.3511025061649\n",
            "epoch : 896 training loss : 908147.8125 testing loss : 602.7934865002084\n",
            "epoch : 897 training loss : 911044.8125 testing loss : 660.9547431722151\n",
            "epoch : 898 training loss : 902111.5 testing loss : 621.5035456746025\n",
            "epoch : 899 training loss : 899738.6875 testing loss : 660.6978676698905\n",
            "epoch : 900 training loss : 902687.5 testing loss : 603.7048040723379\n",
            "epoch : 901 training loss : 904991.125 testing loss : 667.1157619151394\n",
            "epoch : 902 training loss : 908251.0625 testing loss : 610.9220398063153\n",
            "epoch : 903 training loss : 908933.125 testing loss : 634.8422688150828\n",
            "epoch : 904 training loss : 906408.25 testing loss : 647.0917955900716\n",
            "epoch : 905 training loss : 903653.625 testing loss : 611.4150662358883\n",
            "epoch : 906 training loss : 905975.125 testing loss : 654.4318760538523\n",
            "epoch : 907 training loss : 908020.9375 testing loss : 625.346831057979\n",
            "epoch : 908 training loss : 900985.9375 testing loss : 640.275762775303\n",
            "epoch : 909 training loss : 902359.625 testing loss : 614.991810214203\n",
            "epoch : 910 training loss : 900970.8125 testing loss : 628.2431553507273\n",
            "epoch : 911 training loss : 902969.8125 testing loss : 649.646536732142\n",
            "epoch : 912 training loss : 910354.125 testing loss : 610.1736077144083\n",
            "epoch : 913 training loss : 908295.5 testing loss : 662.6595386720337\n",
            "epoch : 914 training loss : 907225.0625 testing loss : 619.436979612418\n",
            "epoch : 915 training loss : 907202.5 testing loss : 638.4671482301392\n",
            "epoch : 916 training loss : 902079.1875 testing loss : 619.8991171554127\n",
            "epoch : 917 training loss : 900963.875 testing loss : 627.9857534201799\n",
            "epoch : 918 training loss : 908928.25 testing loss : 653.5921845204007\n",
            "epoch : 919 training loss : 908514.25 testing loss : 617.1065061261169\n",
            "epoch : 920 training loss : 919205.625 testing loss : 666.2259580540446\n",
            "epoch : 921 training loss : 911348.9375 testing loss : 611.1300186203645\n",
            "epoch : 922 training loss : 913781.0 testing loss : 676.4197545663445\n",
            "epoch : 923 training loss : 917985.1875 testing loss : 613.2279112803197\n",
            "epoch : 924 training loss : 904267.25 testing loss : 652.9114501391892\n",
            "epoch : 925 training loss : 899992.9375 testing loss : 624.8172988532918\n",
            "epoch : 926 training loss : 899600.8125 testing loss : 627.4139798911272\n",
            "epoch : 927 training loss : 901349.1875 testing loss : 636.6631010929041\n",
            "epoch : 928 training loss : 904910.375 testing loss : 616.2952774161786\n",
            "epoch : 929 training loss : 900772.9375 testing loss : 631.7165601591093\n",
            "epoch : 930 training loss : 899043.0 testing loss : 633.255015143251\n",
            "epoch : 931 training loss : 900154.0 testing loss : 638.7726316852907\n",
            "epoch : 932 training loss : 903281.5625 testing loss : 615.5160581854592\n",
            "epoch : 933 training loss : 909085.75 testing loss : 662.3442694718859\n",
            "epoch : 934 training loss : 902552.0625 testing loss : 612.8110125360236\n",
            "epoch : 935 training loss : 901691.75 testing loss : 648.9422448061209\n",
            "epoch : 936 training loss : 896256.9375 testing loss : 612.7910384435569\n",
            "epoch : 937 training loss : 905474.3125 testing loss : 655.0243752382498\n",
            "epoch : 938 training loss : 901456.9375 testing loss : 626.1112073353961\n",
            "epoch : 939 training loss : 897123.75 testing loss : 623.9090423225301\n",
            "epoch : 940 training loss : 901496.4375 testing loss : 640.6288920693693\n",
            "epoch : 941 training loss : 908401.125 testing loss : 610.6816295501405\n",
            "epoch : 942 training loss : 912412.25 testing loss : 662.3431407265958\n",
            "epoch : 943 training loss : 902475.8125 testing loss : 618.7254832761478\n",
            "epoch : 944 training loss : 916460.1875 testing loss : 663.7976419883491\n",
            "epoch : 945 training loss : 904165.3125 testing loss : 598.7257283282491\n",
            "epoch : 946 training loss : 906502.25 testing loss : 650.9183910990183\n",
            "epoch : 947 training loss : 906341.3125 testing loss : 633.2641237963618\n",
            "epoch : 948 training loss : 902818.375 testing loss : 628.0811878959689\n",
            "epoch : 949 training loss : 899418.6875 testing loss : 633.2389276196471\n",
            "epoch : 950 training loss : 901366.375 testing loss : 630.9450260580113\n",
            "epoch : 951 training loss : 900982.0 testing loss : 624.2985072832192\n",
            "epoch : 952 training loss : 896337.1875 testing loss : 639.379895672334\n",
            "epoch : 953 training loss : 897929.8125 testing loss : 623.438379741348\n",
            "epoch : 954 training loss : 898046.5 testing loss : 628.3012123086811\n",
            "epoch : 955 training loss : 895540.125 testing loss : 638.8912158708657\n",
            "epoch : 956 training loss : 899108.0 testing loss : 616.1067723616035\n",
            "epoch : 957 training loss : 897433.25 testing loss : 650.7685253493554\n",
            "epoch : 958 training loss : 901268.9375 testing loss : 624.658860061021\n",
            "epoch : 959 training loss : 897637.125 testing loss : 629.2226038886382\n",
            "epoch : 960 training loss : 900014.8125 testing loss : 627.2325612612531\n",
            "epoch : 961 training loss : 895835.75 testing loss : 633.3063471950261\n",
            "epoch : 962 training loss : 900709.0 testing loss : 642.2459974014654\n",
            "epoch : 963 training loss : 895814.6875 testing loss : 629.5035123888371\n",
            "epoch : 964 training loss : 896154.25 testing loss : 616.2815336695814\n",
            "epoch : 965 training loss : 900126.1875 testing loss : 645.0638259934113\n",
            "epoch : 966 training loss : 916937.4375 testing loss : 627.2316280445166\n",
            "epoch : 967 training loss : 924434.0 testing loss : 663.1575826100543\n",
            "epoch : 968 training loss : 915067.0 testing loss : 626.1242784799728\n",
            "epoch : 969 training loss : 904868.1875 testing loss : 625.081882846039\n",
            "epoch : 970 training loss : 899101.0625 testing loss : 630.076561748454\n",
            "epoch : 971 training loss : 902820.125 testing loss : 637.3717206410602\n",
            "epoch : 972 training loss : 898169.375 testing loss : 628.7664259999199\n",
            "epoch : 973 training loss : 904275.875 testing loss : 642.1780418779998\n",
            "epoch : 974 training loss : 900230.125 testing loss : 629.0075202178111\n",
            "epoch : 975 training loss : 895310.25 testing loss : 614.7321994389052\n",
            "epoch : 976 training loss : 898929.375 testing loss : 649.5231811029721\n",
            "epoch : 977 training loss : 899860.25 testing loss : 611.4038536590813\n",
            "epoch : 978 training loss : 900527.875 testing loss : 648.3663719628764\n",
            "epoch : 979 training loss : 897359.625 testing loss : 612.494422134045\n",
            "epoch : 980 training loss : 897120.4375 testing loss : 655.6144363247188\n",
            "epoch : 981 training loss : 898691.375 testing loss : 620.8790800022867\n",
            "epoch : 982 training loss : 898997.3125 testing loss : 648.3794216371216\n",
            "epoch : 983 training loss : 895674.1875 testing loss : 614.9856259147678\n",
            "epoch : 984 training loss : 898952.25 testing loss : 668.6002841692056\n",
            "epoch : 985 training loss : 903112.375 testing loss : 603.8506447087346\n",
            "epoch : 986 training loss : 915571.75 testing loss : 667.0405589057281\n",
            "epoch : 987 training loss : 908189.9375 testing loss : 623.2975095879715\n",
            "epoch : 988 training loss : 912751.25 testing loss : 646.8092943693684\n",
            "epoch : 989 training loss : 907351.875 testing loss : 626.7824086531074\n",
            "epoch : 990 training loss : 896262.25 testing loss : 640.844988592958\n",
            "epoch : 991 training loss : 893728.3125 testing loss : 612.9267194946256\n",
            "epoch : 992 training loss : 895445.4375 testing loss : 649.1848364614807\n",
            "epoch : 993 training loss : 897876.625 testing loss : 629.9429649061861\n",
            "epoch : 994 training loss : 900351.0625 testing loss : 620.4170111576013\n",
            "epoch : 995 training loss : 898870.8125 testing loss : 635.0241337307787\n",
            "epoch : 996 training loss : 894757.8125 testing loss : 626.7671999952435\n",
            "epoch : 997 training loss : 897074.625 testing loss : 628.7700053168609\n",
            "epoch : 998 training loss : 894547.5625 testing loss : 642.2125908484501\n",
            "epoch : 999 training loss : 892347.4375 testing loss : 615.6530712410412\n",
            "epoch : 1000 training loss : 894564.75 testing loss : 641.4176062841331\n",
            "epoch : 1001 training loss : 896232.4375 testing loss : 633.2863313096815\n",
            "epoch : 1002 training loss : 895103.4375 testing loss : 623.0821385995476\n",
            "epoch : 1003 training loss : 898510.75 testing loss : 657.2912637132459\n",
            "epoch : 1004 training loss : 899184.6875 testing loss : 609.72760941286\n",
            "epoch : 1005 training loss : 898293.375 testing loss : 665.273583232829\n",
            "epoch : 1006 training loss : 896380.125 testing loss : 613.6321721351252\n",
            "epoch : 1007 training loss : 900983.625 testing loss : 646.359217076175\n",
            "epoch : 1008 training loss : 898233.5625 testing loss : 622.4829237587684\n",
            "epoch : 1009 training loss : 897226.0 testing loss : 615.5149510455343\n",
            "epoch : 1010 training loss : 902405.5 testing loss : 658.6729645496976\n",
            "epoch : 1011 training loss : 901950.75 testing loss : 609.2084528471516\n",
            "epoch : 1012 training loss : 903698.75 testing loss : 645.239684988967\n",
            "epoch : 1013 training loss : 895362.0625 testing loss : 621.286353488939\n",
            "epoch : 1014 training loss : 898717.125 testing loss : 653.9724131537749\n",
            "epoch : 1015 training loss : 898062.4375 testing loss : 610.4418516559938\n",
            "epoch : 1016 training loss : 898349.875 testing loss : 661.654012694823\n",
            "epoch : 1017 training loss : 893636.3125 testing loss : 600.5761501324915\n",
            "epoch : 1018 training loss : 903920.25 testing loss : 651.8717652021256\n",
            "epoch : 1019 training loss : 901539.5 testing loss : 631.27242002023\n",
            "epoch : 1020 training loss : 894579.25 testing loss : 623.1544990518452\n",
            "epoch : 1021 training loss : 894504.125 testing loss : 633.8960638447145\n",
            "epoch : 1022 training loss : 896337.8125 testing loss : 627.9768437727363\n",
            "epoch : 1023 training loss : 895359.6875 testing loss : 636.0433106190335\n",
            "epoch : 1024 training loss : 892056.875 testing loss : 634.498489816632\n",
            "epoch : 1025 training loss : 893639.4375 testing loss : 623.7555014665148\n",
            "epoch : 1026 training loss : 895850.5625 testing loss : 650.7979295422546\n",
            "epoch : 1027 training loss : 901809.25 testing loss : 615.2991890337615\n",
            "epoch : 1028 training loss : 899798.5 testing loss : 645.9827350494081\n",
            "epoch : 1029 training loss : 895101.25 testing loss : 624.7602947517834\n",
            "epoch : 1030 training loss : 898353.375 testing loss : 629.6581656173267\n",
            "epoch : 1031 training loss : 896076.0625 testing loss : 633.6401486333492\n",
            "epoch : 1032 training loss : 897880.75 testing loss : 618.9988881993082\n",
            "epoch : 1033 training loss : 895880.5625 testing loss : 643.1858169061948\n",
            "epoch : 1034 training loss : 895734.875 testing loss : 633.0428137969127\n",
            "epoch : 1035 training loss : 897718.625 testing loss : 616.1404050282672\n",
            "epoch : 1036 training loss : 895439.5 testing loss : 624.2026259139576\n",
            "epoch : 1037 training loss : 898981.3125 testing loss : 664.6204762057921\n",
            "epoch : 1038 training loss : 902253.75 testing loss : 600.0255323743398\n",
            "epoch : 1039 training loss : 903326.0625 testing loss : 674.2124140220405\n",
            "epoch : 1040 training loss : 913027.75 testing loss : 616.3375047308153\n",
            "epoch : 1041 training loss : 910075.1875 testing loss : 647.3028706217234\n",
            "epoch : 1042 training loss : 902921.75 testing loss : 626.8794457068485\n",
            "epoch : 1043 training loss : 901467.75 testing loss : 626.6602052988204\n",
            "epoch : 1044 training loss : 893261.8125 testing loss : 632.5237756556114\n",
            "epoch : 1045 training loss : 894086.4375 testing loss : 617.9953723650063\n",
            "epoch : 1046 training loss : 893225.0 testing loss : 637.1874818358801\n",
            "epoch : 1047 training loss : 896064.625 testing loss : 622.4375822333108\n",
            "epoch : 1048 training loss : 895807.5625 testing loss : 629.7454078113084\n",
            "epoch : 1049 training loss : 892438.0 testing loss : 613.7487206902124\n",
            "epoch : 1050 training loss : 891800.875 testing loss : 635.5725677604169\n",
            "epoch : 1051 training loss : 888788.625 testing loss : 627.217438113373\n",
            "epoch : 1052 training loss : 893364.6875 testing loss : 639.4158333951393\n",
            "epoch : 1053 training loss : 891247.5 testing loss : 620.1894908647622\n",
            "epoch : 1054 training loss : 893341.625 testing loss : 636.7816565163367\n",
            "epoch : 1055 training loss : 891707.0 testing loss : 621.2034598810483\n",
            "epoch : 1056 training loss : 895343.9375 testing loss : 623.2667932067297\n",
            "epoch : 1057 training loss : 894432.75 testing loss : 627.8896532290805\n",
            "epoch : 1058 training loss : 893741.25 testing loss : 636.9395833627312\n",
            "epoch : 1059 training loss : 890559.3125 testing loss : 616.4925814244599\n",
            "epoch : 1060 training loss : 897083.6875 testing loss : 660.3953762202136\n",
            "epoch : 1061 training loss : 898879.375 testing loss : 601.0722568119521\n",
            "epoch : 1062 training loss : 897986.5 testing loss : 670.0613608212598\n",
            "epoch : 1063 training loss : 896033.0 testing loss : 603.5382776154881\n",
            "epoch : 1064 training loss : 901433.5 testing loss : 656.0505159745175\n",
            "epoch : 1065 training loss : 895814.0625 testing loss : 610.7565192817586\n",
            "epoch : 1066 training loss : 900111.0 testing loss : 631.5385390404051\n",
            "epoch : 1067 training loss : 892626.4375 testing loss : 633.9297838780732\n",
            "epoch : 1068 training loss : 889303.0 testing loss : 628.4689255752395\n",
            "epoch : 1069 training loss : 889838.625 testing loss : 628.1485574751829\n",
            "epoch : 1070 training loss : 895887.5 testing loss : 637.3555393704271\n",
            "epoch : 1071 training loss : 892777.875 testing loss : 613.6127899959024\n",
            "epoch : 1072 training loss : 889265.0 testing loss : 637.0413708285948\n",
            "epoch : 1073 training loss : 889353.0 testing loss : 618.6908995008047\n",
            "epoch : 1074 training loss : 895619.5 testing loss : 616.3378188525681\n",
            "epoch : 1075 training loss : 891084.5 testing loss : 637.4871236054244\n",
            "epoch : 1076 training loss : 901077.1875 testing loss : 634.3161603855875\n",
            "epoch : 1077 training loss : 891705.0 testing loss : 626.5158675856295\n",
            "epoch : 1078 training loss : 892129.5 testing loss : 629.9200074166323\n",
            "epoch : 1079 training loss : 890289.625 testing loss : 635.8890469960406\n",
            "epoch : 1080 training loss : 890545.75 testing loss : 628.5226054212688\n",
            "epoch : 1081 training loss : 891175.375 testing loss : 623.7942770202603\n",
            "epoch : 1082 training loss : 891348.9375 testing loss : 638.0797664654993\n",
            "epoch : 1083 training loss : 887623.25 testing loss : 627.3381483681434\n",
            "epoch : 1084 training loss : 887155.375 testing loss : 621.118749059407\n",
            "epoch : 1085 training loss : 887209.6875 testing loss : 638.483249155821\n",
            "epoch : 1086 training loss : 888437.125 testing loss : 623.3556459456419\n",
            "epoch : 1087 training loss : 885586.3125 testing loss : 639.3272514659747\n",
            "epoch : 1088 training loss : 885439.0 testing loss : 618.0677436875031\n",
            "epoch : 1089 training loss : 886243.875 testing loss : 635.1113147883289\n",
            "epoch : 1090 training loss : 888752.625 testing loss : 620.1577396540515\n",
            "epoch : 1091 training loss : 885406.875 testing loss : 623.6264679811698\n",
            "epoch : 1092 training loss : 886228.625 testing loss : 626.6138996677062\n",
            "epoch : 1093 training loss : 885771.625 testing loss : 623.8749088502564\n",
            "epoch : 1094 training loss : 888000.125 testing loss : 640.3345733553963\n",
            "epoch : 1095 training loss : 887724.0 testing loss : 614.5414875650828\n",
            "epoch : 1096 training loss : 887868.0625 testing loss : 653.1511376283866\n",
            "epoch : 1097 training loss : 891104.4375 testing loss : 610.2787107598465\n",
            "epoch : 1098 training loss : 885670.9375 testing loss : 669.6305101424192\n",
            "epoch : 1099 training loss : 896296.125 testing loss : 604.8490563430618\n",
            "epoch : 1100 training loss : 892290.3125 testing loss : 644.2676072226161\n",
            "epoch : 1101 training loss : 889244.75 testing loss : 615.2383180445274\n",
            "epoch : 1102 training loss : 888852.875 testing loss : 633.5373039899674\n",
            "epoch : 1103 training loss : 890004.0625 testing loss : 620.2065104573173\n",
            "epoch : 1104 training loss : 886969.4375 testing loss : 628.5099105813862\n",
            "epoch : 1105 training loss : 886268.875 testing loss : 625.9505726615939\n",
            "epoch : 1106 training loss : 890618.375 testing loss : 652.9540679707991\n",
            "epoch : 1107 training loss : 893206.3125 testing loss : 607.4360445389706\n",
            "epoch : 1108 training loss : 892259.75 testing loss : 654.5810448279423\n",
            "epoch : 1109 training loss : 890605.375 testing loss : 601.4476841930795\n",
            "epoch : 1110 training loss : 895192.4375 testing loss : 647.8667425624037\n",
            "epoch : 1111 training loss : 885348.0 testing loss : 619.1723163887463\n",
            "epoch : 1112 training loss : 890068.4375 testing loss : 644.7442662378328\n",
            "epoch : 1113 training loss : 887598.375 testing loss : 612.8063948281043\n",
            "epoch : 1114 training loss : 891655.0 testing loss : 642.5530963640298\n",
            "epoch : 1115 training loss : 895556.375 testing loss : 606.1931002773015\n",
            "epoch : 1116 training loss : 898889.8125 testing loss : 664.976177280983\n",
            "epoch : 1117 training loss : 900904.3125 testing loss : 610.5484596163826\n",
            "epoch : 1118 training loss : 895337.1875 testing loss : 651.586028746799\n",
            "epoch : 1119 training loss : 898376.5 testing loss : 625.6988461903766\n",
            "epoch : 1120 training loss : 889484.5625 testing loss : 621.8503513483874\n",
            "epoch : 1121 training loss : 885344.3125 testing loss : 621.9237746496116\n",
            "epoch : 1122 training loss : 888412.25 testing loss : 625.1186861337814\n",
            "epoch : 1123 training loss : 888752.25 testing loss : 653.5217666942461\n",
            "epoch : 1124 training loss : 889976.5625 testing loss : 610.5469484561313\n",
            "epoch : 1125 training loss : 892915.5 testing loss : 652.6606042891477\n",
            "epoch : 1126 training loss : 894139.0 testing loss : 618.5031517480327\n",
            "epoch : 1127 training loss : 898561.1875 testing loss : 621.5813136797036\n",
            "epoch : 1128 training loss : 897202.5625 testing loss : 648.6306484505138\n",
            "epoch : 1129 training loss : 895539.0 testing loss : 596.6520822216979\n",
            "epoch : 1130 training loss : 896873.0 testing loss : 665.6837611557108\n",
            "epoch : 1131 training loss : 899400.875 testing loss : 610.3963389417767\n",
            "epoch : 1132 training loss : 893381.0625 testing loss : 639.0923842012355\n",
            "epoch : 1133 training loss : 887473.4375 testing loss : 622.2782224110797\n",
            "epoch : 1134 training loss : 886005.0625 testing loss : 619.048034598342\n",
            "epoch : 1135 training loss : 884645.25 testing loss : 621.7338321968517\n",
            "epoch : 1136 training loss : 886572.875 testing loss : 633.2622419690664\n",
            "epoch : 1137 training loss : 884579.8125 testing loss : 628.1065477333237\n",
            "epoch : 1138 training loss : 885240.25 testing loss : 626.1244863978529\n",
            "epoch : 1139 training loss : 886000.75 testing loss : 626.649710560267\n",
            "epoch : 1140 training loss : 883223.375 testing loss : 623.811358019314\n",
            "epoch : 1141 training loss : 888150.9375 testing loss : 636.2860969927459\n",
            "epoch : 1142 training loss : 893420.375 testing loss : 613.6352344930699\n",
            "epoch : 1143 training loss : 888272.875 testing loss : 653.8606757083826\n",
            "epoch : 1144 training loss : 900686.125 testing loss : 612.1770890138846\n",
            "epoch : 1145 training loss : 895900.125 testing loss : 633.5501154313046\n",
            "epoch : 1146 training loss : 890078.3125 testing loss : 618.0690090867271\n",
            "epoch : 1147 training loss : 884654.875 testing loss : 636.3812593422105\n",
            "epoch : 1148 training loss : 886685.375 testing loss : 634.1185375293799\n",
            "epoch : 1149 training loss : 889192.0 testing loss : 622.3531152147108\n",
            "epoch : 1150 training loss : 893912.75 testing loss : 648.341837855567\n",
            "epoch : 1151 training loss : 899645.5 testing loss : 611.8820676866886\n",
            "epoch : 1152 training loss : 894975.75 testing loss : 635.3610839485067\n",
            "epoch : 1153 training loss : 889601.0 testing loss : 634.2360899912572\n",
            "epoch : 1154 training loss : 894719.6875 testing loss : 622.3768061726494\n",
            "epoch : 1155 training loss : 888627.0625 testing loss : 625.2253650880493\n",
            "epoch : 1156 training loss : 888489.875 testing loss : 642.7784311054028\n",
            "epoch : 1157 training loss : 894483.5 testing loss : 618.45063808534\n",
            "epoch : 1158 training loss : 889342.9375 testing loss : 613.8782125806387\n",
            "epoch : 1159 training loss : 893910.125 testing loss : 638.9053782214106\n",
            "epoch : 1160 training loss : 889593.8125 testing loss : 638.5763976151965\n",
            "epoch : 1161 training loss : 883887.4375 testing loss : 616.7433655979358\n",
            "epoch : 1162 training loss : 886161.75 testing loss : 626.4210713458273\n",
            "epoch : 1163 training loss : 885335.25 testing loss : 628.1729707612401\n",
            "epoch : 1164 training loss : 888976.5 testing loss : 642.1405179162996\n",
            "epoch : 1165 training loss : 892876.625 testing loss : 618.4186326571271\n",
            "epoch : 1166 training loss : 891318.1875 testing loss : 639.841648471039\n",
            "epoch : 1167 training loss : 889382.125 testing loss : 625.5952907980015\n",
            "epoch : 1168 training loss : 887270.8125 testing loss : 621.6676182346007\n",
            "epoch : 1169 training loss : 889387.375 testing loss : 651.0066194766391\n",
            "epoch : 1170 training loss : 890969.875 testing loss : 614.2552346402565\n",
            "epoch : 1171 training loss : 892570.25 testing loss : 622.2548416589214\n",
            "epoch : 1172 training loss : 886559.8125 testing loss : 624.9649827311524\n",
            "epoch : 1173 training loss : 884523.4375 testing loss : 635.5395016944514\n",
            "epoch : 1174 training loss : 883493.375 testing loss : 615.5022178561287\n",
            "epoch : 1175 training loss : 886373.6875 testing loss : 640.1299561335977\n",
            "epoch : 1176 training loss : 886279.625 testing loss : 603.4757643484436\n",
            "epoch : 1177 training loss : 895077.0625 testing loss : 626.465353803297\n",
            "epoch : 1178 training loss : 883978.6875 testing loss : 617.0131120913852\n",
            "epoch : 1179 training loss : 885233.75 testing loss : 620.819268654933\n",
            "epoch : 1180 training loss : 885145.875 testing loss : 643.8878610978085\n",
            "epoch : 1181 training loss : 886110.625 testing loss : 608.8717042344862\n",
            "epoch : 1182 training loss : 888869.1875 testing loss : 629.0454078923284\n",
            "epoch : 1183 training loss : 890365.8125 testing loss : 622.1911503931062\n",
            "epoch : 1184 training loss : 888560.25 testing loss : 620.1697359654755\n",
            "epoch : 1185 training loss : 883882.375 testing loss : 637.0018268665381\n",
            "epoch : 1186 training loss : 888121.3125 testing loss : 621.9145826470535\n",
            "epoch : 1187 training loss : 882887.25 testing loss : 628.3362361920618\n",
            "epoch : 1188 training loss : 885766.125 testing loss : 621.5619935546301\n",
            "epoch : 1189 training loss : 889845.75 testing loss : 621.7879477623289\n",
            "epoch : 1190 training loss : 888048.125 testing loss : 630.1731271300696\n",
            "epoch : 1191 training loss : 888127.3125 testing loss : 628.8365928156186\n",
            "epoch : 1192 training loss : 885316.3125 testing loss : 612.5419818215665\n",
            "epoch : 1193 training loss : 887974.75 testing loss : 639.6594864815737\n",
            "epoch : 1194 training loss : 891033.5625 testing loss : 609.3478040420904\n",
            "epoch : 1195 training loss : 889085.625 testing loss : 634.3395710139148\n",
            "epoch : 1196 training loss : 884702.75 testing loss : 623.4990060160645\n",
            "epoch : 1197 training loss : 888133.0 testing loss : 630.5658488969888\n",
            "epoch : 1198 training loss : 882876.4375 testing loss : 620.9519840320654\n",
            "epoch : 1199 training loss : 884321.125 testing loss : 631.3122534604199\n",
            "epoch : 1200 training loss : 883235.5625 testing loss : 628.8924668215018\n",
            "epoch : 1201 training loss : 882752.625 testing loss : 631.5854996854225\n",
            "epoch : 1202 training loss : 886164.75 testing loss : 630.612931207218\n",
            "epoch : 1203 training loss : 886733.625 testing loss : 606.344963375446\n",
            "epoch : 1204 training loss : 882307.8125 testing loss : 643.5512132539159\n",
            "epoch : 1205 training loss : 884096.75 testing loss : 614.1403641131072\n",
            "epoch : 1206 training loss : 883555.125 testing loss : 639.8676673733028\n",
            "epoch : 1207 training loss : 882471.1875 testing loss : 618.0659877789759\n",
            "epoch : 1208 training loss : 881213.875 testing loss : 645.7596289014394\n",
            "epoch : 1209 training loss : 884650.75 testing loss : 602.2397284697643\n",
            "epoch : 1210 training loss : 887921.625 testing loss : 636.9712977092878\n",
            "epoch : 1211 training loss : 888641.8125 testing loss : 623.3275120806906\n",
            "epoch : 1212 training loss : 883345.375 testing loss : 623.1392419169434\n",
            "epoch : 1213 training loss : 885189.125 testing loss : 654.3078577138681\n",
            "epoch : 1214 training loss : 895459.25 testing loss : 598.1057447564285\n",
            "epoch : 1215 training loss : 891411.875 testing loss : 644.1632907812574\n",
            "epoch : 1216 training loss : 892939.0 testing loss : 632.3955690480966\n",
            "epoch : 1217 training loss : 889019.9375 testing loss : 603.865844758211\n",
            "epoch : 1218 training loss : 888385.6875 testing loss : 644.1543367415403\n",
            "epoch : 1219 training loss : 881816.375 testing loss : 625.2654495893327\n",
            "epoch : 1220 training loss : 880233.8125 testing loss : 624.4664654668453\n",
            "epoch : 1221 training loss : 884304.75 testing loss : 621.1542562953139\n",
            "epoch : 1222 training loss : 886616.125 testing loss : 652.7502155451648\n",
            "epoch : 1223 training loss : 896721.3125 testing loss : 606.5829512313404\n",
            "epoch : 1224 training loss : 886997.75 testing loss : 631.1531897667235\n",
            "epoch : 1225 training loss : 884799.0625 testing loss : 616.4398957965649\n",
            "epoch : 1226 training loss : 883404.875 testing loss : 630.3879747622836\n",
            "epoch : 1227 training loss : 883333.375 testing loss : 624.4060032578697\n",
            "epoch : 1228 training loss : 891059.0625 testing loss : 620.4114916957585\n",
            "epoch : 1229 training loss : 892748.4375 testing loss : 638.8788390349498\n",
            "epoch : 1230 training loss : 890281.8125 testing loss : 622.8632020507239\n",
            "epoch : 1231 training loss : 885582.8125 testing loss : 617.096483414152\n",
            "epoch : 1232 training loss : 889886.625 testing loss : 621.671914393923\n",
            "epoch : 1233 training loss : 881205.375 testing loss : 647.5026451359807\n",
            "epoch : 1234 training loss : 882801.375 testing loss : 609.2573558482449\n",
            "epoch : 1235 training loss : 880114.25 testing loss : 636.4199962426076\n",
            "epoch : 1236 training loss : 880729.5625 testing loss : 615.6596765412694\n",
            "epoch : 1237 training loss : 882579.0625 testing loss : 633.6634865461198\n",
            "epoch : 1238 training loss : 882017.125 testing loss : 626.5418319005881\n",
            "epoch : 1239 training loss : 883753.875 testing loss : 626.1376810643525\n",
            "epoch : 1240 training loss : 886905.125 testing loss : 612.0152358650106\n",
            "epoch : 1241 training loss : 887597.125 testing loss : 650.3560791669694\n",
            "epoch : 1242 training loss : 888274.0 testing loss : 604.07300023906\n",
            "epoch : 1243 training loss : 882621.1875 testing loss : 638.4008001795912\n",
            "epoch : 1244 training loss : 884640.0 testing loss : 620.213621795705\n",
            "epoch : 1245 training loss : 883519.375 testing loss : 608.0412620253268\n",
            "epoch : 1246 training loss : 882792.1875 testing loss : 652.4321382889706\n",
            "epoch : 1247 training loss : 883995.0 testing loss : 613.921670962224\n",
            "epoch : 1248 training loss : 880478.5 testing loss : 621.8326255304623\n",
            "epoch : 1249 training loss : 885762.625 testing loss : 654.2442009491203\n",
            "epoch : 1250 training loss : 893643.5 testing loss : 595.2736199957079\n",
            "epoch : 1251 training loss : 889193.875 testing loss : 651.5069902787167\n",
            "epoch : 1252 training loss : 881885.25 testing loss : 617.0580333587343\n",
            "epoch : 1253 training loss : 879231.25 testing loss : 644.2926481445279\n",
            "epoch : 1254 training loss : 883343.375 testing loss : 603.1468464315465\n",
            "epoch : 1255 training loss : 901768.3125 testing loss : 671.5069946335481\n",
            "epoch : 1256 training loss : 888652.0625 testing loss : 607.1068469338712\n",
            "epoch : 1257 training loss : 896134.875 testing loss : 675.2370271239661\n",
            "epoch : 1258 training loss : 894555.9375 testing loss : 604.4004794411954\n",
            "epoch : 1259 training loss : 889624.375 testing loss : 649.2659631556114\n",
            "epoch : 1260 training loss : 885455.5625 testing loss : 618.8848190961686\n",
            "epoch : 1261 training loss : 889432.625 testing loss : 626.370159315852\n",
            "epoch : 1262 training loss : 890550.25 testing loss : 633.0736699041012\n",
            "epoch : 1263 training loss : 887535.625 testing loss : 618.5163004018564\n",
            "epoch : 1264 training loss : 891587.0 testing loss : 625.9087534858062\n",
            "epoch : 1265 training loss : 882022.125 testing loss : 621.0178223647903\n",
            "epoch : 1266 training loss : 881392.875 testing loss : 625.603435379214\n",
            "epoch : 1267 training loss : 877505.875 testing loss : 611.7376413915009\n",
            "epoch : 1268 training loss : 878648.625 testing loss : 634.4939348993048\n",
            "epoch : 1269 training loss : 879484.5 testing loss : 616.9689508602683\n",
            "epoch : 1270 training loss : 881319.0 testing loss : 636.8167721212437\n",
            "epoch : 1271 training loss : 885541.0625 testing loss : 609.1987639549559\n",
            "epoch : 1272 training loss : 886931.9375 testing loss : 648.9298801569812\n",
            "epoch : 1273 training loss : 891724.3125 testing loss : 620.0952566345181\n",
            "epoch : 1274 training loss : 892639.625 testing loss : 608.3316382327966\n",
            "epoch : 1275 training loss : 894361.9375 testing loss : 655.1125629707775\n",
            "epoch : 1276 training loss : 888475.25 testing loss : 600.1548872057316\n",
            "epoch : 1277 training loss : 886036.375 testing loss : 647.8485611075848\n",
            "epoch : 1278 training loss : 885558.9375 testing loss : 603.1295345251539\n",
            "epoch : 1279 training loss : 885949.6875 testing loss : 639.7639267149225\n",
            "epoch : 1280 training loss : 880928.125 testing loss : 611.0582679457369\n",
            "epoch : 1281 training loss : 879094.125 testing loss : 616.4384014479882\n",
            "epoch : 1282 training loss : 880095.6875 testing loss : 630.1771108893166\n",
            "epoch : 1283 training loss : 877649.5 testing loss : 615.2931128286682\n",
            "epoch : 1284 training loss : 878677.5625 testing loss : 618.7828304978599\n",
            "epoch : 1285 training loss : 878917.8125 testing loss : 633.568825339849\n",
            "epoch : 1286 training loss : 877354.4375 testing loss : 619.033324518035\n",
            "epoch : 1287 training loss : 879251.25 testing loss : 640.4393631905581\n",
            "epoch : 1288 training loss : 882139.0625 testing loss : 600.9467681256016\n",
            "epoch : 1289 training loss : 880399.1875 testing loss : 641.5768782332935\n",
            "epoch : 1290 training loss : 879873.875 testing loss : 609.6214338298392\n",
            "epoch : 1291 training loss : 879077.875 testing loss : 625.3583701458653\n",
            "epoch : 1292 training loss : 877507.125 testing loss : 635.8386474183176\n",
            "epoch : 1293 training loss : 877431.875 testing loss : 616.3846320072107\n",
            "epoch : 1294 training loss : 876151.625 testing loss : 624.6730251797533\n",
            "epoch : 1295 training loss : 875523.625 testing loss : 617.618299397747\n",
            "epoch : 1296 training loss : 878556.875 testing loss : 641.836502208119\n",
            "epoch : 1297 training loss : 875337.75 testing loss : 615.7556156036073\n",
            "epoch : 1298 training loss : 877723.625 testing loss : 617.3046185970306\n",
            "epoch : 1299 training loss : 875268.25 testing loss : 622.264883393735\n",
            "epoch : 1300 training loss : 877606.4375 testing loss : 644.3884101441477\n",
            "epoch : 1301 training loss : 881629.25 testing loss : 608.5573733148321\n",
            "epoch : 1302 training loss : 888409.1875 testing loss : 631.8648414928301\n",
            "epoch : 1303 training loss : 879779.625 testing loss : 604.1123848277911\n",
            "epoch : 1304 training loss : 900885.125 testing loss : 615.9482758087395\n",
            "epoch : 1305 training loss : 893414.375 testing loss : 663.2422073140608\n",
            "epoch : 1306 training loss : 897642.9375 testing loss : 612.6521588532271\n",
            "epoch : 1307 training loss : 890552.375 testing loss : 632.436436136212\n",
            "epoch : 1308 training loss : 899604.125 testing loss : 628.6493658534193\n",
            "epoch : 1309 training loss : 890093.875 testing loss : 618.5314110591348\n",
            "epoch : 1310 training loss : 891703.1875 testing loss : 654.9403532901697\n",
            "epoch : 1311 training loss : 886874.75 testing loss : 630.1218108434592\n",
            "epoch : 1312 training loss : 881163.25 testing loss : 626.7906545432268\n",
            "epoch : 1313 training loss : 884570.6875 testing loss : 624.8646924158113\n",
            "epoch : 1314 training loss : 879728.125 testing loss : 628.5325434840886\n",
            "epoch : 1315 training loss : 878868.0625 testing loss : 617.2458618277997\n",
            "epoch : 1316 training loss : 879273.5 testing loss : 628.2844767254011\n",
            "epoch : 1317 training loss : 879166.5625 testing loss : 620.7291339076726\n",
            "epoch : 1318 training loss : 877572.75 testing loss : 617.804791271159\n",
            "epoch : 1319 training loss : 877344.125 testing loss : 628.3772322371998\n",
            "epoch : 1320 training loss : 874897.875 testing loss : 619.287519706034\n",
            "epoch : 1321 training loss : 876320.0625 testing loss : 629.825154564022\n",
            "epoch : 1322 training loss : 873184.125 testing loss : 605.1205191802135\n",
            "epoch : 1323 training loss : 886277.6875 testing loss : 633.3880392407949\n",
            "epoch : 1324 training loss : 879176.875 testing loss : 619.5519477350522\n",
            "epoch : 1325 training loss : 875763.8125 testing loss : 615.8242509533874\n",
            "epoch : 1326 training loss : 876244.875 testing loss : 617.9616342371544\n",
            "epoch : 1327 training loss : 881200.0 testing loss : 637.892547166453\n",
            "epoch : 1328 training loss : 879407.625 testing loss : 606.528678891933\n",
            "epoch : 1329 training loss : 875925.125 testing loss : 649.5477400902098\n",
            "epoch : 1330 training loss : 877738.125 testing loss : 600.861222931769\n",
            "epoch : 1331 training loss : 881280.1875 testing loss : 615.3515447409807\n",
            "epoch : 1332 training loss : 876287.0625 testing loss : 633.1502506268763\n",
            "epoch : 1333 training loss : 875415.4375 testing loss : 615.2755251838042\n",
            "epoch : 1334 training loss : 879275.0 testing loss : 600.408638842338\n",
            "epoch : 1335 training loss : 876939.6875 testing loss : 655.5862830149389\n",
            "epoch : 1336 training loss : 882807.3125 testing loss : 596.8668940025093\n",
            "epoch : 1337 training loss : 884920.9375 testing loss : 640.9894954356472\n",
            "epoch : 1338 training loss : 878191.0 testing loss : 623.3298066717333\n",
            "epoch : 1339 training loss : 875383.25 testing loss : 619.4681481871985\n",
            "epoch : 1340 training loss : 876215.0625 testing loss : 622.3147985386637\n",
            "epoch : 1367 training loss : 884600.0 testing loss : 633.4561629147656\n",
            "epoch : 1368 training loss : 877372.3125 testing loss : 601.3691471044996\n",
            "epoch : 1369 training loss : 877930.5 testing loss : 635.0459760796707\n",
            "epoch : 1370 training loss : 875050.625 testing loss : 625.7731454541198\n",
            "epoch : 1371 training loss : 875965.875 testing loss : 610.3860925974044\n",
            "epoch : 1372 training loss : 877613.375 testing loss : 637.391232209923\n",
            "epoch : 1373 training loss : 878365.25 testing loss : 609.8612064576782\n",
            "epoch : 1374 training loss : 874744.5 testing loss : 623.8772851703442\n",
            "epoch : 1375 training loss : 874321.1875 testing loss : 609.0543672321118\n",
            "epoch : 1376 training loss : 879886.125 testing loss : 609.5437536302921\n",
            "epoch : 1377 training loss : 880939.25 testing loss : 662.0041669301227\n",
            "epoch : 1378 training loss : 893277.9375 testing loss : 589.4825319589767\n",
            "epoch : 1379 training loss : 876403.5 testing loss : 673.2379651681512\n",
            "epoch : 1380 training loss : 884770.5 testing loss : 600.4819921624344\n",
            "epoch : 1381 training loss : 884570.9375 testing loss : 620.1073231633785\n",
            "epoch : 1382 training loss : 883749.0 testing loss : 634.507993847923\n",
            "epoch : 1383 training loss : 884496.4375 testing loss : 589.4743602697828\n",
            "epoch : 1384 training loss : 883539.5625 testing loss : 660.363073870144\n",
            "epoch : 1385 training loss : 879118.125 testing loss : 600.8730299936987\n",
            "epoch : 1386 training loss : 878149.0 testing loss : 656.1022732806417\n",
            "epoch : 1387 training loss : 881995.5 testing loss : 612.0823198820638\n",
            "epoch : 1388 training loss : 878507.6875 testing loss : 609.6729824416406\n",
            "epoch : 1389 training loss : 884935.5 testing loss : 633.3491401651264\n",
            "epoch : 1390 training loss : 878931.5 testing loss : 602.1716756883976\n",
            "epoch : 1391 training loss : 881446.6875 testing loss : 626.1519871243333\n",
            "epoch : 1392 training loss : 872461.4375 testing loss : 628.0654642877325\n",
            "epoch : 1393 training loss : 874230.6875 testing loss : 598.8039523753445\n",
            "epoch : 1394 training loss : 876476.0 testing loss : 648.949533949911\n",
            "epoch : 1395 training loss : 874406.3125 testing loss : 610.8799124840086\n",
            "epoch : 1396 training loss : 872038.8125 testing loss : 625.4803000234924\n",
            "epoch : 1397 training loss : 869272.6875 testing loss : 612.5387337249992\n",
            "epoch : 1398 training loss : 883038.125 testing loss : 617.5018236257333\n",
            "epoch : 1399 training loss : 889379.3125 testing loss : 640.7930953080676\n",
            "epoch : 1400 training loss : 883358.5625 testing loss : 624.3465943441981\n",
            "epoch : 1401 training loss : 880409.0625 testing loss : 639.0626809765807\n",
            "epoch : 1402 training loss : 879838.6875 testing loss : 619.3655133521662\n",
            "epoch : 1403 training loss : 878127.125 testing loss : 613.274218059219\n",
            "epoch : 1404 training loss : 878918.25 testing loss : 621.0809988152664\n",
            "epoch : 1405 training loss : 874680.125 testing loss : 627.3980423897768\n",
            "epoch : 1406 training loss : 870530.25 testing loss : 612.6788316553673\n",
            "epoch : 1407 training loss : 870676.75 testing loss : 618.5831235341266\n",
            "epoch : 1408 training loss : 869469.5 testing loss : 625.6539558119479\n",
            "epoch : 1409 training loss : 870482.6875 testing loss : 606.4219377547239\n",
            "epoch : 1410 training loss : 875666.3125 testing loss : 626.112583732183\n",
            "epoch : 1411 training loss : 879392.75 testing loss : 621.2184034680898\n",
            "epoch : 1412 training loss : 881619.0 testing loss : 610.847135862418\n",
            "epoch : 1413 training loss : 884906.375 testing loss : 646.9848894418868\n",
            "epoch : 1414 training loss : 890973.0 testing loss : 609.898372242936\n",
            "epoch : 1415 training loss : 889153.5 testing loss : 646.4571967652414\n",
            "epoch : 1416 training loss : 887176.625 testing loss : 615.5358381672243\n",
            "epoch : 1417 training loss : 875970.375 testing loss : 610.8761975765228\n",
            "epoch : 1418 training loss : 874483.3125 testing loss : 623.2780706671487\n",
            "epoch : 1419 training loss : 873162.625 testing loss : 613.6952714476965\n",
            "epoch : 1420 training loss : 882440.0 testing loss : 605.1966779232025\n",
            "epoch : 1421 training loss : 881650.25 testing loss : 627.877679366981\n",
            "epoch : 1422 training loss : 876869.375 testing loss : 596.5115850309355\n",
            "epoch : 1423 training loss : 884446.25 testing loss : 667.978577333214\n",
            "epoch : 1424 training loss : 879859.6875 testing loss : 600.3970330816454\n",
            "epoch : 1425 training loss : 887798.5625 testing loss : 642.4139539646891\n",
            "epoch : 1426 training loss : 881048.6875 testing loss : 604.5753493836496\n",
            "epoch : 1427 training loss : 869002.0 testing loss : 637.0178884974623\n",
            "epoch : 1428 training loss : 873790.625 testing loss : 622.7956146286652\n",
            "epoch : 1429 training loss : 876323.1875 testing loss : 624.3988062407063\n",
            "epoch : 1430 training loss : 875422.6875 testing loss : 605.4514470037105\n",
            "epoch : 1431 training loss : 874385.125 testing loss : 631.4480540499223\n",
            "epoch : 1432 training loss : 874518.0625 testing loss : 620.6432314826324\n",
            "epoch : 1433 training loss : 870120.625 testing loss : 613.643409017968\n",
            "epoch : 1434 training loss : 871274.4375 testing loss : 626.0851239208627\n",
            "epoch : 1435 training loss : 872577.75 testing loss : 623.8298119042827\n",
            "epoch : 1436 training loss : 873521.5625 testing loss : 596.8703894805064\n",
            "epoch : 1437 training loss : 874957.875 testing loss : 654.5321157316191\n",
            "epoch : 1438 training loss : 871749.625 testing loss : 586.302862823537\n",
            "epoch : 1439 training loss : 884932.0625 testing loss : 642.3205536677774\n",
            "epoch : 1440 training loss : 885715.5 testing loss : 625.5260150854566\n",
            "epoch : 1441 training loss : 888272.5 testing loss : 590.2260294867827\n",
            "epoch : 1442 training loss : 888507.25 testing loss : 663.273600078262\n",
            "epoch : 1443 training loss : 884621.0625 testing loss : 589.0943262197275\n",
            "epoch : 1444 training loss : 882404.75 testing loss : 651.0353600472475\n",
            "epoch : 1445 training loss : 883611.6875 testing loss : 599.4469698656977\n",
            "epoch : 1446 training loss : 877085.375 testing loss : 626.4305991092615\n",
            "epoch : 1447 training loss : 873660.5625 testing loss : 617.4291336578606\n",
            "epoch : 1448 training loss : 871323.8125 testing loss : 598.7425860493584\n",
            "epoch : 1449 training loss : 871030.25 testing loss : 646.7531773368869\n",
            "epoch : 1450 training loss : 870390.5625 testing loss : 601.0465709023771\n",
            "epoch : 1451 training loss : 871460.3125 testing loss : 606.0027260210662\n",
            "epoch : 1452 training loss : 873393.125 testing loss : 629.1572220030084\n",
            "epoch : 1453 training loss : 872418.25 testing loss : 603.2928946819981\n",
            "epoch : 1454 training loss : 871428.625 testing loss : 631.7394857469914\n",
            "epoch : 1455 training loss : 873543.75 testing loss : 634.9459075906635\n",
            "epoch : 1456 training loss : 872844.125 testing loss : 590.2207936291146\n",
            "epoch : 1457 training loss : 882185.5625 testing loss : 674.1805760522859\n",
            "epoch : 1458 training loss : 888229.5 testing loss : 605.6712722419637\n",
            "epoch : 1459 training loss : 873381.8125 testing loss : 646.0025754249201\n",
            "epoch : 1460 training loss : 879903.875 testing loss : 621.3132900453247\n",
            "epoch : 1461 training loss : 874204.375 testing loss : 600.3965523285149\n",
            "epoch : 1462 training loss : 872384.4375 testing loss : 629.5728352280845\n",
            "epoch : 1463 training loss : 870286.875 testing loss : 608.7059716060098\n",
            "epoch : 1464 training loss : 870478.0625 testing loss : 630.2679964905292\n",
            "epoch : 1465 training loss : 872620.125 testing loss : 613.5889283893383\n",
            "epoch : 1466 training loss : 868407.25 testing loss : 627.3757967294845\n",
            "epoch : 1467 training loss : 870935.4375 testing loss : 621.5065613447038\n",
            "epoch : 1468 training loss : 868009.25 testing loss : 609.6635743179153\n",
            "epoch : 1469 training loss : 871510.1875 testing loss : 640.2600214101572\n",
            "epoch : 1470 training loss : 865531.6875 testing loss : 596.7611969783243\n",
            "epoch : 1471 training loss : 881353.75 testing loss : 662.3904290304774\n",
            "epoch : 1472 training loss : 882214.25 testing loss : 614.4245585192622\n",
            "epoch : 1473 training loss : 869576.6875 testing loss : 631.0342179336379\n",
            "epoch : 1474 training loss : 874248.4375 testing loss : 617.00593765858\n",
            "epoch : 1475 training loss : 873988.625 testing loss : 618.2103638079315\n",
            "epoch : 1476 training loss : 868185.5 testing loss : 607.2943526052795\n",
            "epoch : 1477 training loss : 869402.5625 testing loss : 637.6284230979143\n",
            "epoch : 1478 training loss : 873428.625 testing loss : 604.9657382775197\n",
            "epoch : 1479 training loss : 871318.0 testing loss : 654.7755015191779\n",
            "epoch : 1480 training loss : 875921.8125 testing loss : 601.6869495674572\n",
            "epoch : 1481 training loss : 871192.75 testing loss : 639.804667647961\n",
            "epoch : 1482 training loss : 875690.5 testing loss : 617.7624417174179\n",
            "epoch : 1483 training loss : 867113.3125 testing loss : 611.9164679514623\n",
            "epoch : 1484 training loss : 870399.125 testing loss : 638.7536393878735\n",
            "epoch : 1485 training loss : 867950.6875 testing loss : 605.7217441639013\n",
            "epoch : 1486 training loss : 870792.1875 testing loss : 618.107299397477\n",
            "epoch : 1487 training loss : 869386.8125 testing loss : 624.6956401567544\n",
            "epoch : 1488 training loss : 869794.125 testing loss : 603.5377739404155\n",
            "epoch : 1489 training loss : 874493.125 testing loss : 633.3891416545463\n",
            "epoch : 1490 training loss : 872037.5 testing loss : 615.1771600415221\n",
            "epoch : 1491 training loss : 872853.125 testing loss : 613.5082223246583\n",
            "epoch : 1492 training loss : 865404.25 testing loss : 627.3975936395932\n",
            "epoch : 1493 training loss : 869661.9375 testing loss : 611.4567993615581\n",
            "epoch : 1494 training loss : 866945.5 testing loss : 613.1454973115331\n",
            "epoch : 1495 training loss : 864156.5 testing loss : 618.4815964129119\n",
            "epoch : 1496 training loss : 865006.5 testing loss : 612.8282964229584\n",
            "epoch : 1497 training loss : 866171.4375 testing loss : 614.0073215834863\n",
            "epoch : 1498 training loss : 870420.5 testing loss : 636.9557354323632\n",
            "epoch : 1499 training loss : 869775.5 testing loss : 608.3628045120071\n",
            "epoch : 1500 training loss : 872360.625 testing loss : 651.8489992238779\n",
            "epoch : 1501 training loss : 880176.4375 testing loss : 610.0616005057782\n",
            "epoch : 1502 training loss : 866248.4375 testing loss : 604.4570162591681\n",
            "epoch : 1503 training loss : 869879.8125 testing loss : 647.5135264080183\n",
            "epoch : 1504 training loss : 865027.5 testing loss : 587.4299891965579\n",
            "epoch : 1505 training loss : 871552.4375 testing loss : 632.3068256863451\n",
            "epoch : 1506 training loss : 865332.5 testing loss : 607.5471573867629\n",
            "epoch : 1507 training loss : 862516.5625 testing loss : 621.5975776246164\n",
            "epoch : 1508 training loss : 866096.0 testing loss : 605.4801082083609\n",
            "epoch : 1509 training loss : 867847.8125 testing loss : 604.0120635560129\n",
            "epoch : 1510 training loss : 868364.625 testing loss : 627.0155572785741\n",
            "epoch : 1511 training loss : 877098.0 testing loss : 635.2955896812202\n",
            "epoch : 1512 training loss : 880107.5625 testing loss : 609.2032279609579\n",
            "epoch : 1513 training loss : 872276.5625 testing loss : 627.7580205845621\n",
            "epoch : 1514 training loss : 867823.9375 testing loss : 615.9061215810016\n",
            "epoch : 1515 training loss : 873816.75 testing loss : 630.1353036302381\n",
            "epoch : 1516 training loss : 876709.25 testing loss : 589.8284688607781\n",
            "epoch : 1517 training loss : 893431.75 testing loss : 673.650434171204\n",
            "epoch : 1518 training loss : 885743.875 testing loss : 616.8277460938007\n",
            "epoch : 1519 training loss : 875753.1875 testing loss : 611.078467172859\n",
            "epoch : 1520 training loss : 872790.3125 testing loss : 644.4031603969304\n",
            "epoch : 1521 training loss : 869321.875 testing loss : 602.9134334104251\n",
            "epoch : 1522 training loss : 872320.1875 testing loss : 636.1442130751315\n",
            "epoch : 1523 training loss : 871667.125 testing loss : 627.7481786993752\n",
            "epoch : 1524 training loss : 871081.375 testing loss : 606.2289265636849\n",
            "epoch : 1525 training loss : 866449.3125 testing loss : 632.0376931566053\n",
            "epoch : 1526 training loss : 865017.9375 testing loss : 606.1855559538951\n",
            "epoch : 1527 training loss : 868331.25 testing loss : 614.5690618510795\n",
            "epoch : 1528 training loss : 865980.625 testing loss : 629.5880234283684\n",
            "epoch : 1529 training loss : 863064.25 testing loss : 593.4718113768417\n",
            "epoch : 1530 training loss : 867880.25 testing loss : 625.1362706728742\n",
            "epoch : 1531 training loss : 866733.9375 testing loss : 601.7462822547001\n",
            "epoch : 1532 training loss : 868153.5 testing loss : 625.3979035415481\n",
            "epoch : 1533 training loss : 872655.75 testing loss : 607.4505993644748\n",
            "epoch : 1534 training loss : 869368.25 testing loss : 607.1441348907167\n",
            "epoch : 1535 training loss : 863244.125 testing loss : 627.5663015293864\n",
            "epoch : 1536 training loss : 866596.0 testing loss : 614.5279065001328\n",
            "epoch : 1537 training loss : 867671.375 testing loss : 594.2157414508077\n",
            "epoch : 1538 training loss : 864058.125 testing loss : 612.1216134244362\n",
            "epoch : 1539 training loss : 864065.5625 testing loss : 627.0016393724796\n",
            "epoch : 1540 training loss : 881530.0 testing loss : 655.6901339957144\n",
            "epoch : 1541 training loss : 886091.0625 testing loss : 588.8767914708736\n",
            "epoch : 1542 training loss : 900039.9375 testing loss : 663.111300339741\n",
            "epoch : 1543 training loss : 887973.125 testing loss : 612.5530162891455\n",
            "epoch : 1544 training loss : 885299.625 testing loss : 592.6184496225509\n",
            "epoch : 1545 training loss : 888060.75 testing loss : 663.4871943292364\n",
            "epoch : 1546 training loss : 880143.375 testing loss : 605.9617435130398\n",
            "epoch : 1547 training loss : 881549.875 testing loss : 676.1635542316774\n",
            "epoch : 1548 training loss : 887476.875 testing loss : 590.6518554497609\n",
            "epoch : 1549 training loss : 879406.875 testing loss : 661.6557636746263\n",
            "epoch : 1550 training loss : 872134.5 testing loss : 597.9404194566001\n",
            "epoch : 1551 training loss : 869485.375 testing loss : 612.5288197994232\n",
            "epoch : 1552 training loss : 867214.9375 testing loss : 623.7518934380691\n",
            "epoch : 1553 training loss : 870201.0625 testing loss : 621.6969193631569\n",
            "epoch : 1554 training loss : 866434.875 testing loss : 614.3419881816459\n",
            "epoch : 1555 training loss : 871963.375 testing loss : 626.5431617108067\n",
            "epoch : 1556 training loss : 874454.625 testing loss : 611.2990251030542\n",
            "epoch : 1557 training loss : 872098.6875 testing loss : 600.1981755463423\n",
            "epoch : 1558 training loss : 866974.1875 testing loss : 642.4880509819604\n",
            "epoch : 1559 training loss : 868754.1875 testing loss : 611.829300633574\n",
            "epoch : 1560 training loss : 866678.1875 testing loss : 611.1579984458147\n",
            "epoch : 1561 training loss : 865787.8125 testing loss : 617.531751411151\n",
            "epoch : 1562 training loss : 860527.25 testing loss : 608.314586772328\n",
            "epoch : 1563 training loss : 864976.625 testing loss : 627.5309327703661\n",
            "epoch : 1564 training loss : 866974.6875 testing loss : 620.1226640528282\n",
            "epoch : 1565 training loss : 862888.5 testing loss : 617.1706069655123\n",
            "epoch : 1566 training loss : 860414.0625 testing loss : 597.8986546655672\n",
            "epoch : 1567 training loss : 860147.5 testing loss : 625.9478106182233\n",
            "epoch : 1568 training loss : 858734.6875 testing loss : 603.9810054386611\n",
            "epoch : 1569 training loss : 860498.4375 testing loss : 621.1008933392246\n",
            "epoch : 1570 training loss : 862633.9375 testing loss : 601.7500481035858\n",
            "epoch : 1571 training loss : 868556.5 testing loss : 614.2929626713811\n",
            "epoch : 1572 training loss : 862330.9375 testing loss : 597.442115646548\n",
            "epoch : 1573 training loss : 861936.75 testing loss : 628.835953803189\n",
            "epoch : 1574 training loss : 868087.5625 testing loss : 611.977129503689\n",
            "epoch : 1575 training loss : 863715.625 testing loss : 589.8836427515587\n",
            "epoch : 1576 training loss : 876645.125 testing loss : 646.510801870211\n",
            "epoch : 1577 training loss : 876809.625 testing loss : 592.8549755041578\n",
            "epoch : 1578 training loss : 886400.75 testing loss : 599.2612655415999\n",
            "epoch : 1579 training loss : 885819.9375 testing loss : 651.3154140890172\n",
            "epoch : 1580 training loss : 889461.75 testing loss : 606.1336057354919\n",
            "epoch : 1581 training loss : 892609.125 testing loss : 644.788246220192\n",
            "epoch : 1582 training loss : 884495.125 testing loss : 603.1743318654794\n",
            "epoch : 1583 training loss : 873926.75 testing loss : 634.0715155285017\n",
            "epoch : 1584 training loss : 874721.5 testing loss : 595.0024153426685\n",
            "epoch : 1585 training loss : 869019.125 testing loss : 627.9655081128652\n",
            "epoch : 1586 training loss : 870695.875 testing loss : 603.0943718273028\n",
            "epoch : 1587 training loss : 864570.125 testing loss : 626.7856123574012\n",
            "epoch : 1588 training loss : 864415.8125 testing loss : 591.2231439214892\n",
            "epoch : 1589 training loss : 869455.1875 testing loss : 637.0567732595764\n",
            "epoch : 1590 training loss : 869820.0 testing loss : 606.8455991470709\n",
            "epoch : 1591 training loss : 864929.9375 testing loss : 596.1975424753881\n",
            "epoch : 1592 training loss : 865400.75 testing loss : 638.3886195473966\n",
            "epoch : 1593 training loss : 866752.75 testing loss : 592.4744265374884\n",
            "epoch : 1594 training loss : 865207.125 testing loss : 612.2007073913\n",
            "epoch : 1595 training loss : 868383.8125 testing loss : 644.762504676802\n",
            "epoch : 1596 training loss : 862717.9375 testing loss : 585.607364551156\n",
            "epoch : 1597 training loss : 871292.0 testing loss : 669.1272457067945\n",
            "epoch : 1598 training loss : 883870.75 testing loss : 594.2564951445149\n",
            "epoch : 1599 training loss : 872741.4375 testing loss : 619.6292182428647\n",
            "epoch : 1600 training loss : 863802.1875 testing loss : 629.609536869336\n",
            "epoch : 1601 training loss : 863952.125 testing loss : 601.0868542742941\n",
            "epoch : 1602 training loss : 869761.5 testing loss : 616.0485847924663\n",
            "epoch : 1603 training loss : 874545.6875 testing loss : 633.1902848281692\n",
            "epoch : 1604 training loss : 866788.125 testing loss : 595.7265922727838\n",
            "epoch : 1605 training loss : 859508.125 testing loss : 611.6927271121372\n",
            "epoch : 1606 training loss : 862930.9375 testing loss : 641.6502163958761\n",
            "epoch : 1607 training loss : 867128.0 testing loss : 587.8835001899077\n",
            "epoch : 1608 training loss : 872058.9375 testing loss : 603.3913759193589\n",
            "epoch : 1609 training loss : 871193.8125 testing loss : 647.7762285059532\n",
            "epoch : 1610 training loss : 875693.625 testing loss : 590.0046344837256\n",
            "epoch : 1611 training loss : 877277.5625 testing loss : 621.4792938886491\n",
            "epoch : 1612 training loss : 869673.5625 testing loss : 627.912306420571\n",
            "epoch : 1613 training loss : 861702.4375 testing loss : 592.9242261342242\n",
            "epoch : 1614 training loss : 859682.0 testing loss : 647.3467230986705\n",
            "epoch : 1615 training loss : 872139.125 testing loss : 591.4833564737202\n",
            "epoch : 1616 training loss : 863678.5 testing loss : 607.8076363263932\n",
            "epoch : 1617 training loss : 856768.4375 testing loss : 609.39480597361\n",
            "epoch : 1618 training loss : 857089.0625 testing loss : 598.0527516571822\n",
            "epoch : 1619 training loss : 856046.5 testing loss : 640.6972657579237\n",
            "epoch : 1620 training loss : 861128.75 testing loss : 590.9764130832874\n",
            "epoch : 1621 training loss : 862377.0 testing loss : 630.8977187144018\n",
            "epoch : 1622 training loss : 859495.9375 testing loss : 602.7582895375986\n",
            "epoch : 1623 training loss : 858636.5625 testing loss : 608.7408204859337\n",
            "epoch : 1624 training loss : 860449.375 testing loss : 629.6464659408131\n",
            "epoch : 1625 training loss : 856334.6875 testing loss : 593.9033453591102\n",
            "epoch : 1626 training loss : 856602.9375 testing loss : 622.5472086657466\n",
            "epoch : 1627 training loss : 855557.125 testing loss : 593.7234606763958\n",
            "epoch : 1628 training loss : 861173.875 testing loss : 622.4627313845981\n",
            "epoch : 1629 training loss : 855702.25 testing loss : 600.8330827202417\n",
            "epoch : 1630 training loss : 859666.625 testing loss : 645.4303096311282\n",
            "epoch : 1631 training loss : 866062.4375 testing loss : 585.5016370431512\n",
            "epoch : 1632 training loss : 862096.125 testing loss : 618.4517351315085\n",
            "epoch : 1633 training loss : 872458.125 testing loss : 638.153601821545\n",
            "epoch : 1634 training loss : 867195.0 testing loss : 608.3433109701207\n",
            "epoch : 1635 training loss : 859055.5 testing loss : 613.0778447357955\n",
            "epoch : 1636 training loss : 857752.8125 testing loss : 606.3184819158199\n",
            "epoch : 1637 training loss : 860942.0 testing loss : 624.7856374061213\n",
            "epoch : 1638 training loss : 861844.0625 testing loss : 618.7129613467023\n",
            "epoch : 1639 training loss : 859569.875 testing loss : 627.214627019072\n",
            "epoch : 1640 training loss : 855944.0 testing loss : 590.8800175400962\n",
            "epoch : 1641 training loss : 855697.1875 testing loss : 628.7018083871993\n",
            "epoch : 1642 training loss : 855561.625 testing loss : 594.9235100851649\n",
            "epoch : 1643 training loss : 853994.125 testing loss : 618.7291008919741\n",
            "epoch : 1644 training loss : 856907.625 testing loss : 592.0750317847834\n",
            "epoch : 1645 training loss : 858133.25 testing loss : 608.9509138913281\n",
            "epoch : 1646 training loss : 857600.625 testing loss : 633.0487637118956\n",
            "epoch : 1647 training loss : 859892.0 testing loss : 589.4301515406212\n",
            "epoch : 1648 training loss : 858806.125 testing loss : 597.4755592662676\n",
            "epoch : 1649 training loss : 853196.1875 testing loss : 618.7288383192721\n",
            "epoch : 1650 training loss : 855956.75 testing loss : 589.9373210784609\n",
            "epoch : 1651 training loss : 857047.625 testing loss : 629.198151105273\n",
            "epoch : 1652 training loss : 861854.4375 testing loss : 628.1136876650617\n",
            "epoch : 1653 training loss : 857275.875 testing loss : 581.8665366700266\n",
            "epoch : 1654 training loss : 858614.1875 testing loss : 608.9841035467333\n",
            "epoch : 1655 training loss : 856086.125 testing loss : 590.6622625009148\n",
            "epoch : 1656 training loss : 857471.875 testing loss : 604.0332120351031\n",
            "epoch : 1657 training loss : 851338.0625 testing loss : 604.6558206587766\n",
            "epoch : 1658 training loss : 857379.375 testing loss : 611.1949603641982\n",
            "epoch : 1659 training loss : 860867.875 testing loss : 611.2416104996099\n",
            "epoch : 1660 training loss : 867573.0 testing loss : 634.2268080226088\n",
            "epoch : 1661 training loss : 859574.1875 testing loss : 606.2096870198714\n",
            "epoch : 1662 training loss : 857832.875 testing loss : 608.8525994878954\n",
            "epoch : 1663 training loss : 851359.125 testing loss : 595.6840253488152\n",
            "epoch : 1664 training loss : 854416.5 testing loss : 591.7054299865149\n",
            "epoch : 1665 training loss : 852388.9375 testing loss : 621.2081957433076\n",
            "epoch : 1666 training loss : 854274.6875 testing loss : 590.6305111889291\n",
            "epoch : 1667 training loss : 852861.875 testing loss : 597.0578077514615\n",
            "epoch : 1668 training loss : 851543.6875 testing loss : 604.3800346218379\n",
            "epoch : 1669 training loss : 854011.5 testing loss : 638.0929577561607\n",
            "epoch : 1670 training loss : 858242.9375 testing loss : 622.5898032715891\n",
            "epoch : 1671 training loss : 855591.0625 testing loss : 587.5121328282145\n",
            "epoch : 1672 training loss : 855476.8125 testing loss : 619.088395209439\n",
            "epoch : 1673 training loss : 855807.4375 testing loss : 629.3113209526095\n",
            "epoch : 1674 training loss : 858469.8125 testing loss : 590.5802720310413\n",
            "epoch : 1675 training loss : 865859.75 testing loss : 612.386630300927\n",
            "epoch : 1676 training loss : 856218.375 testing loss : 604.6895186479113\n",
            "epoch : 1677 training loss : 861229.1875 testing loss : 630.62337608464\n",
            "epoch : 1678 training loss : 855818.75 testing loss : 602.1509019273573\n",
            "epoch : 1679 training loss : 858839.3125 testing loss : 609.6207290223215\n",
            "epoch : 1680 training loss : 856406.0 testing loss : 656.5096928398166\n",
            "epoch : 1681 training loss : 863195.3125 testing loss : 572.791379318828\n",
            "epoch : 1682 training loss : 869151.625 testing loss : 640.4782276976425\n",
            "epoch : 1683 training loss : 876325.5625 testing loss : 628.6652406316942\n",
            "epoch : 1684 training loss : 880408.625 testing loss : 628.9395321512644\n",
            "epoch : 1685 training loss : 877690.6875 testing loss : 632.8058238050579\n",
            "epoch : 1686 training loss : 871516.375 testing loss : 607.0421658216325\n",
            "epoch : 1687 training loss : 868544.5625 testing loss : 619.6611378741476\n",
            "epoch : 1688 training loss : 860309.1875 testing loss : 619.434390175659\n",
            "epoch : 1689 training loss : 862015.6875 testing loss : 593.3205468148257\n",
            "epoch : 1690 training loss : 862310.0 testing loss : 609.7988148220873\n",
            "epoch : 1691 training loss : 859774.4375 testing loss : 621.1742788985767\n",
            "epoch : 1692 training loss : 858517.0 testing loss : 634.2338208202767\n",
            "epoch : 1693 training loss : 859948.375 testing loss : 579.9357998919699\n",
            "epoch : 1694 training loss : 864401.0 testing loss : 617.5130814387735\n",
            "epoch : 1695 training loss : 856276.9375 testing loss : 606.9936486619764\n",
            "epoch : 1696 training loss : 856024.0 testing loss : 602.7476288765932\n",
            "epoch : 1697 training loss : 859897.0 testing loss : 606.1349191897739\n",
            "epoch : 1698 training loss : 855444.375 testing loss : 602.3148163630899\n",
            "epoch : 1699 training loss : 853128.6875 testing loss : 608.3290024795364\n",
            "epoch : 1700 training loss : 854868.5 testing loss : 613.1633963774791\n",
            "epoch : 1701 training loss : 854553.3125 testing loss : 612.717951443343\n",
            "epoch : 1702 training loss : 853289.5625 testing loss : 590.6198907003993\n",
            "epoch : 1703 training loss : 866802.8125 testing loss : 590.9477316236074\n",
            "epoch : 1704 training loss : 868772.4375 testing loss : 608.2884804358524\n",
            "epoch : 1705 training loss : 861232.3125 testing loss : 643.6386033096145\n",
            "epoch : 1706 training loss : 871394.5 testing loss : 566.7264197674473\n",
            "epoch : 1707 training loss : 863500.625 testing loss : 659.2152641747905\n",
            "epoch : 1708 training loss : 885820.25 testing loss : 606.0545174906739\n",
            "epoch : 1709 training loss : 872350.5 testing loss : 606.485597186384\n",
            "epoch : 1710 training loss : 877385.1875 testing loss : 651.3655346874642\n",
            "epoch : 1711 training loss : 866743.6875 testing loss : 592.0369391293652\n",
            "epoch : 1712 training loss : 857914.8125 testing loss : 619.7439718984924\n",
            "epoch : 1713 training loss : 862409.0 testing loss : 618.4364832966728\n",
            "epoch : 1714 training loss : 857137.125 testing loss : 589.5049660923206\n",
            "epoch : 1715 training loss : 864151.4375 testing loss : 627.9516394602514\n",
            "epoch : 1716 training loss : 859848.875 testing loss : 596.1183176272739\n",
            "epoch : 1717 training loss : 864389.875 testing loss : 619.7171563996678\n",
            "epoch : 1718 training loss : 860597.4375 testing loss : 620.2516541248929\n",
            "epoch : 1719 training loss : 853056.8125 testing loss : 588.1036937321181\n",
            "epoch : 1720 training loss : 854715.1875 testing loss : 658.0083252202093\n",
            "epoch : 1721 training loss : 858587.75 testing loss : 568.5584263738277\n",
            "epoch : 1722 training loss : 870715.3125 testing loss : 618.1910798043276\n",
            "epoch : 1723 training loss : 857023.3125 testing loss : 610.0039104339296\n",
            "epoch : 1724 training loss : 858003.125 testing loss : 582.9521247032469\n",
            "epoch : 1725 training loss : 864431.625 testing loss : 662.7079980542175\n",
            "epoch : 1726 training loss : 871998.5 testing loss : 582.4396894990871\n",
            "epoch : 1727 training loss : 865356.625 testing loss : 626.7581225348785\n",
            "epoch : 1728 training loss : 858170.625 testing loss : 597.0969377682272\n",
            "epoch : 1729 training loss : 858931.0625 testing loss : 595.4992959330567\n",
            "epoch : 1730 training loss : 854624.6875 testing loss : 613.4227806340276\n",
            "epoch : 1731 training loss : 859919.6875 testing loss : 621.3931393560055\n",
            "epoch : 1732 training loss : 861926.5 testing loss : 576.4079007490546\n",
            "epoch : 1733 training loss : 861425.75 testing loss : 603.1157387231303\n",
            "epoch : 1734 training loss : 851471.875 testing loss : 595.5069828518724\n",
            "epoch : 1735 training loss : 848989.875 testing loss : 603.520233422254\n",
            "epoch : 1736 training loss : 855415.0625 testing loss : 636.8094943540286\n",
            "epoch : 1737 training loss : 864625.625 testing loss : 590.9135994003938\n",
            "epoch : 1738 training loss : 859052.0 testing loss : 600.9529171812851\n",
            "epoch : 1739 training loss : 857851.625 testing loss : 620.0526150370066\n",
            "epoch : 1740 training loss : 849034.5625 testing loss : 607.7660137594273\n",
            "epoch : 1741 training loss : 848056.25 testing loss : 596.3382852246276\n",
            "epoch : 1742 training loss : 848181.5625 testing loss : 603.8384269085606\n",
            "epoch : 1743 training loss : 847243.3125 testing loss : 588.0916993786803\n",
            "epoch : 1744 training loss : 854999.375 testing loss : 615.054665892525\n",
            "epoch : 1745 training loss : 850886.1875 testing loss : 613.5201499715315\n",
            "epoch : 1746 training loss : 847214.9375 testing loss : 596.9091611182795\n",
            "epoch : 1747 training loss : 848718.125 testing loss : 623.567723567507\n",
            "epoch : 1748 training loss : 854540.9375 testing loss : 593.1611788905827\n",
            "epoch : 1749 training loss : 854277.9375 testing loss : 599.8375718762389\n",
            "epoch : 1750 training loss : 852556.4375 testing loss : 626.0513596977808\n",
            "epoch : 1751 training loss : 849488.9375 testing loss : 586.6644781648586\n",
            "epoch : 1752 training loss : 852287.875 testing loss : 605.1780844473205\n",
            "epoch : 1753 training loss : 854735.5625 testing loss : 586.3746388507101\n",
            "epoch : 1754 training loss : 849090.5 testing loss : 631.1082996583618\n",
            "epoch : 1755 training loss : 852780.4375 testing loss : 582.3779105106287\n",
            "epoch : 1756 training loss : 851449.4375 testing loss : 623.2107147938382\n",
            "epoch : 1757 training loss : 854859.625 testing loss : 607.854268941204\n",
            "epoch : 1758 training loss : 858309.6875 testing loss : 579.7875575154228\n",
            "epoch : 1759 training loss : 884099.875 testing loss : 702.6206294350919\n",
            "epoch : 1760 training loss : 878977.5 testing loss : 579.7142736025617\n",
            "epoch : 1761 training loss : 863831.1875 testing loss : 644.5687497210714\n",
            "epoch : 1762 training loss : 857528.875 testing loss : 586.4450753452503\n",
            "epoch : 1763 training loss : 856426.625 testing loss : 627.9866220233715\n",
            "epoch : 1764 training loss : 866172.625 testing loss : 606.5058854343616\n",
            "epoch : 1765 training loss : 854962.0625 testing loss : 588.7737007795182\n",
            "epoch : 1766 training loss : 847168.0625 testing loss : 604.0423439477397\n",
            "epoch : 1767 training loss : 850253.375 testing loss : 634.5899806718911\n",
            "epoch : 1768 training loss : 856926.1875 testing loss : 592.4235513716673\n",
            "epoch : 1769 training loss : 854460.875 testing loss : 596.2405442073282\n",
            "epoch : 1770 training loss : 846899.5 testing loss : 608.7421052624694\n",
            "epoch : 1771 training loss : 846820.5 testing loss : 618.4758861824474\n",
            "epoch : 1772 training loss : 850405.875 testing loss : 595.6294075261175\n",
            "epoch : 1773 training loss : 856332.625 testing loss : 587.8153276337987\n",
            "epoch : 1774 training loss : 853020.75 testing loss : 616.6730596133038\n",
            "epoch : 1775 training loss : 849139.3125 testing loss : 608.249332459627\n",
            "epoch : 1776 training loss : 852824.875 testing loss : 600.7442706264226\n",
            "epoch : 1777 training loss : 856053.8125 testing loss : 626.0561780992863\n",
            "epoch : 1778 training loss : 867845.9375 testing loss : 620.7433660030365\n",
            "epoch : 1779 training loss : 860888.125 testing loss : 602.7347466101688\n",
            "epoch : 1780 training loss : 851839.1875 testing loss : 601.0042758009075\n",
            "epoch : 1781 training loss : 848353.5 testing loss : 615.7060199817724\n",
            "epoch : 1782 training loss : 847715.5625 testing loss : 602.610556811358\n",
            "epoch : 1783 training loss : 848940.6875 testing loss : 608.1458982260881\n",
            "epoch : 1784 training loss : 846654.375 testing loss : 594.6622343801819\n",
            "epoch : 1785 training loss : 846153.625 testing loss : 618.2078206876738\n",
            "epoch : 1786 training loss : 845088.25 testing loss : 590.2762044699846\n",
            "epoch : 1787 training loss : 862426.0 testing loss : 614.7888045838449\n",
            "epoch : 1788 training loss : 862967.0625 testing loss : 621.6729020291725\n",
            "epoch : 1789 training loss : 850745.75 testing loss : 596.742749304898\n",
            "epoch : 1790 training loss : 852829.5 testing loss : 626.8533955443222\n",
            "epoch : 1791 training loss : 851963.4375 testing loss : 605.0345570534731\n",
            "epoch : 1792 training loss : 851440.75 testing loss : 616.5508107189584\n",
            "epoch : 1793 training loss : 847976.5 testing loss : 602.5744869582421\n",
            "epoch : 1794 training loss : 845885.625 testing loss : 613.3174189318598\n",
            "epoch : 1795 training loss : 851930.875 testing loss : 624.5612228507489\n",
            "epoch : 1796 training loss : 852560.375 testing loss : 631.2243994970237\n",
            "epoch : 1797 training loss : 851973.0625 testing loss : 578.9192245766125\n",
            "epoch : 1798 training loss : 864225.125 testing loss : 615.322353428444\n",
            "epoch : 1799 training loss : 848802.0 testing loss : 582.7323064192207\n",
            "epoch : 1800 training loss : 855605.8125 testing loss : 621.2477097279202\n",
            "epoch : 1801 training loss : 849013.125 testing loss : 596.4767980976442\n",
            "epoch : 1802 training loss : 847004.5 testing loss : 624.0306151377416\n",
            "epoch : 1803 training loss : 848012.3125 testing loss : 604.6419269236843\n",
            "epoch : 1804 training loss : 851359.9375 testing loss : 591.9464959992772\n",
            "epoch : 1805 training loss : 854037.9375 testing loss : 613.640626314467\n",
            "epoch : 1806 training loss : 866500.375 testing loss : 626.2343379649441\n",
            "epoch : 1807 training loss : 859761.1875 testing loss : 615.6115599754637\n",
            "epoch : 1808 training loss : 860766.8125 testing loss : 619.6816655028183\n",
            "epoch : 1809 training loss : 866296.0 testing loss : 618.3296350778731\n",
            "epoch : 1810 training loss : 864607.125 testing loss : 598.2090923997154\n",
            "epoch : 1811 training loss : 854098.5625 testing loss : 607.0027736879028\n",
            "epoch : 1812 training loss : 854676.8125 testing loss : 611.1532961056296\n",
            "epoch : 1813 training loss : 851156.5 testing loss : 581.0182809049049\n",
            "epoch : 1814 training loss : 844031.8125 testing loss : 613.1300448168696\n",
            "epoch : 1815 training loss : 844777.4375 testing loss : 601.2291190877424\n",
            "epoch : 1816 training loss : 844232.1875 testing loss : 600.2583706927511\n",
            "epoch : 1817 training loss : 846218.75 testing loss : 596.0127401415226\n",
            "epoch : 1818 training loss : 850979.6875 testing loss : 577.8420385829115\n",
            "epoch : 1819 training loss : 849506.25 testing loss : 670.2892002658507\n",
            "epoch : 1820 training loss : 865962.3125 testing loss : 576.9437645410014\n",
            "epoch : 1821 training loss : 858698.625 testing loss : 605.4011950851542\n",
            "epoch : 1822 training loss : 851847.5625 testing loss : 620.292062267793\n",
            "epoch : 1823 training loss : 845352.25 testing loss : 600.0793921187916\n",
            "epoch : 1824 training loss : 850902.1875 testing loss : 592.8948968174183\n",
            "epoch : 1825 training loss : 845557.5625 testing loss : 615.748954298222\n",
            "epoch : 1826 training loss : 847940.0625 testing loss : 576.0133188952387\n",
            "epoch : 1827 training loss : 847652.1875 testing loss : 638.8355701391675\n",
            "epoch : 1828 training loss : 847604.5 testing loss : 587.7773803419772\n",
            "epoch : 1829 training loss : 841031.75 testing loss : 641.7509007053038\n",
            "epoch : 1830 training loss : 856955.0 testing loss : 598.483796412966\n",
            "epoch : 1831 training loss : 847758.375 testing loss : 580.3094991814773\n",
            "epoch : 1832 training loss : 853597.6875 testing loss : 627.4957430320503\n",
            "epoch : 1833 training loss : 850316.625 testing loss : 587.873646590562\n",
            "epoch : 1834 training loss : 848342.6875 testing loss : 609.7875575829396\n",
            "epoch : 1835 training loss : 843757.125 testing loss : 598.6762990677252\n",
            "epoch : 1836 training loss : 839904.8125 testing loss : 594.5295566436464\n",
            "epoch : 1837 training loss : 844220.4375 testing loss : 614.2178863909393\n",
            "epoch : 1838 training loss : 842042.4375 testing loss : 592.2512825463725\n",
            "epoch : 1839 training loss : 842366.0 testing loss : 593.8473358470782\n",
            "epoch : 1840 training loss : 842030.6875 testing loss : 608.366340094963\n",
            "epoch : 1841 training loss : 840358.8125 testing loss : 597.0797032698066\n",
            "epoch : 1842 training loss : 840623.625 testing loss : 582.8681052870455\n",
            "epoch : 1843 training loss : 847732.6875 testing loss : 591.4060167949812\n",
            "epoch : 1844 training loss : 855905.0 testing loss : 624.9726507452737\n",
            "epoch : 1845 training loss : 847060.0 testing loss : 579.3690124423103\n",
            "epoch : 1846 training loss : 855445.625 testing loss : 574.6764193150849\n",
            "epoch : 1847 training loss : 861365.4375 testing loss : 649.6476976154125\n",
            "epoch : 1848 training loss : 852020.1875 testing loss : 591.0774095902401\n",
            "epoch : 1849 training loss : 842678.75 testing loss : 579.1442623286121\n",
            "epoch : 1850 training loss : 843608.8125 testing loss : 632.0724500111774\n",
            "epoch : 1851 training loss : 847616.125 testing loss : 612.9905303663912\n",
            "epoch : 1852 training loss : 843309.5 testing loss : 581.9427554691787\n",
            "epoch : 1853 training loss : 845910.5 testing loss : 604.3090388859267\n",
            "epoch : 1854 training loss : 841531.8125 testing loss : 606.2749081143236\n",
            "epoch : 1855 training loss : 840570.0 testing loss : 603.3541347516322\n",
            "epoch : 1856 training loss : 839393.5625 testing loss : 608.320134388662\n",
            "epoch : 1857 training loss : 840578.8125 testing loss : 595.9619968697033\n",
            "epoch : 1858 training loss : 837071.1875 testing loss : 606.0000225147314\n",
            "epoch : 1859 training loss : 838592.125 testing loss : 592.6415571005998\n",
            "epoch : 1860 training loss : 845097.1875 testing loss : 586.256985552543\n",
            "epoch : 1861 training loss : 837202.0 testing loss : 597.2026592334814\n",
            "epoch : 1862 training loss : 839158.0 testing loss : 577.9596569411523\n",
            "epoch : 1863 training loss : 843339.5 testing loss : 605.4965297711634\n",
            "epoch : 1864 training loss : 840936.4375 testing loss : 605.0049078253518\n",
            "epoch : 1865 training loss : 842567.75 testing loss : 635.3544649086167\n",
            "epoch : 1866 training loss : 853185.875 testing loss : 593.1542399224982\n",
            "epoch : 1867 training loss : 843206.375 testing loss : 594.8127784707905\n",
            "epoch : 1868 training loss : 843938.5625 testing loss : 600.0351095600465\n",
            "epoch : 1869 training loss : 843203.125 testing loss : 628.3125561043224\n",
            "epoch : 1870 training loss : 849706.6875 testing loss : 614.3275651488684\n",
            "epoch : 1871 training loss : 848306.3125 testing loss : 604.9630196474295\n",
            "epoch : 1872 training loss : 853160.375 testing loss : 584.1668087085791\n",
            "epoch : 1873 training loss : 849848.9375 testing loss : 624.7208717460126\n",
            "epoch : 1874 training loss : 842194.0 testing loss : 583.2602065073705\n",
            "epoch : 1875 training loss : 845940.5 testing loss : 573.4369015630367\n",
            "epoch : 1876 training loss : 847448.0 testing loss : 623.9256968181745\n",
            "epoch : 1877 training loss : 848119.0 testing loss : 623.6110069793938\n",
            "epoch : 1878 training loss : 843647.1875 testing loss : 589.7237700719749\n",
            "epoch : 1879 training loss : 848062.375 testing loss : 594.7797517333411\n",
            "epoch : 1880 training loss : 846952.875 testing loss : 593.206708180166\n",
            "epoch : 1881 training loss : 852284.0 testing loss : 615.3301063111398\n",
            "epoch : 1882 training loss : 848266.0 testing loss : 629.7420547261702\n",
            "epoch : 1883 training loss : 846866.5 testing loss : 588.5413618488649\n",
            "epoch : 1884 training loss : 857647.375 testing loss : 609.8309958107703\n",
            "epoch : 1885 training loss : 865896.75 testing loss : 662.397020692319\n",
            "epoch : 1886 training loss : 858021.5 testing loss : 580.8491454314342\n",
            "epoch : 1887 training loss : 847334.125 testing loss : 613.626614458793\n",
            "epoch : 1888 training loss : 848364.8125 testing loss : 619.680730969505\n",
            "epoch : 1889 training loss : 850581.75 testing loss : 605.1614823784448\n",
            "epoch : 1890 training loss : 850453.8125 testing loss : 582.0934995106891\n",
            "epoch : 1891 training loss : 860078.0 testing loss : 618.0101894931456\n",
            "epoch : 1892 training loss : 848858.5 testing loss : 605.7911511965558\n",
            "epoch : 1893 training loss : 848698.125 testing loss : 586.6984400939098\n",
            "epoch : 1894 training loss : 855848.875 testing loss : 643.970690328463\n",
            "epoch : 1895 training loss : 857142.6875 testing loss : 593.4253364136789\n",
            "epoch : 1896 training loss : 852780.4375 testing loss : 574.1226547355145\n",
            "epoch : 1897 training loss : 858269.3125 testing loss : 655.9843379311857\n",
            "epoch : 1898 training loss : 873838.9375 testing loss : 584.0525908997629\n",
            "epoch : 1899 training loss : 852961.75 testing loss : 586.4680864431161\n",
            "epoch : 1900 training loss : 853518.25 testing loss : 638.7986242560158\n",
            "epoch : 1901 training loss : 853141.375 testing loss : 595.063245281709\n",
            "epoch : 1902 training loss : 858081.0 testing loss : 631.1220648752904\n",
            "epoch : 1903 training loss : 853100.3125 testing loss : 601.4313739654237\n",
            "epoch : 1904 training loss : 854466.4375 testing loss : 582.8381812825667\n",
            "epoch : 1905 training loss : 855576.375 testing loss : 641.2106277647272\n",
            "epoch : 1906 training loss : 851288.1875 testing loss : 601.2061961667728\n",
            "epoch : 1907 training loss : 837202.3125 testing loss : 596.0523767703403\n",
            "epoch : 1908 training loss : 853810.125 testing loss : 637.9952824664327\n",
            "epoch : 1909 training loss : 848207.625 testing loss : 578.2181714804826\n",
            "epoch : 1910 training loss : 843779.375 testing loss : 603.8859221175709\n",
            "epoch : 1911 training loss : 842809.5 testing loss : 607.558049562758\n",
            "epoch : 1912 training loss : 845718.875 testing loss : 614.1321151172166\n",
            "epoch : 1913 training loss : 840978.0625 testing loss : 591.9179982864752\n",
            "epoch : 1914 training loss : 847065.375 testing loss : 585.193231158552\n",
            "epoch : 1915 training loss : 847048.125 testing loss : 612.9732569783134\n",
            "epoch : 1916 training loss : 841692.75 testing loss : 590.8826019130977\n",
            "epoch : 1917 training loss : 846629.1875 testing loss : 624.57085860725\n",
            "epoch : 1918 training loss : 850900.25 testing loss : 603.2958262937259\n",
            "epoch : 1919 training loss : 851026.75 testing loss : 572.6295592025318\n",
            "epoch : 1920 training loss : 839440.0625 testing loss : 630.6847429507602\n",
            "epoch : 1921 training loss : 840112.0 testing loss : 583.8027458169819\n",
            "epoch : 1922 training loss : 852168.8125 testing loss : 654.6554069835528\n",
            "epoch : 1923 training loss : 862503.625 testing loss : 596.6395525278243\n",
            "epoch : 1924 training loss : 847085.625 testing loss : 631.6074110364492\n",
            "epoch : 1925 training loss : 842864.9375 testing loss : 579.1296635159349\n",
            "epoch : 1926 training loss : 848575.8125 testing loss : 583.223170793162\n",
            "epoch : 1927 training loss : 843824.375 testing loss : 608.1044128869487\n",
            "epoch : 1928 training loss : 837051.8125 testing loss : 596.3805351236225\n",
            "epoch : 1929 training loss : 834949.25 testing loss : 608.9040201216673\n",
            "epoch : 1930 training loss : 840529.5 testing loss : 582.4620746490175\n",
            "epoch : 1931 training loss : 836284.375 testing loss : 612.1319383583237\n",
            "epoch : 1932 training loss : 836593.25 testing loss : 606.2497427252541\n",
            "epoch : 1933 training loss : 839914.375 testing loss : 597.8114748233188\n",
            "epoch : 1934 training loss : 848206.4375 testing loss : 612.8659559583242\n",
            "epoch : 1935 training loss : 848080.25 testing loss : 621.5570382628821\n",
            "epoch : 1936 training loss : 849440.3125 testing loss : 580.7072578847935\n",
            "epoch : 1937 training loss : 848801.5625 testing loss : 629.1658270148049\n",
            "epoch : 1938 training loss : 842368.9375 testing loss : 594.1592564519527\n",
            "epoch : 1939 training loss : 836125.5 testing loss : 597.0174199648663\n",
            "epoch : 1940 training loss : 835867.8125 testing loss : 603.4304633668039\n",
            "epoch : 1941 training loss : 832477.8125 testing loss : 599.8014648551434\n",
            "epoch : 1942 training loss : 834677.0625 testing loss : 609.9741455664677\n",
            "epoch : 1943 training loss : 837606.6875 testing loss : 580.9805465272043\n",
            "epoch : 1944 training loss : 843107.875 testing loss : 573.6813744380411\n",
            "epoch : 1945 training loss : 849439.875 testing loss : 653.2199515937704\n",
            "epoch : 1946 training loss : 853336.375 testing loss : 599.2947233229612\n",
            "epoch : 1947 training loss : 841541.5625 testing loss : 598.8827774566887\n",
            "epoch : 1948 training loss : 833231.5 testing loss : 587.2612436324094\n",
            "epoch : 1949 training loss : 835499.5625 testing loss : 597.6086232324617\n",
            "epoch : 1950 training loss : 842892.25 testing loss : 621.2387652080671\n",
            "epoch : 1951 training loss : 844909.75 testing loss : 605.5437757420328\n",
            "epoch : 1952 training loss : 838673.25 testing loss : 595.236972097802\n",
            "epoch : 1953 training loss : 839433.375 testing loss : 595.2372059421202\n",
            "epoch : 1954 training loss : 842415.75 testing loss : 581.9963944675648\n",
            "epoch : 1955 training loss : 841571.0 testing loss : 627.5864865104709\n",
            "epoch : 1956 training loss : 849096.375 testing loss : 602.6222584458579\n",
            "epoch : 1957 training loss : 835393.9375 testing loss : 589.9737357734579\n",
            "epoch : 1958 training loss : 838456.625 testing loss : 584.377823346484\n",
            "epoch : 1959 training loss : 836521.875 testing loss : 640.1904616208203\n",
            "epoch : 1960 training loss : 839020.25 testing loss : 579.1040485664806\n",
            "epoch : 1961 training loss : 834247.875 testing loss : 582.8665340031142\n",
            "epoch : 1962 training loss : 835937.75 testing loss : 624.5212766913186\n",
            "epoch : 1963 training loss : 841042.875 testing loss : 619.6708982665982\n",
            "epoch : 1964 training loss : 842308.25 testing loss : 592.0294809235936\n",
            "epoch : 1965 training loss : 837496.25 testing loss : 583.0745859040624\n",
            "epoch : 1966 training loss : 851858.9375 testing loss : 626.0860035968038\n",
            "epoch : 1967 training loss : 836381.75 testing loss : 578.2947191875593\n",
            "epoch : 1968 training loss : 843631.125 testing loss : 583.4969750442336\n",
            "epoch : 1969 training loss : 839429.3125 testing loss : 632.9290746144488\n",
            "epoch : 1970 training loss : 854825.125 testing loss : 607.2836330021377\n",
            "epoch : 1971 training loss : 842353.1875 testing loss : 579.2352418456458\n",
            "epoch : 1972 training loss : 839309.9375 testing loss : 620.4290770112941\n",
            "epoch : 1973 training loss : 846309.6875 testing loss : 609.9814431160952\n",
            "epoch : 1974 training loss : 843033.0 testing loss : 583.0825770027869\n",
            "epoch : 1975 training loss : 842796.125 testing loss : 626.8376905031964\n",
            "epoch : 1976 training loss : 841689.75 testing loss : 608.910459803269\n",
            "epoch : 1977 training loss : 837953.25 testing loss : 584.0627021093284\n",
            "epoch : 1978 training loss : 849814.9375 testing loss : 591.4421008941347\n",
            "epoch : 1979 training loss : 844216.1875 testing loss : 643.2018995010748\n",
            "epoch : 1980 training loss : 848783.125 testing loss : 574.881455039556\n",
            "epoch : 1981 training loss : 848852.125 testing loss : 638.7385820350815\n",
            "epoch : 1982 training loss : 849534.9375 testing loss : 585.1485084917693\n",
            "epoch : 1983 training loss : 843834.625 testing loss : 571.5733850171081\n",
            "epoch : 1984 training loss : 843988.5 testing loss : 630.9826986241129\n",
            "epoch : 1985 training loss : 845080.0 testing loss : 601.9321736339974\n",
            "epoch : 1986 training loss : 842607.125 testing loss : 607.0869466034712\n",
            "epoch : 1987 training loss : 839479.5 testing loss : 565.2714040616972\n",
            "epoch : 1988 training loss : 847900.875 testing loss : 618.0689803921016\n",
            "epoch : 1989 training loss : 842851.6875 testing loss : 609.3389480704755\n",
            "epoch : 1990 training loss : 847033.875 testing loss : 580.5793391856472\n",
            "epoch : 1991 training loss : 838874.4375 testing loss : 611.3957627199393\n",
            "epoch : 1992 training loss : 831285.0 testing loss : 579.8261908451013\n",
            "epoch : 1993 training loss : 832626.625 testing loss : 603.773713371395\n",
            "epoch : 1994 training loss : 837072.0625 testing loss : 625.4746467771784\n",
            "epoch : 1995 training loss : 845209.4375 testing loss : 575.8151081705515\n",
            "epoch : 1996 training loss : 839291.75 testing loss : 611.7296055460398\n",
            "epoch : 1997 training loss : 841779.9375 testing loss : 624.7101262839494\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-3eec663121f0>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdata_for_testing\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtesting_dataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mdt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_for_testing\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_for_testing\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mnet_testing_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-dc37f0c8df53>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1916\u001b[0m     \u001b[0;31m# See full discussion on the problems with returning `Union` here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1917\u001b[0m     \u001b[0;31m# https://github.com/microsoft/pyright/issues/4213\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1919\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"_parameters\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1920\u001b[0m             \u001b[0m_parameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_parameters\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "#training area and getting validation curve\n",
        "loss = nn.MSELoss()\n",
        "optim = torch.optim.Adam(model.parameters(),lr = 0.001)\n",
        "training_loss = []\n",
        "testing_loss = []\n",
        "epoch = []\n",
        "e = 0\n",
        "while True:\n",
        "    net_training_loss = 0.0\n",
        "    net_iteration = 0\n",
        "    for i,(data,label) in enumerate(dataloader):\n",
        "        label = label.view(1000,1)\n",
        "        optim.zero_grad()\n",
        "        data = data.to(device)\n",
        "        label = label.to(device)\n",
        "        prediction = model.forward(data)\n",
        "        l = loss(prediction,label)\n",
        "        l.backward()\n",
        "        optim.step()\n",
        "        net_training_loss += l\n",
        "        net_iteration += 1\n",
        "    trainingloss = net_training_loss/net_iteration\n",
        "    training_loss.append(trainingloss)\n",
        "    net_testing_loss = 0.0\n",
        "    net_iteration = 0\n",
        "    for data_for_testing in testing_dataset:\n",
        "        dt,lt = data_for_testing[:-1], data_for_testing[-1]\n",
        "        prediction = model.forward(torch.from_numpy(dt).view(1,2).to(device))\n",
        "        l = abs(lt.item() - prediction.item())\n",
        "        net_testing_loss += l\n",
        "        net_iteration += 1\n",
        "    testingloss = net_testing_loss/net_iteration\n",
        "    testing_loss.append(testingloss)\n",
        "    epoch.append(e+1)\n",
        "    e = e + 1\n",
        "    if(trainingloss < testingloss):\n",
        "      break\n",
        "    print(f\"epoch : {e+1} training loss : {trainingloss} testing loss : {testingloss}\")\n",
        "\n"
      ],
      "id": "HzLqzBXBrKMX"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cj0oEbvtrx1T"
      },
      "id": "cj0oEbvtrx1T",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "script = torch.jit.script(model)\n",
        "script.save(\"agriculture_yield.pt\")"
      ],
      "metadata": {
        "id": "cpDzi7VJXwK_"
      },
      "execution_count": null,
      "outputs": [],
      "id": "cpDzi7VJXwK_"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bUikvjQrX8h8"
      },
      "id": "bUikvjQrX8h8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "model = RandomForestRegressor(n_estimators=100, max_depth=30)\n",
        "\n",
        "x_train,y_train = training_dataset[:,:-1],training_dataset[:,-1]\n",
        "x_test,y_test = testing_dataset[:,:-1],testing_dataset[:,-1]\n",
        "model.fit(x_train,y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "z5rbb0rbX82L",
        "outputId": "d1e3a5a7-740a-4b3b-dab5-2dd43e228280"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestRegressor(max_depth=30)"
            ],
            "text/html": [
              "<style>#sk-container-id-3 {\n",
              "  /* Definition of color scheme common for light and dark mode */\n",
              "  --sklearn-color-text: #000;\n",
              "  --sklearn-color-text-muted: #666;\n",
              "  --sklearn-color-line: gray;\n",
              "  /* Definition of color scheme for unfitted estimators */\n",
              "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
              "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
              "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
              "  --sklearn-color-unfitted-level-3: chocolate;\n",
              "  /* Definition of color scheme for fitted estimators */\n",
              "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
              "  --sklearn-color-fitted-level-1: #d4ebff;\n",
              "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
              "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
              "\n",
              "  /* Specific color for light theme */\n",
              "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
              "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-icon: #696969;\n",
              "\n",
              "  @media (prefers-color-scheme: dark) {\n",
              "    /* Redefinition of color scheme for dark theme */\n",
              "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
              "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-icon: #878787;\n",
              "  }\n",
              "}\n",
              "\n",
              "#sk-container-id-3 {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "#sk-container-id-3 pre {\n",
              "  padding: 0;\n",
              "}\n",
              "\n",
              "#sk-container-id-3 input.sk-hidden--visually {\n",
              "  border: 0;\n",
              "  clip: rect(1px 1px 1px 1px);\n",
              "  clip: rect(1px, 1px, 1px, 1px);\n",
              "  height: 1px;\n",
              "  margin: -1px;\n",
              "  overflow: hidden;\n",
              "  padding: 0;\n",
              "  position: absolute;\n",
              "  width: 1px;\n",
              "}\n",
              "\n",
              "#sk-container-id-3 div.sk-dashed-wrapped {\n",
              "  border: 1px dashed var(--sklearn-color-line);\n",
              "  margin: 0 0.4em 0.5em 0.4em;\n",
              "  box-sizing: border-box;\n",
              "  padding-bottom: 0.4em;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "#sk-container-id-3 div.sk-container {\n",
              "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
              "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
              "     so we also need the `!important` here to be able to override the\n",
              "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
              "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
              "  display: inline-block !important;\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-3 div.sk-text-repr-fallback {\n",
              "  display: none;\n",
              "}\n",
              "\n",
              "div.sk-parallel-item,\n",
              "div.sk-serial,\n",
              "div.sk-item {\n",
              "  /* draw centered vertical line to link estimators */\n",
              "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
              "  background-size: 2px 100%;\n",
              "  background-repeat: no-repeat;\n",
              "  background-position: center center;\n",
              "}\n",
              "\n",
              "/* Parallel-specific style estimator block */\n",
              "\n",
              "#sk-container-id-3 div.sk-parallel-item::after {\n",
              "  content: \"\";\n",
              "  width: 100%;\n",
              "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
              "  flex-grow: 1;\n",
              "}\n",
              "\n",
              "#sk-container-id-3 div.sk-parallel {\n",
              "  display: flex;\n",
              "  align-items: stretch;\n",
              "  justify-content: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-3 div.sk-parallel-item {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "}\n",
              "\n",
              "#sk-container-id-3 div.sk-parallel-item:first-child::after {\n",
              "  align-self: flex-end;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-3 div.sk-parallel-item:last-child::after {\n",
              "  align-self: flex-start;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-3 div.sk-parallel-item:only-child::after {\n",
              "  width: 0;\n",
              "}\n",
              "\n",
              "/* Serial-specific style estimator block */\n",
              "\n",
              "#sk-container-id-3 div.sk-serial {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "  align-items: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  padding-right: 1em;\n",
              "  padding-left: 1em;\n",
              "}\n",
              "\n",
              "\n",
              "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
              "clickable and can be expanded/collapsed.\n",
              "- Pipeline and ColumnTransformer use this feature and define the default style\n",
              "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
              "*/\n",
              "\n",
              "/* Pipeline and ColumnTransformer style (default) */\n",
              "\n",
              "#sk-container-id-3 div.sk-toggleable {\n",
              "  /* Default theme specific background. It is overwritten whether we have a\n",
              "  specific estimator or a Pipeline/ColumnTransformer */\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "/* Toggleable label */\n",
              "#sk-container-id-3 label.sk-toggleable__label {\n",
              "  cursor: pointer;\n",
              "  display: flex;\n",
              "  width: 100%;\n",
              "  margin-bottom: 0;\n",
              "  padding: 0.5em;\n",
              "  box-sizing: border-box;\n",
              "  text-align: center;\n",
              "  align-items: start;\n",
              "  justify-content: space-between;\n",
              "  gap: 0.5em;\n",
              "}\n",
              "\n",
              "#sk-container-id-3 label.sk-toggleable__label .caption {\n",
              "  font-size: 0.6rem;\n",
              "  font-weight: lighter;\n",
              "  color: var(--sklearn-color-text-muted);\n",
              "}\n",
              "\n",
              "#sk-container-id-3 label.sk-toggleable__label-arrow:before {\n",
              "  /* Arrow on the left of the label */\n",
              "  content: \"▸\";\n",
              "  float: left;\n",
              "  margin-right: 0.25em;\n",
              "  color: var(--sklearn-color-icon);\n",
              "}\n",
              "\n",
              "#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "/* Toggleable content - dropdown */\n",
              "\n",
              "#sk-container-id-3 div.sk-toggleable__content {\n",
              "  max-height: 0;\n",
              "  max-width: 0;\n",
              "  overflow: hidden;\n",
              "  text-align: left;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-3 div.sk-toggleable__content.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-3 div.sk-toggleable__content pre {\n",
              "  margin: 0.2em;\n",
              "  border-radius: 0.25em;\n",
              "  color: var(--sklearn-color-text);\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-3 div.sk-toggleable__content.fitted pre {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
              "  /* Expand drop-down */\n",
              "  max-height: 200px;\n",
              "  max-width: 100%;\n",
              "  overflow: auto;\n",
              "}\n",
              "\n",
              "#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
              "  content: \"▾\";\n",
              "}\n",
              "\n",
              "/* Pipeline/ColumnTransformer-specific style */\n",
              "\n",
              "#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-3 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator-specific style */\n",
              "\n",
              "/* Colorize estimator box */\n",
              "#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-3 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-3 div.sk-label label.sk-toggleable__label,\n",
              "#sk-container-id-3 div.sk-label label {\n",
              "  /* The background is the default theme color */\n",
              "  color: var(--sklearn-color-text-on-default-background);\n",
              "}\n",
              "\n",
              "/* On hover, darken the color of the background */\n",
              "#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "/* Label box, darken color on hover, fitted */\n",
              "#sk-container-id-3 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator label */\n",
              "\n",
              "#sk-container-id-3 div.sk-label label {\n",
              "  font-family: monospace;\n",
              "  font-weight: bold;\n",
              "  display: inline-block;\n",
              "  line-height: 1.2em;\n",
              "}\n",
              "\n",
              "#sk-container-id-3 div.sk-label-container {\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "/* Estimator-specific */\n",
              "#sk-container-id-3 div.sk-estimator {\n",
              "  font-family: monospace;\n",
              "  border: 1px dotted var(--sklearn-color-border-box);\n",
              "  border-radius: 0.25em;\n",
              "  box-sizing: border-box;\n",
              "  margin-bottom: 0.5em;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-3 div.sk-estimator.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "/* on hover */\n",
              "#sk-container-id-3 div.sk-estimator:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-3 div.sk-estimator.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
              "\n",
              "/* Common style for \"i\" and \"?\" */\n",
              "\n",
              ".sk-estimator-doc-link,\n",
              "a:link.sk-estimator-doc-link,\n",
              "a:visited.sk-estimator-doc-link {\n",
              "  float: right;\n",
              "  font-size: smaller;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1em;\n",
              "  height: 1em;\n",
              "  width: 1em;\n",
              "  text-decoration: none !important;\n",
              "  margin-left: 0.5em;\n",
              "  text-align: center;\n",
              "  /* unfitted */\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted,\n",
              "a:link.sk-estimator-doc-link.fitted,\n",
              "a:visited.sk-estimator-doc-link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "/* Span, style for the box shown on hovering the info icon */\n",
              ".sk-estimator-doc-link span {\n",
              "  display: none;\n",
              "  z-index: 9999;\n",
              "  position: relative;\n",
              "  font-weight: normal;\n",
              "  right: .2ex;\n",
              "  padding: .5ex;\n",
              "  margin: .5ex;\n",
              "  width: min-content;\n",
              "  min-width: 20ex;\n",
              "  max-width: 50ex;\n",
              "  color: var(--sklearn-color-text);\n",
              "  box-shadow: 2pt 2pt 4pt #999;\n",
              "  /* unfitted */\n",
              "  background: var(--sklearn-color-unfitted-level-0);\n",
              "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted span {\n",
              "  /* fitted */\n",
              "  background: var(--sklearn-color-fitted-level-0);\n",
              "  border: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link:hover span {\n",
              "  display: block;\n",
              "}\n",
              "\n",
              "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
              "\n",
              "#sk-container-id-3 a.estimator_doc_link {\n",
              "  float: right;\n",
              "  font-size: 1rem;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1rem;\n",
              "  height: 1rem;\n",
              "  width: 1rem;\n",
              "  text-decoration: none;\n",
              "  /* unfitted */\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "}\n",
              "\n",
              "#sk-container-id-3 a.estimator_doc_link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "#sk-container-id-3 a.estimator_doc_link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "#sk-container-id-3 a.estimator_doc_link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestRegressor(max_depth=30)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>RandomForestRegressor</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.ensemble.RandomForestRegressor.html\">?<span>Documentation for RandomForestRegressor</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>RandomForestRegressor(max_depth=30)</pre></div> </div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "id": "z5rbb0rbX82L"
    },
    {
      "cell_type": "code",
      "source": [
        "model.score(x_test,y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJiMd3wSYXtA",
        "outputId": "26bca78a-3656-4cf5-e699-1886ac395c50"
      },
      "id": "IJiMd3wSYXtA",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8934647425328816"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "joblib.dump(model,\"crop_price_prediction.rn\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7VA08otpYgmh",
        "outputId": "837b8a1d-0af2-4d05-f2ac-2e2040c3c062"
      },
      "id": "7VA08otpYgmh",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['crop_price_prediction.rn']"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jY5qqPg7ZajJ"
      },
      "id": "jY5qqPg7ZajJ",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:base] *",
      "language": "python",
      "name": "conda-base-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}